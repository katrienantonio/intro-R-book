[["index.html", "Data science in insurance: an R intro 1 Introduction 1.1 Learning outcomes 1.2 Data/science pipeline 1.3 Inspirations and references 1.4 About this book", " Data science in insurance: an R intro Chester Ismay and Albert Y. Kim and Katrien Antonio and Bavo Campo 2020-11-27 1 Introduction This book assumes no prerequisites: no algebra, no calculus, and no prior programming/coding experience. This is intended to be a gentle introduction to the practice of analyzing data and answering questions using data the way data scientists, statisticians, data journalists, and other researchers would. Our inspiration is the open source ModernDive book (Ismay and Kim 2018), with many tweaks, additions and changes by Katrien Antonio. This text book is primarily written for actuarial students as well as practitioners, but is of course not limited to the latter group. We get started with R in Chapter 2: R vs RStudio, coding in R, installing and loading R packages, the references used in this book. Thereafter, we look at different types of data and objects in R, including vectors, matrices, data frames and lists in Chapter 3. We get started with data in Chapter 4. Data visualisation is the focus of Chapter 5. More on data wrangling in Chapter 6. As probability distributions are of special importance to actuaries, these are discussed in Chapter 7. Using and writing functions is the topic of Chapter 8. Optimization tools help to optimize non straightforward likelihoods as discussed in Chapter 9. First examples of model building focus on linear and generalized linear models in Chapters 10 and 11 References follow in 12. 1.1 Learning outcomes By the end of this book, you should have mastered the following concepts How R can be used as an environment for data handling, visualization, analysis and programming. How to use R to to import/export data, to explore and manipulate data, to create insightful graphics and to write functions. How to find help in the R community, including finding examples of coding, books, support. How to perform simple tasks with R and how to look for more advanced tasks, further learning with specific packages. How to answer actuarial questions related to pricing and reserving. How to effectively create data stories using these tools. This book will help you develop your data science toolbox, including tools such as data visualization, data formatting, data wrangling, and data modeling using regression models. 1.2 Data/science pipeline Within the data analysis field there are many sub-fields that we will discuss throughout this book (though not necessarily in this order): data collection data wrangling data visualization data modeling interpretation of results data communication/storytelling These sub-fields are summarized in what Grolemund and Wickham term the data/science pipeline in Figure 1.1. Figure 1.1: Data/Science Pipeline 1.3 Inspirations and references In essence, this book combines my own research papers (Katrien Antonio) and course notes with many useful quotes and examples from my favourite R books listed below. This book is very much inspired by the following books or courses: Mathematical Statistics with Resampling and R (Chihara and Hesterberg 2011), OpenIntro: Intro Stat with Randomization and Simulation (Diez, Barr, and Çetinkaya-Rundel 2014), and R for Data Science (Grolemund and Wickham 2016), Moderndive (Ismay and Kim 2018), Jared Landers R for everyone (Lander 2017) Applied Econometrics with R (Kleiber and Zeileis 2008) An Introduction to Statistical Learning (James et al. 20AD) all the work of Michael Clark, see Michael Clarks website many, many courses on the DataCamp platform, including Katrien Antonio and Roel Verbelens Valuation of Life Insurance Products in R. 1.4 About this book This book was written using RStudios bookdown package by Yihui Xie (Xie 2020). This package simplifies the publishing of books by having all content written in R Markdown. The bookdown/R Markdown source code for all versions of ModernDive is available on GitHub. Could this be a new paradigm for textbooks? Instead of the traditional model of textbook companies publishing updated editions of the textbook every few years, we apply a software design influenced model of publishing more easily updated versions. We can then leverage open-source communities of instructors and developers for ideas, tools, resources, and feedback. As such, we welcome your pull requests. Finally, feel free to modify the book as you wish for your own needs, but please list the authors at the top of index.Rmd as Chester Ismay, Albert Y. Kim, and YOU! So, that is exactly what Katrien Antonio did! "],["getting-started.html", "2 Getting started in R 2.1 Some history 2.2 What are R and RStudio? 2.3 How do I code in R? 2.4 What are R packages? 2.5 Conclusion", " 2 Getting started in R Before we can start exploring data in R, there are some key concepts to understand first: What are R and RStudio? How do I code in R? What are R packages? Much of this chapter is based on two sources, which you should feel free to use as references, if you are looking for additional details: Ismays Getting used to R, RStudio, and R Markdown (Ismay 2016), which includes video screen recordings that you can follow along and pause as you learn. DataCamps online tutorials. DataCamp is a browser-based interactive platform for learning data science and their tutorials will help facilitate your learning of the above concepts (and other topics in this book). Go to DataCamp and create an account before continuing. 2.1 Some history R is a dialect of the S language (developed by John Chambers at Bell Labs in the 70s), namely Gnu-S. R was written by Robert Gentleman and Ross Ihaka (while being at the University of Auckland) in 1993. The (Kleiber and Zeileis 2008) book sketches the history of R, in Sections 1.5 and 1.6. The R source code was first released under the GNU General Public License (GPL) in 1995. Since mid-1997, there has been the R Development Core Team, currently comprising 20 members. In 1998, the Comprehensive R Archive Network CRAN was established, a family of mirror sites around the world that store identical, up-to-date versions of code and documentation for R. The first official release, R version 1.0.0, dates to 2000-02-29. Currently, version 4.0.3 is available. R is open source, i.e. GNU General Public License. The R environment (see About R) is an integrated suite of software facilities for data manipulation, calculation and graphical display. 2.2 What are R and RStudio? Throughout this book, we will assume that you are using R via RStudio. First time users often confuse the two. At its simplest: R is like a cars engine RStudio is like a cars dashboard R: Engine RStudio: Dashboard More precisely, R is a programming language that runs computations while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. In the same way that a speedometer, rear view mirrors, and a navigation system makes driving much easier, using RStudios interface makes using R much easier as well. Optional: For a more in-depth discussion on the difference between R and RStudio IDE, watch this DataCamp video (2m52s). 2.2.1 Installing R and RStudio You will first need to download and install both R and RStudio (Desktop version) on your computer. Download and install R. Note: You must do this first. Click on the download link corresponding to your computers operating system. Download and install RStudio. Scroll down to Installers for Supported Platforms Click on the download link corresponding to your computers operating system. Optional: If you need more detailed instructions on how to install R and RStudio, watch this DataCamp video (1m22s). 2.2.2 Using R via RStudio Recall our car analogy from above. Just as we dont drive a car by interacting directly with the engine, but rather by using elements on the cars dashboard, we wont be using R directly. Instead we will use RStudios interface. After you install R and RStudio on your computer, youll have two new programs AKA applications you can open. We will always work in RStudio and not R. In other words: R: Do not open this RStudio: Open this Do remember though, that (heavier) R-scripts run faster in R compared to RStudio. So, when you are familiar enough with the basics and are brave enough to be a mechanic, you can always use RGui which is the standard graphical user interface (GUI) for R. For now, however, we will use Rstudio only as it makes driving our car/running our analyses a whole lot easier. In addition, it also helps you to get familiar with some of the basic R concepts. After you open RStudio, you should see the following: Watch the following DataCamp video (4m10s) to learn about the different panes in RStudio, in particular the Console pane where you will later run R code. 2.3 How do I code in R? Now that youre set up with R and RStudio, you are probably asking yourself OK. Now how do I use R? The first thing to note as that unlike other software like Excel, STATA, or SAS that provide point and click interfaces, R is an interpreted language, meaning you have to enter in R commands written in R code i.e. you have to program in R (we use the terms coding and programming interchangeably in this book). While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that R users need to understand. Consequently, while this book is not a book on programming, you will still learn just enough of these basic programming concepts needed to explore and analyze data effectively. 2.3.1 Tips on learning to code Learning to code/program is very much like learning a foreign language, it can be very daunting and frustrating at first. However just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn. Before you know it, you will have fun learning more about it and be eager to learn even more. Something that also helps, is that most programmers have a good sense of humor. Just Google package R beepr or go to http://www.sumsar.net/blog/2014/01/announcing-pingr/ and go see for yourself. Or just run the following code (no need to know what it does right now): if(!&quot;beepr&quot; %in% rownames(installed.packages())) install.packages(&quot;beepr&quot;) for(i in 1:10) { Sys.sleep(1) beepr::beep(i) } So, whoever said learning cant be fun, obviously hasnt met programmers yet. Lastly, there are a few useful things to keep in mind as you learn to program: Computers are stupid: You have to tell a computer everything it needs to do. Furthermore, your instructions cant have any mistakes in them, nor can they be ambiguous in any way. We, for example, know that we both mean the same thing when typing length and lenght. The computer, however, doesnt. Take the copy/paste/tweak approach: Especially when learning your first programming language, it is often much easier to taking existing code that you know works and modify it to suit your ends, rather than trying to write new code from scratch. We call this the copy/paste/tweak approach. So early on, we suggest not trying to code from scratch, but please take the code we provide throughout this book and play around with it! Practice is key: Just as the only solution to improving your foreign language skills is practice, so also the only way to get better at R is through practice. Dont worry however, well give you plenty of opportunities to practice! In addition, dont forget to have fun while learning. This will make learning a whole lot easier and enable you to learn things quicker and faster. Just think back of all the boring and interesting, fun courses you had or just random things you learned. Which one do you still remember? Its the boring course, right? 2.4 What are R packages? Another point of confusion with new R users is the notion of a package. R packages extend the functionality of R by providing additional functions, data, documentation and can be downloaded for free from the internet. They are written by a world-wide community of R users. For example, among the many packages we will use in this book are the ggplot2 package for data visualization in Chapter 5 dplyr package for data wrangling in Chapter 6 There are two key things to remember about R packages: Installation: Most packages are not installed by default when you install R and RStudio. You need to install a package before you can use it. Once youve installed it, you likely dont need to install it again unless you want to update it to a newer version of the package. Loading: Packages are not loaded automatically when you open RStudio. You need to load them every time you open RStudio using the library() command. A good analogy for R packages is they are like apps you can download onto a mobile phone: R: A new phone R Packages: Apps you can download So, expanding on this analogy a bit: R is like a new mobile phone. It has a certain amount of functionality when you use it for the first time, but it doesnt have everything. R packages are like the apps you can download onto your phone, much like those offered in the App Store and Google Play. For example: Instagram. In order to use a package, just like in order to use Instagram, you must: First download it and install it. You do this only once. Load it, or in other words, open it, using the library() command. So just as you can only start sharing photos with your friends on Instagram if you first install the app and then open it, you can only access an R packages data and functions if you first install the package and then load it with the library() command. Lets cover these two steps: 2.4.1 Package installation (Note that if you are working on an RStudio Server, you probably will not need to install your own packages as that has been already done for you. Still it is important that you know this process for later when you are not using the RStudio Server but rather your own installation of RStudio Desktop.) There are two ways to install an R package. For example, to install the ggplot2 package: Easy way: In the Files pane of RStudio: Click on the Packages tab Click on Install Type the name of the package under Packages (separate multiple with space or comma): In this case, type ggplot2 Click Install Alternative way: In the Console pane run install.packages(\"ggplot2\") (you must include the quotation marks). Repeat this for the dplyr and nycflights13 packages. If you still experience problems, have a look at this blog post Installing R packages. Note: You only have to install a package once, unless you want to update an already installed package to the latest version. If you want to update a package to the latest version, then re-install it by repeating the above steps. 2.4.2 Package loading After youve installed a package, you can now load it using the library() command. For example, to load the ggplot2 and dplyr packages, run the following code in the Console pane: library(ggplot2) library(dplyr) Note: You have to reload each package you want to use every time you open a new session of RStudio. This is a little annoying to get used to and will be your most common error as you begin. When you see an error such as Error: could not find function remember that this likely comes from you trying to use a function in a package that has not been loaded. Remember to run the library() function with the appropriate package to fix this error. 2.4.3 Packages on CRAN R comes with a set of base packages or base system, maintained by the R core team only. Examples: base, datasets, graphics. Additional packages are on CRAN (Thursday 1/23/2014: 5,140 packages; Sunday 9/7/2014: 5,852 packages; Saturday 11/08/2014: 6,041 packages; Tuesday 11/8/2016: 9,473 packages; Monday 12/04/2017: 11,946 packages; Tuesday 04/10/2018: 12,430 packages). These packages are developed and maintained by R users worldwide, and shared with the R community through CRAN. 2.5 Conclusion You are now ready to get start your journey as an R-enthusiast! "],["objects-data-types.html", "3 Objects and data types in R 3.1 How it works 3.2 Objects 3.3 Everything is an object 3.4 Basic data types 3.5 Vectors 3.6 Matrices 3.7 Lists 3.8 Data frames 3.9 Exercises", " 3 Objects and data types in R 3.1 How it works You will now start with writing R code in the console and you will explore a first script of R code. Every line of code is interpreted and executed by R. Once R is done computing, the output of your R code will be shown in the console. In some cases, however, something might go wrong (e.g. due to an error in the code) and then you will either get a warning or an error. R makes use of the # sign to add comments, so that you and others can understand what the R code is about. Just like Twitter! Luckily, here your comments are not limited to 280 characters. When passing lines of code preceded by # to the R console, these will simply be ignored and hence, will not influence your results. [Quote from DataCamps Introduction to R course.] In its most basic form, R can be used as a simple calculator. We illustrate the use of some arithmetic operators in the code below. # use &#39;right click, run line or selection&#39;, of Ctrl + R 10^2 + 36 [1] 136 3.2 Objects A basic concept in (statistical) programming is called a variable and in R, this is commonly referred to as an object. An object allows you to store a value (e.g. 4) or a more complex data structure (e.g. a database). You can then later use this objects name to easily access the value or the data structure that is stored within this object. [Quote from DataCamps Introduction to R course.] We create an object by giving it a name and using the assignment operator &lt;- or -&gt; to assign a value to this object (Douglas et al. 2020). The value gets stored into the object to which the arrow is pointing. You can then view the value of the object by passing it to the console and the value will then be given as output. HappyObject &lt;- 1 -1 -&gt; SadObject HappyObject [1] 1 SadObject # Don&#39;t be so negative [1] -1 Can you guess what the output will be for the following code? HappyObject -&gt; SadObject IAmConfused &lt;- SadObject IAmConfused [1] 1 Once we have created an object, we can easily perform some calculations with it. HappyObject * 5 [1] 5 (HappyObject + 10) / 2 [1] 5.5 SadObject^2 [1] 1 Further, = is an alternative assignment operator to &lt;-, but is often discouraged for people new to R. The &lt;- operator is considered to be more important by R and precedes = in importance (for a more detailed explanation see https://stackoverflow.com/questions/1741820/what-are-the-differences-between-and-assignment-operators-in-r). In most contexts, however, = can be used as a safe alternative (Venables, Smith, and R Core Team 2020). Just know that you should use it with care. a &lt;- b = 2 # throws an error, these 2 operators should not be mixed mean(b = 5:10) # b is not an argument in this function and the object b is not created mean(b &lt;- 5:10) # here, b is created and then considered to be the argument of the function b In addition, the code above illustrates that, within functions, = is reserved to assign objects to the arguments. 3.3 Everything is an object In R, an analysis is normally broken down into a series of steps. Intermediate results are stored in objects, with minimal output at each step (often none). Instead, the objects are further manipulated to obtain the information required. In fact, the fundamental design principle underlying R (and S) is everything is an object. Hence, not only vectors and matrices are objects that can be passed to and returned by functions, but also functions themselves, and even function calls. (Quote from Applied Econometrics in R, by Kleiber &amp; Zeileis) A variable in R can take on any available data type, or hold any R object. # see all objects stored in R&#39;s memory, where &#39;ls()&#39; is for &#39;List Objects&#39; # and returns a vector of character strings # giving the names of the objects in the specified environment rm(list = ls()[!grepl(&quot;Object|Confused&quot;, ls(), perl = T)]) # Clean environment to have a short list ls() [1] &quot;HappyObject&quot; &quot;IAmConfused&quot; &quot;ObjectSize&quot; &quot;SadObject&quot; # to remove objects from R&#39;s memory, use rm(SadObject) ls() [1] &quot;HappyObject&quot; &quot;IAmConfused&quot; &quot;ObjectSize&quot; a &lt;- 1 b &lt;- 2 c &lt;- 3 d &lt;- 4 rm(a, b) rm(list = c(&#39;c&#39;, &#39;d&#39;)) a &lt;- 1 b &lt;- 2 c &lt;- 3 d &lt;- 4 # with the following code, you will remove everything in your working directory rm(list = ls()) All objects that you create, are stored in your current workspace and in RStudio you can view the list of objects by clicking on the Environment tab in the top right hand pane. This workspace is also referred to as the global environment and this is where all the interactive computations take place (i.e. outside of a function) (Wickham 2019). Without going to much into the technical details, we can sort of compare your workspace with your own, special sandbox. Everything that you create in your sandbox, stays there and gets saved in your .RData file when you close your session. When creating an Rstudio project, this RData gets automatically imported (with the default settings) when you open your project again and with this, your session gets restored as it contains all objects you created last time. When creating a new project in a different directory, you create a new sandbox and this makes it easy to structure all of your different projects and analyses. 3.4 Basic data types R works with numerous data types. Some of the most basic types to get started with are: Decimal values like 4.5 are called numerics. Natural numbers like 4 are called integers. Integers are also numerics. Boolean values (TRUE or FALSE) are called logical. Dates or POSIXct for time based variables. Here, Date stores just a date and POSIXct stores a date and time. Both objects are actually represented as the number of days (Date) or seconds (POSIXct) since January 1, 1970. Text (or string) values are called characters. Note how the quotation marks on the right indicate that some text is a character. my_numeric &lt;- 42.5 my_character &lt;- &quot;some text&quot; my_logical &lt;- TRUE my_date &lt;- as.Date(&quot;05/29/2018&quot;, &quot;%m/%d/%Y&quot;) Note that the logical values TRUE and FALSE can also be abbreviated as T and F, respectively. T [1] TRUE F [1] FALSE You can check the data type of an object beforehand. You can do this with the class() function. class(my_numeric) [1] &quot;numeric&quot; # your turn to check the type of &#39;my_character&#39; and &#39;my_logical&#39; and &#39;my_date&#39; When you are interested if an object is of a certain type, you can use the following functions: is.numeric(my_numeric) [1] TRUE is.character(my_numeric) [1] FALSE is.character(my_character) [1] TRUE is.logical(my_logical) [1] TRUE This is incredibly useful when you have to check the input thats passed to a self-written function and to prevent that objects of a wrong type get passed. In addition, as you might have noticed, theres no function is.Date. No need to worry, however, because Rs flexibility allows us to create a function like this ourselves, but well go over it more in detail in Chapter 8. For now, just know that you can alternatively use the function inherits or is inherits(my_date, &quot;Date&quot;) [1] TRUE is(my_date, &quot;Date&quot;) [1] TRUE 3.5 Vectors Vectors are one-dimension arrays that can hold numeric data, character data, or logical data. In other words, a vector is a simple tool to store data. In R, you create a vector with the combine function c(). You place the vector elements separated by a comma between the parentheses. (Quote from DataCamps Introduction to R course) Vectors are key! Operations are applied to each element of the vector automatically, there is no need to loop through the vector. # To combine elements into a vector, use c(): a = c(1, 2, 3, 4) Once we have created this vector, we can pass it to functions to gather some useful information about it. ?min ?max ?mean ?sd ?var min(a) max(a) mean(a) sd(a) var(a) In addition to the above functions, length is another function thats incredibly useful and one of the functions you will use a lot. When passing a vector to this function, it returns the number of elements that it contains length(a) [1] 4 Often, we want to create a vector thats a sequence of numbers. In this case, we can use the : symbol to create a sequence of values in steps of one (Douglas et al. 2020). Alternatively, we can use the function seq which allows for more flexibility. # steps of one 1:10 [1] 1 2 3 4 5 6 7 8 9 10 seq_len(10) [1] 1 2 3 4 5 6 7 8 9 10 # specify the steps yourself seq(from = 0, to = 10, by = 0.5) [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 [16] 7.5 8.0 8.5 9.0 9.5 10.0 # or the length of the vector, and the steps will be computed by R seq(from = 0, to = 10, length = 6) [1] 0 2 4 6 8 10 When we need to repeat certain values, we can use the rep function. rep(1, times = 5) [1] 1 1 1 1 1 rep(1:5, times = 5) [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 rep(1:5, each = 2) [1] 1 1 2 2 3 3 4 4 5 5 3.5.1 Vector indexing To access certain elements of a vector, we use the square brackets []. For example, Abra = 1:5 Abra[1] [1] 1 Abra[5] [1] 5 To select a subset of elements, we can specify an index vector (Venables, Smith, and R Core Team 2020) that specifies which elements should be selected and in which order. Abra[c(2, 4)] [1] 2 4 Abra[c(4, 2)] [1] 4 2 The index vector can be of four different types (Venables, Smith, and R Core Team 2020): A logical vector. Abra[c(TRUE, FALSE, TRUE, TRUE, FALSE)] [1] 1 3 4 Kadabra &lt;- Abra &gt; 3 Kadabra [1] FALSE FALSE FALSE TRUE TRUE Abra[Kadabra] [1] 4 5 A vector with positive indices, which specifies which elements should be selected. Abra[1:3] [1] 1 2 3 A vector with negative indices, which specifies which elements should be excluded. Abra[-c(1:3)] [1] 4 5 A vector of character strings, in case of a named vector. This is then similar to the index vector with positive indices, but now we select the items based on their names. This will be particularly useful later on, when we are working with data frames. a &lt;- 1:3 names(a) &lt;- c(&quot;Squirtle&quot;, &quot;Bulbasaur&quot;, &quot;Charmander&quot;) a Squirtle Bulbasaur Charmander 1 2 3 a[&quot;Squirtle&quot;] Squirtle 1 # or IChooseyou &lt;- c(&quot;Charmander&quot;) a[IChooseyou] Charmander 3 Next to selecting elements, we can also use this to perform an operation on these elements only. a[1] = 25 a Squirtle Bulbasaur Charmander 25 2 3 3.5.2 Character and logical vectors A vector can either hold numeric, character or logical values. family &lt;- c(&quot;Katrien&quot;, &quot;Jan&quot;, &quot;Leen&quot;) family [1] &quot;Katrien&quot; &quot;Jan&quot; &quot;Leen&quot; family[2] [1] &quot;Jan&quot; str(family) # str() displays the structure of an R object in compact way chr [1:3] &quot;Katrien&quot; &quot;Jan&quot; &quot;Leen&quot; class(family) [1] &quot;character&quot; In addition, you can give a name to the elements of a vector with the names() function. Here is how it works my_vector &lt;- c(&quot;Katrien Antonio&quot;, &quot;teacher&quot;) names(my_vector) &lt;- c(&quot;Name&quot;, &quot;Profession&quot;) my_vector Name Profession &quot;Katrien Antonio&quot; &quot;teacher&quot; Important to remember is that a vector can only hold elements of the same type. Consequently, when you specify elements of different types in a vector, it saves it to that type that contains the most information (logical &lt; numeric &lt; character). c(0, TRUE) [1] 0 1 c(0, &quot;Character&quot;) [1] &quot;0&quot; &quot;Character&quot; 3.5.3 Missing values When working with real-life data, you are confronted with missing data more often than youd care to admit. The values are indicated by NA and any operation on this value will remain NA. To assess which elements are missing in a vector, you can use the function is.na. a &lt;- c(1:2, NA, 4:5) a [1] 1 2 NA 4 5 is.na(a) [1] FALSE FALSE TRUE FALSE FALSE As it returns a logical vector, we can use it as an index vector. a[is.na(a)] [1] NA 3.5.4 Logical operators We are able to create logical expressions using the logical operators &lt;, &lt;=, &gt;, &gt;=, ==, where the last one is reserved exact equality. This enables us to select subset of elements. Further, we can combine logical expressions using &amp; or | to denote their intersection or union, respectively. a &lt;- 1:5 a &gt; 3 [1] FALSE FALSE FALSE TRUE TRUE a == 3 [1] FALSE FALSE TRUE FALSE FALSE a[a &gt; 2 &amp; a &lt; 4] [1] 3 a[a == 3 | a == 5] [1] 3 5 To get the negation of a logical expression, we make use of the ! operator. FALSE [1] FALSE !FALSE [1] TRUE b &lt;- c(TRUE, FALSE, TRUE, TRUE) !b [1] FALSE TRUE FALSE FALSE This ! operator can then be used for a whole range of useful manipulations. Going back to the vector with missing values, we can use this to exclude the missing values in the vector. a = c(1:2, NA, 4:5) a[!is.na(a)] [1] 1 2 4 5 na.omit(a) # alternative to omit missing values [1] 1 2 4 5 attr(,&quot;na.action&quot;) [1] 3 attr(,&quot;class&quot;) [1] &quot;omit&quot; The above also illustrates that we can combine multiple statements or manipulations in one line of code. Combining them gives us a very powerful tool to handle and analyze data in an efficient way. a &lt;- -5:5 max(a[a &gt; 0 &amp; a &lt;= 3]) [1] 3 3.5.5 Factors To specify that you have a vector with a discrete classification, we make use of a factor object which can either be ordered or unordered. These are mainly used in formulae, but we will already introduce the basics here. Fruits &lt;- c(&quot;Apple&quot;, &quot;Banana&quot;, &quot;Grape&quot;, &quot;Lemons&quot;) Fruits &lt;- factor(Fruits) Var &lt;- rep(1:4, each = 2) Var &lt;- factor(Var, levels = 1:4, labels = c(&quot;Apple&quot;, &quot;Banana&quot;, &quot;Grape&quot;, &quot;Lemons&quot;)) Var [1] Apple Apple Banana Banana Grape Grape Lemons Lemons Levels: Apple Banana Grape Lemons levels(Var) [1] &quot;Apple&quot; &quot;Banana&quot; &quot;Grape&quot; &quot;Lemons&quot; nlevels(Var) [1] 4 Be careful, however, when converting factor variables to numeric. The factor variables have an underlying numeric value assigned to them and you should therefore always be careful when converting them. as.numeric(Var) [1] 1 1 2 2 3 3 4 4 a &lt;- as.character(c(3, 5, 29, 5)) a &lt;- factor(a) a [1] 3 5 29 5 Levels: 29 3 5 as.numeric(a) [1] 2 3 1 3 3.6 Matrices In R, a matrix is a collection of elements of the same data type (numeric, character, or logical) arranged into a fixed number of rows and columns. Since you are only working with rows and columns, a matrix is called two-dimensional. You can construct a matrix in R with the matrix() function. (Quote from DataCamps Introduction to R course) # a 3x4 matrix, filled with 1,2,..., 12 matrix(1:12, 3, 4, byrow = TRUE) [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 5 6 7 8 [3,] 9 10 11 12 matrix(1:12, byrow = TRUE, nrow = 3) [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 5 6 7 8 [3,] 9 10 11 12 # hmmm, check help on &#39;matrix&#39; ? matrix starting httpd help server ... done In addition to the function matrix, we can also create matrices by combining vectors through use of the cbind and rbind functions. # one way of creating matrices is to bind vectors together cbind(1:2, 6:9) # by columns [,1] [,2] [1,] 1 6 [2,] 2 7 [3,] 1 8 [4,] 2 9 rbind(1:3, -(1:3)) # by rows [,1] [,2] [,3] [1,] 1 2 3 [2,] -1 -2 -3 m &lt;- cbind(a = 1:3, b = letters[1:3]) m a b [1,] &quot;1&quot; &quot;a&quot; [2,] &quot;2&quot; &quot;b&quot; [3,] &quot;3&quot; &quot;c&quot; rbind(a = 1:3, b = letters[1:3]) [,1] [,2] [,3] a &quot;1&quot; &quot;2&quot; &quot;3&quot; b &quot;a&quot; &quot;b&quot; &quot;c&quot; # ask help, what is the built-in &#39;letters&#39;? ? letters 3.6.1 Matrix operations and indexing Matrices and their theory are an essential part of linear algebra and R therefore has a lot of functions specifically designed for matrices. # create matrix object &#39;m&#39; x &lt;- matrix(1:12, 3, 4) x [,1] [,2] [,3] [,4] [1,] 1 4 7 10 [2,] 2 5 8 11 [3,] 3 6 9 12 nrow(x) [1] 3 ncol(x) [1] 4 dim(x) [1] 3 4 t(x) # matrix transpose [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 [4,] 10 11 12 x = matrix(1:4, nrow = 2) x %o% x # outer product , , 1, 1 [,1] [,2] [1,] 1 3 [2,] 2 4 , , 2, 1 [,1] [,2] [1,] 2 6 [2,] 4 8 , , 1, 2 [,1] [,2] [1,] 3 9 [2,] 6 12 , , 2, 2 [,1] [,2] [1,] 4 12 [2,] 8 16 outer(x, x, &quot;*&quot;) # alternative , , 1, 1 [,1] [,2] [1,] 1 3 [2,] 2 4 , , 2, 1 [,1] [,2] [1,] 2 6 [2,] 4 8 , , 1, 2 [,1] [,2] [1,] 3 9 [2,] 6 12 , , 2, 2 [,1] [,2] [1,] 4 12 [2,] 8 16 diag(x) # extract diagonal elements [1] 1 4 det(x) # determinant [1] -2 eigen(x) # eigenvalues and eigenvectors eigen() decomposition $values [1] 5.3723 -0.3723 $vectors [,1] [,2] [1,] -0.5658 -0.9094 [2,] -0.8246 0.4160 An important difference with other statistical software programs, is that * is used for element-wise multiplication. When you want to multiply matrices, you should use the %*% operator. x * x # element-wise multiplication [,1] [,2] [1,] 1 9 [2,] 4 16 t(x) %*% x # use %*% for matrix multiplication [,1] [,2] [1,] 5 11 [2,] 11 25 crossprod(x) # alternative to t(x) %*% x [,1] [,2] [1,] 5 11 [2,] 11 25 x %*% t(x) [,1] [,2] [1,] 10 14 [2,] 14 20 tcrossprod(x) [,1] [,2] [1,] 10 14 [2,] 14 20 Further, to get the inverse of a matrix, we use the solve function. solve(x) [,1] [,2] [1,] -2 1.5 [2,] 1 -0.5 To select a subset of elements of a matrix, we again use vector indices within the square brackets []. When we only want to select certain rows, columns or both, we put a comma in the square brackets. x[1:5] # select first 5 elements, starts from 1st element from the 1st column and proceeds to the next elements in the 1st column [1] 1 2 3 4 NA x[1, ] # select first row [1] 1 3 x[, 1] # select first column [1] 1 2 x[2, 2] # select fourth element in fourth column [1] 4 3.7 Lists A list in R allows you to gather a variety of objects under one object in an ordered way. These objects can be matrices, vectors, data frames, even other lists, etc. It is not even required that these objects are related to each other in any way. You could say that a list is some kind super data type: you can store practically any piece of information in it! (Quote from DataCamps Introduction to R course) # a first example of a list L &lt;- list(one = 1, two = c(1, 2), five = seq(1, 4, length=5), six = c(&quot;Katrien&quot;, &quot;Jan&quot;)) names(L) [1] &quot;one&quot; &quot;two&quot; &quot;five&quot; &quot;six&quot; summary(L) Length Class Mode one 1 -none- numeric two 2 -none- numeric five 5 -none- numeric six 2 -none- character class(L) [1] &quot;list&quot; str(L) List of 4 $ one : num 1 $ two : num [1:2] 1 2 $ five: num [1:5] 1 1.75 2.5 3.25 4 $ six : chr [1:2] &quot;Katrien&quot; &quot;Jan&quot; # list within a list # a list containing: a sample from a N(0,1), plus some markup # list within list mylist &lt;- list(sample = rnorm(5), family = &quot;normal distribution&quot;, parameters = list(mean = 0, sd = 1)) mylist $sample [1] 0.0001814 -0.6873159 -0.6844453 0.2392138 1.6020159 $family [1] &quot;normal distribution&quot; $parameters $parameters$mean [1] 0 $parameters$sd [1] 1 str(mylist) List of 3 $ sample : num [1:5] 0.000181 -0.687316 -0.684445 0.239214 1.602016 $ family : chr &quot;normal distribution&quot; $ parameters:List of 2 ..$ mean: num 0 ..$ sd : num 1 The objects stored on the list are known as its components (Venables, Smith, and R Core Team 2020) and to access these components, we either use a numerical value indicating the position in the list or the name of the component (only possible when it has been given a name of course). # now check mylist[[1]] [1] 0.0001814 -0.6873159 -0.6844453 0.2392138 1.6020159 mylist[[&quot;sample&quot;]] [1] 0.0001814 -0.6873159 -0.6844453 0.2392138 1.6020159 If the components have names, we can also access them using the $ operator in the following way. mylist$sample [1] 0.0001814 -0.6873159 -0.6844453 0.2392138 1.6020159 mylist$parameter $mean [1] 0 $sd [1] 1 mylist$parameters$mean [1] 0 Moreover, we can even access the elements of the component in the same way as we did before. mylist[[1]][2:4] [1] -0.6873 -0.6844 0.2392 To access lists within lists, we use the following code Dream = list(WithinADream = list(WithinAnotherDream = &quot;DieTraumdeutung&quot;)) Dream$WithinADream$WithinAnotherDream [1] &quot;DieTraumdeutung&quot; Dream[[1]][[1]] [1] &quot;DieTraumdeutung&quot; We use double square brackets to get the component in its original form. If we just use single brackets, we get it as an object of class list. Dream = list(WithinADream = &quot;SomethingFunny&quot;) class(Dream[[1]]) [1] &quot;character&quot; class(Dream[1]) [1] &quot;list&quot; 3.8 Data frames Most data sets you will be working with will be stored as data frames. A data frame has the variables of a data set as columns and the observations as rows. This will be a familiar concept for those coming from different statistical software packages such as SAS or SPSS. First, you will look at a classic data set from the datasets package that comes with the base R installation. The mtcars (Motor Trend Car Road Tests) data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). (Quote from DataCamps Introduction to R course) mtcars str(mtcars) &#39;data.frame&#39;: 32 obs. of 11 variables: $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... $ disp: num 160 160 108 258 360 ... $ hp : num 110 110 93 110 175 105 245 62 95 123 ... $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... $ wt : num 2.62 2.88 2.32 3.21 3.44 ... $ qsec: num 16.5 17 18.6 19.4 17 ... $ vs : num 0 0 1 1 0 1 0 1 1 1 ... $ am : num 1 1 1 0 0 0 0 0 0 0 ... $ gear: num 4 4 4 3 3 3 3 4 4 4 ... $ carb: num 4 4 1 1 2 1 4 2 2 4 ... head(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 tail(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.7 0 1 5 2 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2 Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.5 0 1 5 4 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.5 0 1 5 6 Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.6 0 1 5 8 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.6 1 1 4 2 Since using built-in data sets is not even half the fun of creating your own data sets, you will now work with your own personally created data set. (Quote from DataCamps Introduction to R course) Df &lt;- data.frame(x = c(11, 12, 7), y = c(19, 20, 21), z = c(10, 9, 7)) # quick scan of the object &#39;t&#39; summary(Df) x y z Min. : 7.0 Min. :19.0 Min. : 7.00 1st Qu.: 9.0 1st Qu.:19.5 1st Qu.: 8.00 Median :11.0 Median :20.0 Median : 9.00 Mean :10.0 Mean :20.0 Mean : 8.67 3rd Qu.:11.5 3rd Qu.:20.5 3rd Qu.: 9.50 Max. :12.0 Max. :21.0 Max. :10.00 str(Df) &#39;data.frame&#39;: 3 obs. of 3 variables: $ x: num 11 12 7 $ y: num 19 20 21 $ z: num 10 9 7 # another way to create the same data frame x &lt;- c(11, 12, 7) y &lt;- c(19, 20, 21) z &lt;- c(10, 9, 7) Df &lt;- data.frame(x, y, z) Accessing elements in a data frame is similar to how we access elements in a matrix. We can again use an index vector to access either the rows, columns or both. In addition, similar to lists, we can access columns using the $ operator or using the double square brackets. Df[1:2, ] x y z 1 11 19 10 2 12 20 9 Df[, 2:3] y z 1 19 10 2 20 9 3 21 7 Df$x [1] 11 12 7 Df[[&quot;x&quot;]] [1] 11 12 7 Df[[1]] [1] 11 12 7 In essence, a data frame can be seen as a combination of a list and a matrix. The variables are its components and the object has a separate class \"data.frame\" (Venables, Smith, and R Core Team 2020). is.list(Df) [1] TRUE class(Df) [1] &quot;data.frame&quot; But thats enough technical stuff for now, lets do our first data exploration and calculate the mean of the variable z in data frame t! mean(Df$z) [1] 8.667 mean(z) # does not work, why not? [1] 8.667 The code mean(z) doesnt work, because z wasnt defined in the global environment but only within your data frame. Going back to the sandbox analogy, you can look at the data frame as a mini-sandbox within your bigger sandbox. Everything that gets defined in this sandbox, stays there. This way, we keep our sandbox nice and organized. Just imagine the mess when all of your variables of your data frame would just float around in your sandbox. One dirty way to access the variables in your data frame without specifying the said data frame, is to use the attach function. With this function, we tell R that it also has to search within the attached data frame. rm(x, y, z) # Remove variables attach(Df) mean(z) [1] 8.667 detach(Df) Using attach, however, can be dangerous. If you created an object with a similar name to a variable in your data frame, R will not use the variable in your data frame but the one that was created before. x = rnorm(1e2) z = &quot;KadabraCastsConfusion&quot; attach(Df) The following objects are masked _by_ .GlobalEnv: x, z mean(x) [1] 0.03432 mean(Df$x) [1] 10 mean(z) Warning in mean.default(z): argument is not numeric or logical: returning NA [1] NA detach(Df) One way to avoid this, is to use the function with. with(Df, mean(z)) [1] 8.667 More on data frames # this does not work # Df &lt;- data.frame(x = c(11,12), y = c(19,20,21), z = c(10,9,7)) # but you _can_ do Df &lt;- data.frame(x = c(11, 12, NA), y = c(19, 20, 21), z = c(10, 9, 7)) # data frame with different types of information b &lt;- data.frame(x = c(11, 12, NA), y = c(&quot;me&quot;, &quot;you&quot;, &quot;everyone&quot;)) str(b) &#39;data.frame&#39;: 3 obs. of 2 variables: $ x: num 11 12 NA $ y: chr &quot;me&quot; &quot;you&quot; &quot;everyone&quot; In previous versions of R, character variables in a data frame were automatically converted to factor variables. They were briefly mentioned before and in essence, factor variables are used to store categorical variables (i.e. nominal, ordinal or dichotomous variables). Categorical variables can only take on a limited number of values. Conversely, continuous variables can take on an uncountable set of values. If you want to R to convert the variables with character strings to factor variables when creating a data frame, just specify stringsAsFactors = TRUE. b &lt;- data.frame(x = c(11, 12, NA), y = c(&quot;me&quot;, &quot;you&quot;, &quot;everyone&quot;), stringsAsFactors = TRUE) str(b) &#39;data.frame&#39;: 3 obs. of 2 variables: $ x: num 11 12 NA $ y: Factor w/ 3 levels &quot;everyone&quot;,&quot;me&quot;,..: 2 3 1 3.9 Exercises Learning check Explore the objects and data types in R. Create a vector fav_music with the names of your favorite artists. Create a vector num_records with the number of records you have in your collection of each of those artists. Create vector num_concerts with the number of times you attended a concert of these artists. Put everything together in a data frame, assign the name my_music to this data frame and change the labels of the information stored in the columns to artist, records and concerts. Extract the variable num_records from the data frame my_music. Calculate the total number of records in your collection (for the defined set of artists). Check the structure of the data frame, ask for a summary. "],["started-with-data.html", "4 Getting started with data in R 4.1 Importing data 4.2 Basic data handling steps 4.3 Exploratory Data Analysis (EDA) 4.4 Exercises", " 4 Getting started with data in R 4.1 Importing data Importing data into R to start your analyses-it should be the easiest step. Unfortunately, this is almost never the case. Data come in all sorts of formats, ranging from CSV and text files and statistical software files to databases and HTML data. Knowing which approach to use is key to getting started with the actual analysis. (Quote from DataCamps Importing Data in R (Part 1) course) The default location where R will look or store files, is your working directory (Venables, Smith, and R Core Team 2020). When opening an RStudio project, the working directory is automatically set to the folder where the .Rproj file is located. # what is the current working directory? getwd() # which files are currently stored in my working directory? dir() To change the working directory, we use the setwd() function. setwd(&quot;C:/Users/JohnDoe/Documents/RandomProject&quot;) If we have stored the data in a subfolder of our current working directory, we can specify the path as follows. # where are my data files? pathData &lt;- file.path(&#39;data&#39;) Creating a subfolder for your data files a good way to keep your files and project organized. When learning R, however, it may be easier to use the function file.choose() to use the point and click approach to select your file. 4.1.1 Importing a .csv file with read.csv() The utils package, which is automatically loaded in your R session on startup, can import CSV files with the read.csv() function. You will now load a data set on swimming pools in Brisbane, Australia (source: data.gov.au). The file contains the column names in the first row and uses commas to separate values within rows (CSV stands for comma-separated values). (Quote and example from DataCamps Importing Data in R (Part 1) course) path.pools &lt;- file.path(pathData, &quot;swimming_pools.csv&quot;) pools &lt;- read.csv(path.pools) str(pools) &#39;data.frame&#39;: 20 obs. of 4 variables: $ Name : chr &quot;Acacia Ridge Leisure Centre&quot; &quot;Bellbowrie Pool&quot; &quot;Carole Park&quot; &quot;Centenary Pool (inner City)&quot; ... $ Address : chr &quot;1391 Beaudesert Road, Acacia Ridge&quot; &quot;Sugarwood Street, Bellbowrie&quot; &quot;Cnr Boundary Road and Waterford Road Wacol&quot; &quot;400 Gregory Terrace, Spring Hill&quot; ... $ Latitude : num -27.6 -27.6 -27.6 -27.5 -27.4 ... $ Longitude: num 153 153 153 153 153 ... With stringsAsFactors, you can tell R whether it should convert strings in the flat file to factors. pools &lt;- read.csv(path.pools, stringsAsFactors = FALSE) We can then use the function str() to get a first compact view of the imported database. This function can be used on any object in R and will help you to understand its structure. str(pools) &#39;data.frame&#39;: 20 obs. of 4 variables: $ Name : chr &quot;Acacia Ridge Leisure Centre&quot; &quot;Bellbowrie Pool&quot; &quot;Carole Park&quot; &quot;Centenary Pool (inner City)&quot; ... $ Address : chr &quot;1391 Beaudesert Road, Acacia Ridge&quot; &quot;Sugarwood Street, Bellbowrie&quot; &quot;Cnr Boundary Road and Waterford Road Wacol&quot; &quot;400 Gregory Terrace, Spring Hill&quot; ... $ Latitude : num -27.6 -27.6 -27.6 -27.5 -27.4 ... $ Longitude: num 153 153 153 153 153 ... 4.1.1.1 Importing a .csv file with semicolons as delimiter Instead of a comma, it is also possible that the different rows are separated by semicolons. It is therefore important to know what kind of delimiter is used in your raw data file. This is why the str() function is so useful, since it will directly show you if the database was imported correctly or not. policy.path &lt;- file.path(pathData, &quot;policy.csv&quot;) policy &lt;- read.csv(policy.path) str(policy) &#39;data.frame&#39;: 39075 obs. of 1 variable: $ numeropol.debut_pol.fin_pol.freq_paiement.langue.type_prof.alimentation.type_territoire.utilisation.presence_alarme.marque_voiture.sexe.cout1.cout2.cout3.cout4.nbsin.exposition.cout.age.duree_permis.annee_vehicule: chr &quot;3;14/09/1995;24/04/1996;mensuel;F;Technicien;Végétarien;Urbain;Travail-quotidien;non;VOLKSWAGEN;F;;;;;0;0.61095&quot;| __truncated__ &quot;3;25/04/1996;23/12/1996;mensuel;F;Technicien;Végétarien;Urbain;Travail-quotidien;non;VOLKSWAGEN;F;;;;;0;0.66301&quot;| __truncated__ &quot;6;1/03/1995;27/02/1996;annuel;A;Technicien;Carnivore;Urbain;Travail-occasionnel;oui;NISSAN;M;279.5838509;;;;1;0&quot;| __truncated__ &quot;6;1/03/1996;14/01/1997;annuel;A;Technicien;Carnivore;Urbain;Travail-occasionnel;oui;NISSAN;M;;;;;0;0.873972603;;43;22;1994&quot; ... dim(policy) [1] 39075 1 Here, we clearly see that the database was not imported correctly. This is because the policy database uses a semicolon as a delimiter. By default, the delimiter in read.csv() is a comma. To correctly import the database, we can either use read.csv2(policy.path) which has the semicolon as default delimiter or we can change the delimiter by changing the argument sep in the read.csv() function. read.csv(policy.path, sep = &quot;;&quot;) We can even use the function read.table() to import the database, by specifying header = TRUE and sep = \";\". policy &lt;- read.table(policy.path, header = TRUE, sep = &quot;;&quot;) Once we have imported the database, we can further explore it using the following functions. head(policy) numeropol debut_pol fin_pol freq_paiement langue type_prof alimentation 1 3 14/09/1995 24/04/1996 mensuel F Technicien Végétarien 2 3 25/04/1996 23/12/1996 mensuel F Technicien Végétarien 3 6 1/03/1995 27/02/1996 annuel A Technicien Carnivore 4 6 1/03/1996 14/01/1997 annuel A Technicien Carnivore 5 6 15/01/1997 31/01/1997 annuel A Technicien Carnivore 6 6 1/02/1997 28/02/1997 annuel A Technicien Carnivore type_territoire utilisation presence_alarme marque_voiture sexe cout1 1 Urbain Travail-quotidien non VOLKSWAGEN F NA 2 Urbain Travail-quotidien non VOLKSWAGEN F NA 3 Urbain Travail-occasionnel oui NISSAN M 279.6 4 Urbain Travail-occasionnel oui NISSAN M NA 5 Urbain Travail-occasionnel oui NISSAN M NA 6 Urbain Travail-occasionnel oui NISSAN M NA cout2 cout3 cout4 nbsin exposition cout age duree_permis annee_vehicule 1 NA NA NA 0 0.61096 NA 29 10 1989 2 NA NA NA 0 0.66301 NA 30 11 1989 3 NA NA NA 1 0.99452 279.6 42 21 1994 4 NA NA NA 0 0.87397 NA 43 22 1994 5 NA NA NA 0 0.04384 NA 44 23 1994 6 NA NA NA 0 0.07397 NA 44 23 1994 tail(policy) numeropol debut_pol fin_pol freq_paiement langue type_prof 39070 88942 31/03/2003 30/03/2004 mensuel A Actuaire 39071 88945 21/03/2003 20/03/2004 annuel A Technicien 39072 88972 18/03/2003 17/03/2004 mensuel F Technicien 39073 88981 19/03/2003 18/03/2004 mensuel A Technicien 39074 88986 28/02/2004 27/02/2005 mensuel A Médecin 39075 89009 24/03/2003 23/03/2004 mensuel A Technicien alimentation type_territoire utilisation presence_alarme 39070 Carnivore Urbain Travail-occasionnel oui 39071 Végétalien Urbain Travail-occasionnel oui 39072 Végétarien Semi-urbain Travail-quotidien non 39073 Végétalien Urbain Travail-occasionnel oui 39074 Carnivore Urbain Travail-quotidien oui 39075 Végétarien Urbain Travail-occasionnel non marque_voiture sexe cout1 cout2 cout3 cout4 nbsin exposition cout age 39070 BMW M NA NA NA NA 0 1 NA 45 39071 TOYOTA M NA NA NA NA 0 1 NA 24 39072 PEUGEOT M NA NA NA NA 0 1 NA 58 39073 SUZUKI F NA NA NA NA 0 1 NA 23 39074 FIAT M NA NA NA NA 0 1 NA 41 39075 TOYOTA M NA NA NA NA 0 1 NA 58 duree_permis annee_vehicule 39070 30 1989 39071 5 2000 39072 33 2003 39073 5 1998 39074 19 1989 39075 37 2003 names(policy) [1] &quot;numeropol&quot; &quot;debut_pol&quot; &quot;fin_pol&quot; &quot;freq_paiement&quot; [5] &quot;langue&quot; &quot;type_prof&quot; &quot;alimentation&quot; &quot;type_territoire&quot; [9] &quot;utilisation&quot; &quot;presence_alarme&quot; &quot;marque_voiture&quot; &quot;sexe&quot; [13] &quot;cout1&quot; &quot;cout2&quot; &quot;cout3&quot; &quot;cout4&quot; [17] &quot;nbsin&quot; &quot;exposition&quot; &quot;cout&quot; &quot;age&quot; [21] &quot;duree_permis&quot; &quot;annee_vehicule&quot; dim(policy) [1] 39075 22 For the purpose of this chapter, we will write a custom function to explore the data.frame. For now, it is not necessary to understand it, as we will discuss functions in detail in Chapter 8. ExploreDf &lt;- function(x) { if(!is.data.frame(x)) stop(&quot;Only objects of type data.frame are allowed.&quot;) StrAdj &lt;- function(x) capture.output(str(x)) fntions &lt;- setNames(list(names, dim, head, tail, StrAdj), c(&quot;names&quot;, &quot;dim&quot;, &quot;head&quot;, &quot;tail&quot;, &quot;str&quot;)) Res &lt;- sapply(fntions, function(f) f(x), simplify = F) for(i in seq_along(Res)) { cat(&quot;\\n\\n&quot;, names(Res)[i], &quot;:\\n&quot;) if(i %in% 3:4) print.data.frame(Res[[i]]) else if(i != 5) print(Res[[i]]) else cat(paste0(Res[[5]], collapse = &quot;\\n&quot;)) } } 4.1.2 Importing a .txt file: the Danish fire insurance data read.table() is the most basic function to import data sets. The read.table() function is in fact the function that is used by the read.csv() and read.csv2() functions, but the arguments of the two latter functions are different. You can easily see this by looking at the functions themselves. read.csv function (file, header = TRUE, sep = &quot;,&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;.&quot;, fill = TRUE, comment.char = &quot;&quot;, ...) read.table(file = file, header = header, sep = sep, quote = quote, dec = dec, fill = fill, comment.char = comment.char, ...) &lt;bytecode: 0x0000000012bca650&gt; &lt;environment: namespace:utils&gt; read.csv2 function (file, header = TRUE, sep = &quot;;&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;,&quot;, fill = TRUE, comment.char = &quot;&quot;, ...) read.table(file = file, header = header, sep = sep, quote = quote, dec = dec, fill = fill, comment.char = comment.char, ...) &lt;bytecode: 0x000000001bdd5c60&gt; &lt;environment: namespace:utils&gt; An important difference with the read.csv() function, is that the header argument defaults to FALSE and the sep argument is \"\" by default in read.table(). (Quote from DataCamps Importing Data in R (Part 1) course) header is an argument that requires a logical argument (i.e. TRUE or FALSE) and is used to indicate whether the file contains the variable names as its first line. So, in almost all cases, you will have to set this to TRUE. path.fire &lt;- file.path(pathData, &quot;danish.txt&quot;) danish &lt;- read.table(path.fire, header = TRUE) head(danish, n = 10) # use the argument &#39;n&#39; to display less/more records Date Loss.in.DKM 1 01/03/1980 1.684 2 01/04/1980 2.094 3 01/05/1980 1.733 4 01/07/1980 1.780 5 01/07/1980 4.612 6 01/10/1980 8.725 7 01/10/1980 7.899 8 01/16/1980 2.208 9 01/16/1980 1.486 10 01/19/1980 2.796 ExploreDf(danish) names : [1] &quot;Date&quot; &quot;Loss.in.DKM&quot; dim : [1] 2167 2 head : Date Loss.in.DKM 1 01/03/1980 1.684 2 01/04/1980 2.094 3 01/05/1980 1.733 4 01/07/1980 1.780 5 01/07/1980 4.612 6 01/10/1980 8.725 tail : Date Loss.in.DKM 2162 12/24/1990 1.238 2163 12/27/1990 1.115 2164 12/30/1990 1.403 2165 12/30/1990 4.868 2166 12/30/1990 1.073 2167 12/31/1990 4.125 str : &#39;data.frame&#39;: 2167 obs. of 2 variables: $ Date : chr &quot;01/03/1980&quot; &quot;01/04/1980&quot; &quot;01/05/1980&quot; &quot;01/07/1980&quot; ... $ Loss.in.DKM: num 1.68 2.09 1.73 1.78 4.61 ... Compared to the raw .txt file, something is a bit different in the imported dataset. Can you see whats different? Thats right, the second column name has dots instead of hyphens. # Function to open files using R opendir &lt;- function(dir = getwd()){ if (.Platform[&#39;OS.type&#39;] == &quot;windows&quot;){ shell.exec(dir) } else { system(paste(Sys.getenv(&quot;R_BROWSER&quot;), dir)) } } # open file opendir(paste0(getwd(), &quot;/&quot;, path.fire)) When importing a data file with headers, R checks if the column names are syntactically valid (i.e. valid in R-code). If not, these are adjusted to make sure that they are usable in R. One way to avoid this kind of behavior, is to set the check.names argument to FALSE. danish &lt;- read.table(path.fire, header = TRUE, check.names = FALSE) names(danish) [1] &quot;Date&quot; &quot;Loss-in-DKM&quot; Note, however, that you will now have to select your columns using backticks (e.g. df$variable``). # The following does not work # danish$Loss-in-DKM # This does work head(danish$`Loss-in-DKM`) [1] 1.684 2.094 1.733 1.780 4.612 8.725 You can also explicitly specify the column names and as well as column types/classes of the resulting data frame. You can do this by setting the col.names and the colClasses argument to a vector of strings. (Quote from DataCamps Importing Data in R (Part 1) course) path.hotdogs &lt;- file.path(pathData, &quot;hotdogs.txt&quot;) hotdogs &lt;- read.table(path.hotdogs, header = FALSE, col.names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;)) # display structure of hotdogs str(hotdogs) &#39;data.frame&#39;: 54 obs. of 3 variables: $ type : chr &quot;Beef&quot; &quot;Beef&quot; &quot;Beef&quot; &quot;Beef&quot; ... $ calories: int 186 181 176 149 184 190 158 139 175 148 ... $ sodium : int 495 477 425 322 482 587 370 322 479 375 ... # edit the colClasses argument to import the data correctly: hotdogs2 hotdogs2 &lt;- read.table(path.hotdogs, header = FALSE, col.names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;), colClasses = c(&quot;factor&quot;, &quot;NULL&quot;, &quot;numeric&quot;)) # display structure of hotdogs2 str(hotdogs2) &#39;data.frame&#39;: 54 obs. of 2 variables: $ type : Factor w/ 3 levels &quot;Beef&quot;,&quot;Meat&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ sodium: num 495 477 425 322 482 587 370 322 479 375 ... What happened? What is the effect of specifying one of the colClasses as NULL? Another very useful function to explore your data.frame is the summary() function. This function can be used on a whole range of objects and provides you with a short summary of the object (as you might have expected). summary(danish) Date Loss-in-DKM Length:2167 Min. : 1.00 Class :character 1st Qu.: 1.32 Mode :character Median : 1.78 Mean : 3.39 3rd Qu.: 2.97 Max. :263.25 Inspecting the data.frame, we see that the class of the variable Date is not correct. One way to convert this variable to an object with class Date is by using the as.Date() function. danish$Date &lt;- as.Date(danish$Date, format = &quot;%m/%d/%Y&quot;) str(danish) &#39;data.frame&#39;: 2167 obs. of 2 variables: $ Date : Date, format: &quot;1980-01-03&quot; &quot;1980-01-04&quot; ... $ Loss-in-DKM: num 1.68 2.09 1.73 1.78 4.61 ... To understand why format = \"%m/%d/%Y\", you can check the help page of strptime(). Shortly summarized, %m indicates that month is given as a decimal number, %d that the day of the month is also given as a decimal number, %Y that year with century is given and the latter three are separated by / as this is also the delimiter that us used in the Date column. ?strptime OrigLocal = Sys.getlocale(&quot;LC_TIME&quot;) Sys.setlocale(&quot;LC_TIME&quot;, &quot;English_United States.1252&quot;) [1] &quot;English_United States.1252&quot; as.Date(&quot;16-Apr-1963&quot;, format = &quot;%d-%b-%Y&quot;) [1] &quot;1963-04-16&quot; Sys.setlocale(&quot;LC_TIME&quot;, OrigLocal) [1] &quot;English_United States.1252&quot; Or you can try to fix this directly when importing the danish.txt. path.fire &lt;- file.path(pathData, &quot;danish.txt&quot;) danish &lt;- read.table(path.fire, header = TRUE, colClasses = c(&quot;Date&quot;, &quot;numeric&quot;)) head(danish$Date) [1] &quot;0001-03-19&quot; &quot;0001-04-19&quot; &quot;0001-05-19&quot; &quot;0001-07-19&quot; &quot;0001-07-19&quot; [6] &quot;0001-10-19&quot; Setting colClasses to Date, however, only works when the format is either \"%Y-%m-%d\" or \"%Y/%m/%d\" (see ?as.Date). We therefore have to put in some extra effort and create a custom function/class to correctly import the Date variable. setClass(&quot;myDate&quot;) setAs(&quot;character&quot;, &quot;myDate&quot;, function(from) as.Date(from, format = &quot;%m/%d/%Y&quot;)) danish2 &lt;- read.table(path.fire, header = TRUE, colClasses = c(&quot;myDate&quot;, &quot;numeric&quot;)) str(danish2) &#39;data.frame&#39;: 2167 obs. of 2 variables: $ Date : Date, format: &quot;1980-01-03&quot; &quot;1980-01-04&quot; ... $ Loss.in.DKM: num 1.68 2.09 1.73 1.78 4.61 ... 4.1.3 Importing a .sas7bdat file When you decided to make the switch from SAS to R, you obviously made the right choice. SAS has been around for a very long time, but even since 1991 it was clear that R has always been the better choice. if(!&quot;fortunes&quot; %in% rownames(installed.packages())) install.packages(&quot;fortunes&quot;) fortunes::fortune(22) I quit using SAS in 1991 because my productivity jumped at least 20% within one month of using S-Plus. -- Frank Harrell R-help (November 2003) fortunes::fortune(84) There are companies whose yearly license fees to SAS total millions of dollars. Then those companies hire armies of SAS programmers to program an archaic macro language using old statistical methods to produce ugly tables and the worst graphics in the statistical software world. -- Frank Harrell R-help (November 2004) fortunes::fortune(224) It&#39;s interesting that SAS Institute feels that non-peer-reviewed software with hidden implementations of analytic methods that cannot be reproduced by others should be trusted when building aircraft engines. -- Frank Harrell (in response to the statement of the SAS director of technology product marketing: &quot;We have customers who build engines for aircraft. I am happy they are not using freeware when I get on a jet.&quot;) R-help (January 2009) To import a SAS file that has a sas7bdat format, we use the read.sas7bdat() function from the sas7bdat package. if(!&quot;sas7bdat&quot; %in% rownames(installed.packages())) install.packages(&quot;sas7bdat&quot;) library(sas7bdat) path.severity &lt;- file.path(pathData, &quot;severity.sas7bdat&quot;) severity &lt;- read.sas7bdat(path.severity) ExploreDf(severity) names : [1] &quot;policyId&quot; &quot;claimId&quot; &quot;rc&quot; &quot;deductible&quot; &quot;claimAmount&quot; dim : [1] 19287 5 head : policyId claimId rc deductible claimAmount 1 6e+05 9e+05 35306 1200 35306 2 6e+05 9e+05 19773 50 19773 3 6e+05 9e+05 41639 100 41639 4 6e+05 9e+05 10649 50 10649 5 6e+05 9e+05 20479 50 20479 6 6e+05 9e+05 9853 50 9853 tail : policyId claimId rc deductible claimAmount 19282 612853 919300 NaN 50 151 19283 612854 919301 1587 300 1587 19284 612854 919302 NaN 300 574 19285 612855 919303 NaN 50 323 19286 612856 919304 1287 1200 1287 19287 612857 919305 1910 1200 1910 str : &#39;data.frame&#39;: 19287 obs. of 5 variables: $ policyId : num 6e+05 6e+05 6e+05 6e+05 6e+05 ... $ claimId : num 9e+05 9e+05 9e+05 9e+05 9e+05 ... $ rc : num 35306 19773 41639 10649 20479 ... $ deductible : num 1200 50 100 50 50 50 50 50 50 50 ... $ claimAmount: num 35306 19773 41639 10649 20479 ... - attr(*, &quot;pkg.version&quot;)= chr &quot;0.5&quot; - attr(*, &quot;column.info&quot;)=List of 5 ..$ :List of 11 .. ..$ name : chr &quot;policyId&quot; .. ..$ offset: int 0 .. ..$ length: int 8 .. ..$ type : chr &quot;numeric&quot; .. ..$ format: chr &quot;BEST&quot; .. ..$ fhdr : int 0 .. ..$ foff : int 44 .. ..$ flen : int 4 .. ..$ lhdr : int 0 .. ..$ loff : int 0 .. ..$ llen : int 0 ..$ :List of 11 .. ..$ name : chr &quot;claimId&quot; .. ..$ offset: int 8 .. ..$ length: int 8 .. ..$ type : chr &quot;numeric&quot; .. ..$ format: chr &quot;BEST&quot; .. ..$ fhdr : int 0 .. ..$ foff : int 56 .. ..$ flen : int 4 .. ..$ lhdr : int 0 .. ..$ loff : int 0 .. ..$ llen : int 0 ..$ :List of 11 .. ..$ name : chr &quot;rc&quot; .. ..$ offset: int 16 .. ..$ length: int 8 .. ..$ type : chr &quot;numeric&quot; .. ..$ format: chr &quot;BEST&quot; .. ..$ fhdr : int 0 .. ..$ foff : int 64 .. ..$ flen : int 4 .. ..$ lhdr : int 0 .. ..$ loff : int 0 .. ..$ llen : int 0 ..$ :List of 11 .. ..$ name : chr &quot;deductible&quot; .. ..$ offset: int 24 .. ..$ length: int 8 .. ..$ type : chr &quot;numeric&quot; .. ..$ format: chr &quot;BEST&quot; .. ..$ fhdr : int 0 .. ..$ foff : int 80 .. ..$ flen : int 4 .. ..$ lhdr : int 0 .. ..$ loff : int 0 .. ..$ llen : int 0 ..$ :List of 11 .. ..$ name : chr &quot;claimAmount&quot; .. ..$ offset: int 32 .. ..$ length: int 8 .. ..$ type : chr &quot;numeric&quot; .. ..$ format: chr &quot;COMMA&quot; .. ..$ fhdr : int 0 .. ..$ foff : int 96 .. ..$ flen : int 5 .. ..$ lhdr : int 0 .. ..$ loff : int 0 .. ..$ llen : int 0 - attr(*, &quot;date.created&quot;)= POSIXct[1:1], format: &quot;1960-01-01&quot; - attr(*, &quot;date.modified&quot;)= POSIXct[1:1], format: &quot;1960-01-01&quot; - attr(*, &quot;SAS.release&quot;)= chr &quot;9.0401M1&quot; - attr(*, &quot;SAS.host&quot;)= chr &quot;X64_7PRO&quot; - attr(*, &quot;OS.version&quot;)= chr &quot;&quot; - attr(*, &quot;OS.maker&quot;)= chr &quot;&quot; - attr(*, &quot;OS.name&quot;)= chr &quot;&quot; - attr(*, &quot;endian&quot;)= chr &quot;little&quot; - attr(*, &quot;winunix&quot;)= chr &quot;windows&quot; 4.1.4 Importing an .xlsx file You will import data from Excel using the readxl package (authored by Hadley Wickham and maintained by RStudio). Before you can start importing from Excel, you should find out which sheets are available in the workbook. You can use the excel_sheets() function for this. (Quote and example from DataCamps Importing Data in R (Part 1) course) # load the readxl package library(readxl) path.urbanpop &lt;- file.path(pathData, &quot;urbanpop.xlsx&quot;) excel_sheets(path.urbanpop) [1] &quot;1960-1966&quot; &quot;1967-1974&quot; &quot;1975-2011&quot; You can import the Excel file with the read_excel() function. Here is the recipe: pop_1 &lt;- read_excel(path.urbanpop, sheet = 1) pop_2 &lt;- read_excel(path.urbanpop, sheet = 2) pop_3 &lt;- read_excel(path.urbanpop, sheet = 3) str(pop_1) tibble [209 x 8] (S3: tbl_df/tbl/data.frame) $ country: chr [1:209] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... $ 1960 : num [1:209] 769308 494443 3293999 NA NA ... $ 1961 : num [1:209] 814923 511803 3515148 13660 8724 ... $ 1962 : num [1:209] 858522 529439 3739963 14166 9700 ... $ 1963 : num [1:209] 903914 547377 3973289 14759 10748 ... $ 1964 : num [1:209] 951226 565572 4220987 15396 11866 ... $ 1965 : num [1:209] 1000582 583983 4488176 16045 13053 ... $ 1966 : num [1:209] 1058743 602512 4649105 16693 14217 ... # put pop_1, pop_2 and pop_3 in a list: pop_list pop_list &lt;- list(pop_1, pop_2, pop_3) The object pop_1 is a tibble, an object of tbl_df class (the tibble) that provides stricter checking and better formatting than the traditional data frame. The main advantage to using a tbl_df over a regular data frame is the printing: tbl objects only print a few rows and all the columns that fit on one screen, describing the rest of it as text. If you want to stick to traditional data frames, you can convert it using the as.data.frame function. pop_1_df &lt;- as.data.frame(pop_1) str(pop_1_df) &#39;data.frame&#39;: 209 obs. of 8 variables: $ country: chr &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... $ 1960 : num 769308 494443 3293999 NA NA ... $ 1961 : num 814923 511803 3515148 13660 8724 ... $ 1962 : num 858522 529439 3739963 14166 9700 ... $ 1963 : num 903914 547377 3973289 14759 10748 ... $ 1964 : num 951226 565572 4220987 15396 11866 ... $ 1965 : num 1000582 583983 4488176 16045 13053 ... $ 1966 : num 1058743 602512 4649105 16693 14217 ... In the previous demo you generated a list of three Excel sheets that you imported. However, loading in every sheet manually and then merging them in a list can be quite tedious. Luckily, you can automate this with lapply(). (Quote from DataCamps Importing Data in R (Part 1) course) pop_list &lt;- lapply(excel_sheets(path.urbanpop), read_excel, path = path.urbanpop) str(pop_list) List of 3 $ : tibble [209 x 8] (S3: tbl_df/tbl/data.frame) ..$ country: chr [1:209] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ..$ 1960 : num [1:209] 769308 494443 3293999 NA NA ... ..$ 1961 : num [1:209] 814923 511803 3515148 13660 8724 ... ..$ 1962 : num [1:209] 858522 529439 3739963 14166 9700 ... ..$ 1963 : num [1:209] 903914 547377 3973289 14759 10748 ... ..$ 1964 : num [1:209] 951226 565572 4220987 15396 11866 ... ..$ 1965 : num [1:209] 1000582 583983 4488176 16045 13053 ... ..$ 1966 : num [1:209] 1058743 602512 4649105 16693 14217 ... $ : tibble [209 x 9] (S3: tbl_df/tbl/data.frame) ..$ country: chr [1:209] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ..$ 1967 : num [1:209] 1119067 621180 4826104 17349 15440 ... ..$ 1968 : num [1:209] 1182159 639964 5017299 17996 16727 ... ..$ 1969 : num [1:209] 1248901 658853 5219332 18619 18088 ... ..$ 1970 : num [1:209] 1319849 677839 5429743 19206 19529 ... ..$ 1971 : num [1:209] 1409001 698932 5619042 19752 20929 ... ..$ 1972 : num [1:209] 1502402 720207 5815734 20263 22406 ... ..$ 1973 : num [1:209] 1598835 741681 6020647 20742 23937 ... ..$ 1974 : num [1:209] 1696445 763385 6235114 21194 25482 ... $ : tibble [209 x 38] (S3: tbl_df/tbl/data.frame) ..$ country: chr [1:209] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ..$ 1975 : num [1:209] 1793266 785350 6460138 21632 27019 ... ..$ 1976 : num [1:209] 1905033 807990 6774099 22047 28366 ... ..$ 1977 : num [1:209] 2021308 830959 7102902 22452 29677 ... ..$ 1978 : num [1:209] 2142248 854262 7447728 22899 31037 ... ..$ 1979 : num [1:209] 2268015 877898 7810073 23457 32572 ... ..$ 1980 : num [1:209] 2398775 901884 8190772 24177 34366 ... ..$ 1981 : num [1:209] 2493265 927224 8637724 25173 36356 ... ..$ 1982 : num [1:209] 2590846 952447 9105820 26342 38618 ... ..$ 1983 : num [1:209] 2691612 978476 9591900 27655 40983 ... ..$ 1984 : num [1:209] 2795656 1006613 10091289 29062 43207 ... ..$ 1985 : num [1:209] 2903078 1037541 10600112 30524 45119 ... ..$ 1986 : num [1:209] 3006983 1072365 11101757 32014 46254 ... ..$ 1987 : num [1:209] 3113957 1109954 11609104 33548 47019 ... ..$ 1988 : num [1:209] 3224082 1146633 12122941 35095 47669 ... ..$ 1989 : num [1:209] 3337444 1177286 12645263 36618 48577 ... ..$ 1990 : num [1:209] 3454129 1198293 13177079 38088 49982 ... ..$ 1991 : num [1:209] 3617842 1215445 13708813 39600 51972 ... ..$ 1992 : num [1:209] 3788685 1222544 14248297 41049 54469 ... ..$ 1993 : num [1:209] 3966956 1222812 14789176 42443 57079 ... ..$ 1994 : num [1:209] 4152960 1221364 15322651 43798 59243 ... ..$ 1995 : num [1:209] 4347018 1222234 15842442 45129 60598 ... ..$ 1996 : num [1:209] 4531285 1228760 16395553 46343 60927 ... ..$ 1997 : num [1:209] 4722603 1238090 16935451 47527 60462 ... ..$ 1998 : num [1:209] 4921227 1250366 17469200 48705 59685 ... ..$ 1999 : num [1:209] 5127421 1265195 18007937 49906 59281 ... ..$ 2000 : num [1:209] 5341456 1282223 18560597 51151 59719 ... ..$ 2001 : num [1:209] 5564492 1315690 19198872 52341 61062 ... ..$ 2002 : num [1:209] 5795940 1352278 19854835 53583 63212 ... ..$ 2003 : num [1:209] 6036100 1391143 20529356 54864 65802 ... ..$ 2004 : num [1:209] 6285281 1430918 21222198 56166 68301 ... ..$ 2005 : num [1:209] 6543804 1470488 21932978 57474 70329 ... ..$ 2006 : num [1:209] 6812538 1512255 22625052 58679 71726 ... ..$ 2007 : num [1:209] 7091245 1553491 23335543 59894 72684 ... ..$ 2008 : num [1:209] 7380272 1594351 24061749 61118 73335 ... ..$ 2009 : num [1:209] 7679982 1635262 24799591 62357 73897 ... ..$ 2010 : num [1:209] 7990746 1676545 25545622 63616 74525 ... ..$ 2011 : num [1:209] 8316976 1716842 26216968 64817 75207 ... Apart from path and sheet, there are several other arguments you can specify in read_excel(). One of these arguments is called col_names. By default it is TRUE, denoting whether the first row in the Excel sheets contains the column names. If this is not the case, you can set col_names to FALSE. In this case, R will choose column names for you. You can also choose to set col_names to a character vector with names for each column. (Quote from DataCamps Importing Data in R (Part 1) course) path.urbanpop_nonames &lt;- file.path(pathData, &quot;urbanpop_nonames.xlsx&quot;) # Import the the first Excel sheet of urbanpop_nonames.xlsx (R gives names): pop_a pop_a &lt;- read_excel(path.urbanpop_nonames, col_names = FALSE) New names: * `` -&gt; ...1 * `` -&gt; ...2 * `` -&gt; ...3 * `` -&gt; ...4 * `` -&gt; ...5 * ... # Import the the first Excel sheet of urbanpop_nonames.xlsx (specify col_names): pop_b cols &lt;- c(&quot;country&quot;, paste0(&quot;year_&quot;, 1960:1966)) pop_b &lt;- read_excel(path.urbanpop_nonames, col_names = cols) # Print the summary of pop_a summary(pop_a) ...1 ...2 ...3 ...4 Length:209 Min. :3.38e+03 Min. :1.03e+03 Min. :1.09e+03 Class :character 1st Qu.:8.90e+04 1st Qu.:7.06e+04 1st Qu.:7.50e+04 Mode :character Median :5.81e+05 Median :5.70e+05 Median :5.94e+05 Mean :4.99e+06 Mean :4.99e+06 Mean :5.14e+06 3rd Qu.:3.08e+06 3rd Qu.:2.81e+06 3rd Qu.:2.95e+06 Max. :1.26e+08 Max. :1.29e+08 Max. :1.32e+08 NA&#39;s :11 ...5 ...6 ...7 ...8 Min. :1.15e+03 Min. :1.22e+03 Min. :1.28e+03 Min. :1.35e+03 1st Qu.:8.19e+04 1st Qu.:8.50e+04 1st Qu.:8.86e+04 1st Qu.:9.36e+04 Median :6.19e+05 Median :6.45e+05 Median :6.79e+05 Median :7.35e+05 Mean :5.30e+06 Mean :5.47e+06 Mean :5.64e+06 Mean :5.79e+06 3rd Qu.:3.15e+06 3rd Qu.:3.30e+06 3rd Qu.:3.32e+06 3rd Qu.:3.42e+06 Max. :1.35e+08 Max. :1.37e+08 Max. :1.40e+08 Max. :1.42e+08 # Print the summary of pop_b summary(pop_b) country year_1960 year_1961 year_1962 Length:209 Min. :3.38e+03 Min. :1.03e+03 Min. :1.09e+03 Class :character 1st Qu.:8.90e+04 1st Qu.:7.06e+04 1st Qu.:7.50e+04 Mode :character Median :5.81e+05 Median :5.70e+05 Median :5.94e+05 Mean :4.99e+06 Mean :4.99e+06 Mean :5.14e+06 3rd Qu.:3.08e+06 3rd Qu.:2.81e+06 3rd Qu.:2.95e+06 Max. :1.26e+08 Max. :1.29e+08 Max. :1.32e+08 NA&#39;s :11 year_1963 year_1964 year_1965 year_1966 Min. :1.15e+03 Min. :1.22e+03 Min. :1.28e+03 Min. :1.35e+03 1st Qu.:8.19e+04 1st Qu.:8.50e+04 1st Qu.:8.86e+04 1st Qu.:9.36e+04 Median :6.19e+05 Median :6.45e+05 Median :6.79e+05 Median :7.35e+05 Mean :5.30e+06 Mean :5.47e+06 Mean :5.64e+06 Mean :5.79e+06 3rd Qu.:3.15e+06 3rd Qu.:3.30e+06 3rd Qu.:3.32e+06 3rd Qu.:3.42e+06 Max. :1.35e+08 Max. :1.37e+08 Max. :1.40e+08 Max. :1.42e+08 In the code printed above, paste0 (and also paste) converts its arguments (via as.character) to character strings, and concatenates them (in case of paste separating them by the string given by the argument sep, which is a single space by default). Many other packages exist to import Excel data, including XLConnect, an Excel Connector for R that provides comprehensive functionality to read, write and format Excel data. See DataCamps Importing Data in R (Part 1) course. 4.2 Basic data handling steps You will now learn some basic functions to handle data in R. You start with basic instructions (from base R) for data handling and more on data wrangling follows in Chapter 6. Useful functions from base are subset, sort, order, merge, cbind and rbind. Manipulating the data typically consumes a lot of effort in the beginning, but will become second nature once you get the hang of it. Manipulating the data often requires repeated operations on different sections of the data, in a split-apply-combine way of working. Lets illustrate all of this below. Some of the examples that follow are taken from Michael Clarks `An introduction to R. 4.2.1 Subsetting The data set state.x77 is available from the package datasets. This package contains a variety of data sets and some of them contain information on all 50 states of the United States of America. The state.x77 data set, for example, is a matrix with 50 rows and 8 columns which contains a wealth of information on all 50 different states. ?state.x77 states &lt;- data.frame(state.x77) ExploreDf(states) names : [1] &quot;Population&quot; &quot;Income&quot; &quot;Illiteracy&quot; &quot;Life.Exp&quot; &quot;Murder&quot; [6] &quot;HS.Grad&quot; &quot;Frost&quot; &quot;Area&quot; dim : [1] 50 8 head : Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area Alabama 3615 3624 2.1 69.05 15.1 41.3 20 50708 Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 Arizona 2212 4530 1.8 70.55 7.8 58.1 15 113417 Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 51945 California 21198 5114 1.1 71.71 10.3 62.6 20 156361 Colorado 2541 4884 0.7 72.06 6.8 63.9 166 103766 tail : Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area Vermont 472 3907 0.6 71.64 5.5 57.1 168 9267 Virginia 4981 4701 1.4 70.08 9.5 47.8 85 39780 Washington 3559 4864 0.6 71.72 4.3 63.5 32 66570 West Virginia 1799 3617 1.4 69.48 6.7 41.6 100 24070 Wisconsin 4589 4468 0.7 72.48 3.0 54.5 149 54464 Wyoming 376 4566 0.6 70.29 6.9 62.9 173 97203 str : &#39;data.frame&#39;: 50 obs. of 8 variables: $ Population: num 3615 365 2212 2110 21198 ... $ Income : num 3624 6315 4530 3378 5114 ... $ Illiteracy: num 2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ... $ Life.Exp : num 69 69.3 70.5 70.7 71.7 ... $ Murder : num 15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ... $ HS.Grad : num 41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ... $ Frost : num 20 152 15 65 20 166 139 103 11 60 ... $ Area : num 50708 566432 113417 51945 156361 ... states[14, ] Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area Indiana 5313 4458 0.7 70.88 7.1 52.9 122 36097 states[3, 6, drop = F] # use drop = FALSE to keep it as a data.frame HS.Grad Arizona 58.1 states[, &#39;Frost&#39;] [1] 20 152 15 65 20 166 139 103 11 60 0 126 127 122 140 114 95 12 161 [20] 101 103 125 160 50 108 155 139 188 174 115 120 82 80 186 124 82 44 126 [39] 127 65 172 70 35 137 168 85 32 100 149 173 states$Frost [1] 20 152 15 65 20 166 139 103 11 60 0 126 127 122 140 114 95 12 161 [20] 101 103 125 160 50 108 155 139 188 174 115 120 82 80 186 124 82 44 126 [39] 127 65 172 70 35 137 168 85 32 100 149 173 You will also use the data stored in state.region, a factor giving the region (Northeast, South, North Central, West) that each state belongs to. state.region [1] South West West South West [6] West Northeast South South South [11] West West North Central North Central North Central [16] North Central South South Northeast South [21] Northeast North Central North Central South North Central [26] West North Central West Northeast Northeast [31] West Northeast South North Central North Central [36] South West Northeast Northeast South [41] North Central South South West Northeast [46] South West South North Central West Levels: Northeast South North Central West length(state.region) [1] 50 # select those states that are in the south of the US mysubset &lt;- subset(states, state.region == &quot;South&quot;) # subset a selection of variables str(states) &#39;data.frame&#39;: 50 obs. of 8 variables: $ Population: num 3615 365 2212 2110 21198 ... $ Income : num 3624 6315 4530 3378 5114 ... $ Illiteracy: num 2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ... $ Life.Exp : num 69 69.3 70.5 70.7 71.7 ... $ Murder : num 15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ... $ HS.Grad : num 41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ... $ Frost : num 20 152 15 65 20 166 139 103 11 60 ... $ Area : num 50708 566432 113417 51945 156361 ... (mysubset &lt;- states[, c(1:2, 7:8)]) Population Income Frost Area Alabama 3615 3624 20 50708 Alaska 365 6315 152 566432 Arizona 2212 4530 15 113417 Arkansas 2110 3378 65 51945 California 21198 5114 20 156361 Colorado 2541 4884 166 103766 Connecticut 3100 5348 139 4862 Delaware 579 4809 103 1982 Florida 8277 4815 11 54090 Georgia 4931 4091 60 58073 Hawaii 868 4963 0 6425 Idaho 813 4119 126 82677 Illinois 11197 5107 127 55748 Indiana 5313 4458 122 36097 Iowa 2861 4628 140 55941 Kansas 2280 4669 114 81787 Kentucky 3387 3712 95 39650 Louisiana 3806 3545 12 44930 Maine 1058 3694 161 30920 Maryland 4122 5299 101 9891 Massachusetts 5814 4755 103 7826 Michigan 9111 4751 125 56817 Minnesota 3921 4675 160 79289 Mississippi 2341 3098 50 47296 Missouri 4767 4254 108 68995 Montana 746 4347 155 145587 Nebraska 1544 4508 139 76483 Nevada 590 5149 188 109889 New Hampshire 812 4281 174 9027 New Jersey 7333 5237 115 7521 New Mexico 1144 3601 120 121412 New York 18076 4903 82 47831 North Carolina 5441 3875 80 48798 North Dakota 637 5087 186 69273 Ohio 10735 4561 124 40975 Oklahoma 2715 3983 82 68782 Oregon 2284 4660 44 96184 Pennsylvania 11860 4449 126 44966 Rhode Island 931 4558 127 1049 South Carolina 2816 3635 65 30225 South Dakota 681 4167 172 75955 Tennessee 4173 3821 70 41328 Texas 12237 4188 35 262134 Utah 1203 4022 137 82096 Vermont 472 3907 168 9267 Virginia 4981 4701 85 39780 Washington 3559 4864 32 66570 West Virginia 1799 3617 100 24070 Wisconsin 4589 4468 149 54464 Wyoming 376 4566 173 97203 (mysubset &lt;- states[, c(&quot;Population&quot;, &quot;Income&quot;, &quot;Frost&quot;, &quot;Area&quot;)]) Population Income Frost Area Alabama 3615 3624 20 50708 Alaska 365 6315 152 566432 Arizona 2212 4530 15 113417 Arkansas 2110 3378 65 51945 California 21198 5114 20 156361 Colorado 2541 4884 166 103766 Connecticut 3100 5348 139 4862 Delaware 579 4809 103 1982 Florida 8277 4815 11 54090 Georgia 4931 4091 60 58073 Hawaii 868 4963 0 6425 Idaho 813 4119 126 82677 Illinois 11197 5107 127 55748 Indiana 5313 4458 122 36097 Iowa 2861 4628 140 55941 Kansas 2280 4669 114 81787 Kentucky 3387 3712 95 39650 Louisiana 3806 3545 12 44930 Maine 1058 3694 161 30920 Maryland 4122 5299 101 9891 Massachusetts 5814 4755 103 7826 Michigan 9111 4751 125 56817 Minnesota 3921 4675 160 79289 Mississippi 2341 3098 50 47296 Missouri 4767 4254 108 68995 Montana 746 4347 155 145587 Nebraska 1544 4508 139 76483 Nevada 590 5149 188 109889 New Hampshire 812 4281 174 9027 New Jersey 7333 5237 115 7521 New Mexico 1144 3601 120 121412 New York 18076 4903 82 47831 North Carolina 5441 3875 80 48798 North Dakota 637 5087 186 69273 Ohio 10735 4561 124 40975 Oklahoma 2715 3983 82 68782 Oregon 2284 4660 44 96184 Pennsylvania 11860 4449 126 44966 Rhode Island 931 4558 127 1049 South Carolina 2816 3635 65 30225 South Dakota 681 4167 172 75955 Tennessee 4173 3821 70 41328 Texas 12237 4188 35 262134 Utah 1203 4022 137 82096 Vermont 472 3907 168 9267 Virginia 4981 4701 85 39780 Washington 3559 4864 32 66570 West Virginia 1799 3617 100 24070 Wisconsin 4589 4468 149 54464 Wyoming 376 4566 173 97203 Next to the function subset(), we can also use the vector indices (see Chapter 3). When using which(), it returns the indices for which the logical expression is TRUE. It is in fact always safer to use the logical statement in combination with which, since which automatically treats missing values as FALSE. Just look at what goes wrong in the following example. CopyStates = states CopyStates$Income[sample(1:nrow(CopyStates), 3, F)] = NA CopyStates[CopyStates$Income &gt; 5000, ] Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 California 21198 5114 1.1 71.71 10.3 62.6 20 156361 Connecticut 3100 5348 1.1 72.48 3.1 56.0 139 4862 NA NA NA NA NA NA NA NA NA Illinois 11197 5107 0.9 70.14 10.3 52.6 127 55748 Maryland 4122 5299 0.9 70.22 8.5 52.3 101 9891 NA.1 NA NA NA NA NA NA NA NA NA.2 NA NA NA NA NA NA NA NA Nevada 590 5149 0.5 69.03 11.5 65.2 188 109889 New Jersey 7333 5237 1.1 70.93 5.2 52.5 115 7521 North Dakota 637 5087 0.8 72.78 1.4 50.3 186 69273 CopyStates[which(CopyStates$Income &gt; 5000), ] Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 California 21198 5114 1.1 71.71 10.3 62.6 20 156361 Connecticut 3100 5348 1.1 72.48 3.1 56.0 139 4862 Illinois 11197 5107 0.9 70.14 10.3 52.6 127 55748 Maryland 4122 5299 0.9 70.22 8.5 52.3 101 9891 Nevada 590 5149 0.5 69.03 11.5 65.2 188 109889 New Jersey 7333 5237 1.1 70.93 5.2 52.5 115 7521 North Dakota 637 5087 0.8 72.78 1.4 50.3 186 69273 4.2.2 Find minimum or maximum A similar function to which() is the function which.min(), which returns the index of the smallest value in a vector. which.max() works in a similar way. Using the information stored in states, which states in the US have the smallest, respectively highest, population density? least_pop &lt;- which.min(states$Population) states[least_pop, ] Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 most_pop &lt;- which.max(states$Population) states[most_pop, ] Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area California 21198 5114 1.1 71.71 10.3 62.6 20 156361 Next to these functions, pmin() and pmax() are also incredibly useful. Lets illustrate the difference with the functions min() and max() with a short example. a &lt;- 1:5 b &lt;- -1 * (1:5) min(a, b) [1] -5 pmin(a, b) [1] -1 -2 -3 -4 -5 max(a, b) [1] 5 pmax(a, b) [1] 1 2 3 4 5 4.2.3 Sorting To sort a vector, you can use the function sort(). sort(states$Population) [1] 365 376 472 579 590 637 681 746 812 813 868 931 [13] 1058 1144 1203 1544 1799 2110 2212 2280 2284 2341 2541 2715 [25] 2816 2861 3100 3387 3559 3615 3806 3921 4122 4173 4589 4767 [37] 4931 4981 5313 5441 5814 7333 8277 9111 10735 11197 11860 12237 [49] 18076 21198 sort(states$Population, decreasing = T) [1] 21198 18076 12237 11860 11197 10735 9111 8277 7333 5814 5441 5313 [13] 4981 4931 4767 4589 4173 4122 3921 3806 3615 3559 3387 3100 [25] 2861 2816 2715 2541 2341 2284 2280 2212 2110 1799 1544 1203 [37] 1144 1058 931 868 813 812 746 681 637 590 579 472 [49] 376 365 This function, however, is not useful and even dangerous within data frames since you only sort the values of the vector itself. To sort data in a data frame, you use the function order() which returns the indices of the vector. Hence, to sort the states based on their population, you use the following code. head(sort(states$Population)) [1] 365 376 472 579 590 637 head(order(states$Population)) [1] 2 50 45 8 28 34 sort1.states &lt;- states[order(states$Population), ] head(sort1.states) Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 Wyoming 376 4566 0.6 70.29 6.9 62.9 173 97203 Vermont 472 3907 0.6 71.64 5.5 57.1 168 9267 Delaware 579 4809 0.9 70.06 6.2 54.6 103 1982 Nevada 590 5149 0.5 69.03 11.5 65.2 188 109889 North Dakota 637 5087 0.8 72.78 1.4 50.3 186 69273 # sort by two variables sort2.states &lt;- states[order(states$Illiteracy, states$Income), ] head(sort2.states) Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area South Dakota 681 4167 0.5 72.08 1.7 53.3 172 75955 Iowa 2861 4628 0.5 72.56 2.3 59.0 140 55941 Nevada 590 5149 0.5 69.03 11.5 65.2 188 109889 Vermont 472 3907 0.6 71.64 5.5 57.1 168 9267 Utah 1203 4022 0.6 72.90 4.5 67.3 137 82096 Idaho 813 4119 0.6 71.87 5.3 59.5 126 82677 By default, the sort order is increasing (or alphabetical in case of a character string). You can change this by setting decreasing = TRUE. # sort in reverse order sort3.states &lt;- states[order(states$Life.Exp, decreasing = T), ] head(sort3.states) Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area Hawaii 868 4963 1.9 73.60 6.2 61.9 0 6425 Minnesota 3921 4675 0.6 72.96 2.3 57.6 160 79289 Utah 1203 4022 0.6 72.90 4.5 67.3 137 82096 North Dakota 637 5087 0.8 72.78 1.4 50.3 186 69273 Nebraska 1544 4508 0.6 72.60 2.9 59.3 139 76483 Kansas 2280 4669 0.6 72.58 4.5 59.9 114 81787 4.2.4 Merging To add a column to an existing data frame Df, you can either use Df$NewVar &lt;- 1 or Df[[\"NewVar\"]] &lt;- 1. Df &lt;- data.frame(id = factor(1:12), group = factor(rep(1:2, each = 3))) str(Df) &#39;data.frame&#39;: 12 obs. of 2 variables: $ id : Factor w/ 12 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ group: Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 1 1 2 2 2 1 1 1 2 ... head(Df) id group 1 1 1 2 2 1 3 3 1 4 4 2 5 5 2 6 6 2 x &lt;- rnorm(12) y &lt;- sample(70:100, 12) x2 &lt;- rnorm(12) # add a column Df$grade &lt;- y # or Df[[&quot;grade&quot;]] &lt;- y head(Df) id group grade 1 1 1 85 2 2 1 70 3 3 1 98 4 4 2 75 5 5 2 81 6 6 2 82 To merge different data frames, you can use the function merge(). For this, we of course need a column in both data frames thats a unique identifier for each of the observations. Alternatively, you can use cbind(). I dont need to tell you that this of course comes with its own dangers and that you need to make sure that both data frames are then sorted. Df2 &lt;- data.frame(id = Df$id, y) head(Df2) id y 1 1 85 2 2 70 3 3 98 4 4 75 5 5 81 6 6 82 Df3 &lt;- merge(Df, Df2, by = &quot;id&quot;, sort = F) # using merge head(Df3) id group grade y 1 1 1 85 85 2 2 1 70 70 3 3 1 98 98 4 4 2 75 75 5 5 2 81 81 6 6 2 82 82 Df4 &lt;- cbind(Df, x) # using cbind() head(Df4) id group grade x 1 1 1 85 0.6318 2 2 1 70 0.4113 3 3 1 98 -1.6698 4 4 2 75 -0.4771 5 5 2 81 0.8770 6 6 2 82 0.6436 To add rows to an existing data frame, you use the function rbind(). # add rows Df2 &lt;- data.frame(id = factor(13:24), group = factor(rep(1:2, e = 3)), grade = sample(y)) Df2 id group grade 1 13 1 93 2 14 1 74 3 15 1 81 4 16 2 98 5 17 2 85 6 18 2 70 7 19 1 82 8 20 1 75 9 21 1 87 10 22 2 92 11 23 2 94 12 24 2 97 Df5 &lt;- rbind(Df, Df2) Df5 id group grade 1 1 1 85 2 2 1 70 3 3 1 98 4 4 2 75 5 5 2 81 6 6 2 82 7 7 1 93 8 8 1 97 9 9 1 87 10 10 2 74 11 11 2 92 12 12 2 94 13 13 1 93 14 14 1 74 15 15 1 81 16 16 2 98 17 17 2 85 18 18 2 70 19 19 1 82 20 20 1 75 21 21 1 87 22 22 2 92 23 23 2 94 24 24 2 97 rm(list = ls()[grepl(&quot;Df&quot;, ls())]) # Clean environment 4.2.5 Aggregate People experienced with SQL generally want to run an aggregation and group by as one of their first tasks with R. aggregate() splits the data into subsets, computes summary statistics for each, and returns the result in a convenient form. You will work with diamonds, a data set in the ggplot2 package containing the prices and other attributes of almost 54,000 diamonds. ggplot2 is a package authored and maintained by Hadley Wickham to `Create Elegant Data Visualisations Using the Grammar of Graphics. library(ggplot2) head(diamonds) # A tibble: 6 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 # average price for each type of cut aggregate(price ~ cut, diamonds, mean) cut price 1 Fair 4359 2 Good 3929 3 Very Good 3982 4 Premium 4584 5 Ideal 3458 aggregate(diamonds$price, list(diamonds$cut), mean) Group.1 x 1 Fair 4359 2 Good 3929 3 Very Good 3982 4 Premium 4584 5 Ideal 3458 # do a manual check, using `subset()` s &lt;- subset(diamonds, cut == &#39;Fair&#39;) mean(s$price) [1] 4359 Hence, with the above code we tell R to split the data set diamonds into subsets according to the values of the variable cut and to calculate the mean for each of the subsets. Lets break this code down to get a better grasp of what it does. 1. aggregate(x ~ SplitBy, Df, Function, ...) + x ~ SplitBy: We pass a formula which tells R to split the data frame Df according to the different values of the variable SplitBy and to pass the values of the variable x to the function Function. + Df: The data frame to use. + Function: The function that is computed on each of the subsets. + ... : Additional arguments that are passed to Function. The ellipsis argument ... is incredibly useful to pass arguments to the function. This way we are able to adjust the default arguments of the function that is used on each of the subsets. For example, when we have missing values for the variable price, we can tell R to remove these when computing the mean by setting na.rm = TRUE. The argument na.rm of the function mean takes a logical value indicating whether NA values should be stripped before the computation proceeds (see ?mean).. # add arguments to the function called diamonds$price[sample(seq_len(nrow(diamonds)), 3, F)] &lt;- NA aggregate(price ~ cut, diamonds, mean, na.action = NULL) # na.action is set to NULL for illustration purposes cut price 1 Fair 4359 2 Good 3929 3 Very Good NA 4 Premium NA 5 Ideal NA aggregate(price ~ cut, diamonds, mean, na.rm = TRUE, na.action = NULL) cut price 1 Fair 4359 2 Good 3929 3 Very Good 3982 4 Premium 4585 5 Ideal 3458 The function aggregate() is often one of the first more complex functions that you will use as a first-time user. If you are a bit confused, just try to play around with it a bit and have fun. Just load in the Pokémon data set from Kaggle (retrieved from https://www.kaggle.com/rounakbanik/pokemon) and play a bit with the aggregate function. Remember that it doesnt always have to be serious (unless you want to be the very best and catch em all). Pokemon &lt;- read.csv(&quot;./data/pokemon.csv&quot;) Pokemon$capture_rate &lt;- gsub(&quot; .*&quot;, &quot;&quot;, Pokemon$capture_rate) Pokemon$capture_rate &lt;- as.numeric(Pokemon$capture_rate) head(aggregate(capture_rate ~ type1, Pokemon, mean)) type1 capture_rate 1 bug 119.83 2 dark 84.48 3 dragon 37.33 4 electric 106.97 5 fairy 116.94 6 fighting 103.75 Next to playing a bit around with the code, you also learn a great deal from other peoples code and this is why we also included the following useful illustrations: s &lt;- aggregate(price ~ cut, diamonds, mean) s cut price 1 Fair 4359 2 Good 3929 3 Very Good 3982 4 Premium 4585 5 Ideal 3458 dd &lt;- merge(diamonds, s, by = &quot;cut&quot;, sort = &quot;FALSE&quot;) head(dd) cut carat color clarity depth table price.x x y z price.y 1 Ideal 0.23 E SI2 61.5 55.0 326 3.95 3.98 2.43 3458 2 Ideal 1.02 E SI1 61.1 56.0 4675 6.51 6.45 3.96 3458 3 Ideal 1.05 F SI2 60.9 56.0 4675 6.64 6.56 4.02 3458 4 Ideal 0.38 I VS1 61.5 53.9 703 4.66 4.70 2.89 3458 5 Ideal 0.30 E VS1 62.5 54.0 703 4.27 4.32 2.69 3458 6 Ideal 1.22 J SI2 61.2 57.0 4676 6.86 6.90 4.21 3458 head(diamonds) # A tibble: 6 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 head(subset(dd, cut == &quot;Very Good&quot;)) cut carat color clarity depth table price.x x y z price.y 40249 Very Good 1.01 F VS2 61.6 57 6773 6.39 6.46 3.96 3982 40250 Very Good 0.30 F VVS1 61.8 55 783 4.32 4.35 2.68 3982 40251 Very Good 1.15 I SI2 62.0 58 4405 6.69 6.73 4.16 3982 40252 Very Good 0.90 G SI2 62.0 59 3445 6.14 6.19 3.82 3982 40253 Very Good 1.00 H VVS2 62.6 56 6249 6.36 6.39 3.99 3982 40254 Very Good 0.36 E VS2 62.1 59 789 4.55 4.59 2.84 3982 # change name of the column names(dd)[names(dd) == &#39;price.y&#39;] &lt;- &#39;average price&#39; # add additional grouping variable aggregate(price ~ cut + color, diamonds, mean, na.rm = TRUE) cut color price 1 Fair D 4291 2 Good D 3405 3 Very Good D 3470 4 Premium D 3631 5 Ideal D 2629 6 Fair E 3682 7 Good E 3424 8 Very Good E 3215 9 Premium E 3540 10 Ideal E 2598 11 Fair F 3827 12 Good F 3496 13 Very Good F 3779 14 Premium F 4325 15 Ideal F 3375 16 Fair G 4239 17 Good G 4123 18 Very Good G 3873 19 Premium G 4501 20 Ideal G 3721 21 Fair H 5136 22 Good H 4276 23 Very Good H 4535 24 Premium H 5217 25 Ideal H 3889 26 Fair I 4685 27 Good I 5079 28 Very Good I 5259 29 Premium I 5946 30 Ideal I 4452 31 Fair J 4976 32 Good J 4574 33 Very Good J 5104 34 Premium J 6295 35 Ideal J 4918 # store results in an object res &lt;- aggregate(price ~ cut + color, diamonds, mean, na.rm = TRUE) str(res) &#39;data.frame&#39;: 35 obs. of 3 variables: $ cut : Ord.factor w/ 5 levels &quot;Fair&quot;&lt;&quot;Good&quot;&lt;..: 1 2 3 4 5 1 2 3 4 5 ... $ color: Ord.factor w/ 7 levels &quot;D&quot;&lt;&quot;E&quot;&lt;&quot;F&quot;&lt;&quot;G&quot;&lt;..: 1 1 1 1 1 2 2 2 2 2 ... $ price: num 4291 3405 3470 3631 2629 ... head(res) cut color price 1 Fair D 4291 2 Good D 3405 3 Very Good D 3470 4 Premium D 3631 5 Ideal D 2629 6 Fair E 3682 # aggregate two variables, combine with &#39;cbind&#39; aggregate(cbind(price, carat) ~ cut, diamonds, mean) cut price carat 1 Fair 4359 1.0461 2 Good 3929 0.8492 3 Very Good 3982 0.8064 4 Premium 4585 0.8920 5 Ideal 3458 0.7029 aggregate(cbind(price, carat) ~ cut + color, diamonds, mean) cut color price carat 1 Fair D 4291 0.9201 2 Good D 3405 0.7445 3 Very Good D 3470 0.6964 4 Premium D 3631 0.7215 5 Ideal D 2629 0.5658 6 Fair E 3682 0.8566 7 Good E 3424 0.7451 8 Very Good E 3215 0.6763 9 Premium E 3540 0.7179 10 Ideal E 2598 0.5784 11 Fair F 3827 0.9047 12 Good F 3496 0.7759 13 Very Good F 3779 0.7410 14 Premium F 4325 0.8270 15 Ideal F 3375 0.6558 16 Fair G 4239 1.0238 17 Good G 4123 0.8509 18 Very Good G 3873 0.7668 19 Premium G 4501 0.8415 20 Ideal G 3721 0.7007 21 Fair H 5136 1.2192 22 Good H 4276 0.9147 23 Very Good H 4535 0.9159 24 Premium H 5217 1.0164 25 Ideal H 3889 0.7995 26 Fair I 4685 1.1981 27 Good I 5079 1.0572 28 Very Good I 5259 1.0474 29 Premium I 5946 1.1449 30 Ideal I 4452 0.9130 31 Fair J 4976 1.3412 32 Good J 4574 1.0995 33 Very Good J 5104 1.1332 34 Premium J 6295 1.2931 35 Ideal J 4918 1.0636 4.3 Exploratory Data Analysis (EDA) EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends. As your exploration continues, you will home in on a few particularly productive areas that youll eventually write up and communicate to others. (Quote from (Grolemund and Wickham 2016)) 4.3.1 Exploring a numerical variable You will work with the CPS1985 data from the AER package that accompanies (Kleiber and Zeileis 2008). library(AER) data(&quot;CPS1985&quot;) str(CPS1985) &#39;data.frame&#39;: 534 obs. of 11 variables: $ wage : num 5.1 4.95 6.67 4 7.5 ... $ education : num 8 9 12 12 12 13 10 12 16 12 ... $ experience: num 21 42 1 4 17 9 27 9 11 9 ... $ age : num 35 57 19 22 35 28 43 27 33 27 ... $ ethnicity : Factor w/ 3 levels &quot;cauc&quot;,&quot;hispanic&quot;,..: 2 1 1 1 1 1 1 1 1 1 ... $ region : Factor w/ 2 levels &quot;south&quot;,&quot;other&quot;: 2 2 2 2 2 2 1 2 2 2 ... $ gender : Factor w/ 2 levels &quot;male&quot;,&quot;female&quot;: 2 2 1 1 1 1 1 1 1 1 ... $ occupation: Factor w/ 6 levels &quot;worker&quot;,&quot;technical&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ sector : Factor w/ 3 levels &quot;manufacturing&quot;,..: 1 1 1 3 3 3 3 3 1 3 ... $ union : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 2 1 1 1 1 ... $ married : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 1 1 2 1 1 1 2 1 ... head(CPS1985) wage education experience age ethnicity region gender occupation 1 5.10 8 21 35 hispanic other female worker 1100 4.95 9 42 57 cauc other female worker 2 6.67 12 1 19 cauc other male worker 3 4.00 12 4 22 cauc other male worker 4 7.50 12 17 35 cauc other male worker 5 13.07 13 9 28 cauc other male worker sector union married 1 manufacturing no yes 1100 manufacturing no yes 2 manufacturing no no 3 other no no 4 other no yes 5 other yes no summary(CPS1985$wage) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.00 5.25 7.78 9.02 11.25 44.50 Lets start with the variable wage and quickly illustrate one of the dangers of using attach. The warning message tells us that, when we use the object wage, it will use the one that was created in our working directory/global environment. wage &lt;- 25 attach(CPS1985) # the warning message already tells you that it will use the object wage that was created before The following object is masked _by_ .GlobalEnv: wage mean(wage) [1] 25 mean(CPS1985$wage) [1] 9.024 detach(CPS1985) # Use the function searchpaths() to see the &#39;hierarchy&#39; So instead of using attach, just use the $ operator or export it to your global environment. If you dont want to type the full name of your data frame when using $, you can just rename it to something short such as Df and this makes typing Df$Variable a whole lot faster. # attach the data set; R knows where to find the variables wage &lt;- CPS1985$wage summary(wage) Min. 1st Qu. Median Mean 3rd Qu. Max. 1.00 5.25 7.78 9.02 11.25 44.50 is.numeric(wage) [1] TRUE mean(wage) [1] 9.024 median(wage) [1] 7.78 fivenum(wage) # Tukey&#39;s five number summary [1] 1.00 5.25 7.78 11.25 44.50 min(wage) [1] 1 max(wage) [1] 44.5 var(wage) [1] 26.41 sd(wage) [1] 5.139 hist(wage, freq = FALSE) hist(log(wage), freq = FALSE, nclass = 20, col = &quot;pink&quot;) lines(density(log(wage)), col = 4) 4.3.2 Exploring a categorical (or: factor) variable occupation &lt;- CPS1985$occupation str(occupation) # factor variable with 6 levels Factor w/ 6 levels &quot;worker&quot;,&quot;technical&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... summary(occupation) worker technical services office sales management 156 105 83 97 38 55 nlevels(occupation) [1] 6 levels(occupation) [1] &quot;worker&quot; &quot;technical&quot; &quot;services&quot; &quot;office&quot; &quot;sales&quot; [6] &quot;management&quot; To compactify the output you will rename levels 2 and 6 of the factor variable occupation. levels(occupation)[c(2, 6)] &lt;- c(&quot;techn&quot;, &quot;mgmt&quot;) summary(occupation) worker techn services office sales mgmt 156 105 83 97 38 55 Now youll learn how to construct summary tables, barplots and pie charts in R. tab &lt;- table(occupation) tab occupation worker techn services office sales mgmt 156 105 83 97 38 55 prop.table(tab) occupation worker techn services office sales mgmt 0.29213 0.19663 0.15543 0.18165 0.07116 0.10300 barplot(tab) pie(tab) pie(tab,col = gray(seq(0.4, 1.0, length = 6))) 4.3.3 Exploring two categorical (or: factor) variables gender &lt;- CPS1985$gender table(gender, occupation) occupation gender worker techn services office sales mgmt male 126 53 34 21 21 34 female 30 52 49 76 17 21 prop.table(table(gender, occupation)) occupation gender worker techn services office sales mgmt male 0.23596 0.09925 0.06367 0.03933 0.03933 0.06367 female 0.05618 0.09738 0.09176 0.14232 0.03184 0.03933 prop.table(table(gender, occupation), 2) occupation gender worker techn services office sales mgmt male 0.8077 0.5048 0.4096 0.2165 0.5526 0.6182 female 0.1923 0.4952 0.5904 0.7835 0.4474 0.3818 # use mosaic plot plot(gender ~ occupation, data = CPS1985) 4.3.4 Exploring one numerical and one categorical variable # here: apply &#39;mean(.)&#39; to &#39;log(wage)&#39; by &#39;gender&#39; tapply(wage, gender, mean) male female 9.995 7.879 options(digits=5) tapply(log(wage), list(gender, occupation), mean) worker techn services office sales mgmt male 2.1004 2.4466 1.8296 1.9553 2.1411 2.4475 female 1.6679 2.3075 1.7017 1.9311 1.5794 2.2293 # let&#39;s check these results # use subset(.) to extract part of the data s &lt;- subset(CPS1985, select=c(gender, occupation, wage)) s1 &lt;- subset(s, gender == &quot;female&quot; &amp; occupation == &quot;technical&quot;) mean(log(s1$wage)) [1] 2.3075 Now youll build an appropriate visualization tool. # see e.g. http://www.r-bloggers.com/box-plot-with-r-tutorial/ boxplot(log(wage) ~ gender) boxplot(log(wage) ~ gender + occupation, col=&quot;light blue&quot;) boxplot(log(wage) ~ gender + occupation, col=&quot;light blue&quot;, las=2) # make it a nice graph .pardefault &lt;- par(no.readonly = T) # to store the default settings of par(.) boxplot(log(wage) ~ gender + occupation, col=&quot;light blue&quot;, las=2, par(mar = c(12, 5, 4, 2) + 0.1)) par(.pardefault) 4.4 Exercises Learning check Import the data set na.txt that is available in the folder data that comes with the book. Use read.table and interpret the resulting data frame. Do you detect any problems (wrt missing values, strange observations)? Check for missing values using the is.na funtion applied to a variable from the na data set. If so, try solving those using the arguments of the read.table function. [Hint: check the argument na.strings] Check again for missing values. Make sure female is a factor variable (with two levels). Count the number of missing values per variable. (An exercise taken from (Kleiber and Zeileis 2008)) PARADE is the Sunday newspaper magazine supplementing the Sunday or weekend edition of some 500 daily newspapers in the United States of America. An important yearly feature is an article providing information on some 120150 randomly selected US citizens, indicating their profession, hometown and state, and their yearly earnings. The Parade2005 (in library AER) data contain the 2005 version, amended by a variable indicating celebrity status (motivated by substantial oversampling of celebrities in these data). For the Parade2005 data: Load the data Parade2005 from the AER package, use data(\"Parade2005\") to make the data accessible. Determine the mean earnings in California. Determine the number of individuals residing in Idaho. Determine the mean and the median earnings of celebrities. Obtain boxplots of log(earnings) stratified by celebrity Plot the density of log(earnings), use density. You will almost always receive a clean data set when you have to use it for an assignment or when its used in a course to illustrate something. In real-life, however, this will almost never be the case. There will be errors and its important that you are able to handle these or at least notice these. Garbage in = Garbage out. So for this exercise, you have to look for inconsistencies in the data and use the basic data handling steps to solve them. We adjusted the state.x77 data set and you can find it in the data folder as the file States.csv. Load the package datasets, if you havent already, and run the command ?state.x77 to obtain information on the variables. Import the csv file. Remember that there are multiple ways to import it and that a lot of things can go wrong (which is the goal of the exercise, we are terribly sorry). Its completely normal to get the error message Error ... : more columns than column names, since we did this on purpose. You will hate us now, but thank us later. Hint: \"\\t\" is used for tab. Check if the file was imported correctly. Next to delimiters for the column values, theres also something called a decimal separator. As you might have expected, we also made sure that you cannot use the default argument here. Inspect the data frame and check if you notice something weird. Which variables contain some weird looking values? Some states claim that Jesus was executed in their state and that he resurrected. Can you find out which states claim to have witnessed this miracle? Use the function pmax to fix this problem. The population estimate is not correct for some states. Set these to NA and perform mean imputation for these cases (i.e. replace it with the mean of the available cases). Do the same for other variables with impossible/improbable values. The variables that had values replaced were Murder, Illiteracy, Population, Life.Exp and Area. "],["data-viz.html", "5 Visualizing data in R 5.1 Basic plot instructions 5.2 More fancy plots 5.3 Exercises", " 5 Visualizing data in R In this chapter you will learn the basics to build insightful graphics in R. 5.1 Basic plot instructions 5.1.1 Scatterplot Your starting point is the construction of a scatterplot. Youll work with the Journals data from the AER package. # load the &#39;Journals&#39; data set in the AER package data(&quot;Journals&quot;) # scan the data head(Journals) title APEL Asian-Pacific Economic Literature SAJoEH South African Journal of Economic History CE Computational Economics MEPiTE MOCT-MOST Economic Policy in Transitional Economics JoSE Journal of Socio-Economics LabEc Labour Economics publisher society price pages charpp citations foundingyear APEL Blackwell no 123 440 3822 21 1986 SAJoEH So Afr ec history assn no 20 309 1782 22 1986 CE Kluwer no 443 567 2924 22 1987 MEPiTE Kluwer no 276 520 3234 22 1991 JoSE Elsevier no 295 791 3024 24 1972 LabEc Elsevier no 344 609 2967 24 1994 subs field APEL 14 General SAJoEH 59 Economic History CE 17 Specialized MEPiTE 2 Area Studies JoSE 96 Interdisciplinary LabEc 15 Labor names(Journals) [1] &quot;title&quot; &quot;publisher&quot; &quot;society&quot; &quot;price&quot; &quot;pages&quot; [6] &quot;charpp&quot; &quot;citations&quot; &quot;foundingyear&quot; &quot;subs&quot; &quot;field&quot; # e.g. get variable &#39;price&#39; Journals$price [1] 123 20 443 276 295 344 90 242 226 262 279 165 242 905 355 [16] 375 135 171 284 242 371 115 355 355 835 223 172 62 191 411 [31] 274 130 100 80 235 392 410 464 650 558 317 495 535 123 717 [46] 481 54 379 168 82 355 95 240 448 255 448 392 475 85 108 [61] 394 336 565 255 165 99 203 318 476 473 186 170 824 805 132 [76] 50 424 130 90 805 96 448 130 595 474 410 395 437 270 265 [91] 899 133 262 506 1140 211 799 760 442 296 272 45 614 436 481 [106] 95 357 280 142 710 490 870 1147 743 759 36 224 82 160 1170 [121] 90 742 575 163 175 120 590 2120 205 128 1539 346 1046 97 686 [136] 914 85 206 1154 45 1146 95 138 115 640 122 110 923 1000 1234 [151] 1492 810 90 177 74 113 145 590 1154 1450 1431 47 45 47 81 [166] 1010 334 190 180 1893 1400 301 1339 90 310 226 148 159 178 47 summary(Journals$price) Min. 1st Qu. Median Mean 3rd Qu. Max. 20 134 282 418 541 2120 # focus on price of journal per citation Journals$citeprice &lt;- Journals$price/Journals$citations In the next block of code, we construct a scatterplot of the number of subscriptions versus the price per citation. with(Journals, plot(log(subs), log(citeprice))) with(Journals, rug(log(subs))) # adds ticks, thus visualizing the marginal distributions of # the variables, along one or both axes of an existing plot. with(Journals, rug(log(citeprice), side = 2)) # avoid &quot;attach()&quot; and &quot;detach()&quot; plot(log(subs) ~ log(citeprice), data = Journals) R has many plotting options that allow you to flex a graph. For example, pch for the plotting character, col for the color of the plotting characters, xlim and ylim to adjust the limits on the x- and y-axis of the scatterplot. To add a legend, you can use the legend() function. plot( log(citeprice) ~ log(subs), data = Journals, pch = 19, col = &quot;blue&quot;, xlim = c(0, 8), ylim = c(-7, 4), main = &quot;Library subscriptions&quot; ) rug(log(Journals$subs)) rug(log(Journals$citeprice), side = 2) # subset data, look at journal entitled &quot;Econometrica&quot; journal &lt;- &quot;Econometrica&quot; journal_info &lt;- subset(Journals, title == journal) x.val &lt;- log(journal_info$subs) y.val &lt;- log(journal_info$citeprice) text(x.val, y.val, journal, pos = 2) legend(&quot;topright&quot;, &quot;Individual observations&quot;, col = &quot;blue&quot;, pch = 19) You can even use your own images as plotting characters! In case you want to find out how you do this, just go to http://www.statisticstoproveanything.com/2013/09/using-custom-images-as-pch-values-in-r.html 5.1.2 Saving the scatterplot as a pdf It is often very useful to directly store your customized graph as a pdf (with appropriate dimensions) in a particular directory on your machine. Next to saving a plot as pdf, you can also bmp, jpeg, png and so on (see ?png). path &lt;- tempdir() graph.path &lt;- file.path(path, &quot;myfile.pdf&quot;) pdf(graph.path, height = 5, width = 6) plot(log(citeprice)~log(subs), data = Journals, pch = 19, col = &quot;blue&quot;, xlim = c(0, 8), ylim = c(-7, 4), main = &quot;Library subscriptions&quot;) rug(log(Journals$subs)) rug(log(Journals$citeprice),side=2) journal &lt;- &quot;Econometrica&quot; journal_info &lt;- subset(Journals, title==journal) x.val &lt;- log(journal_info$subs) y.val &lt;- log(journal_info$citeprice) text(x.val, y.val, journal, pos=2) dev.off() 5.1.3 The curve() function This function draws a curve corresponding to a function over the interval [from, to]. curve(dnorm, from = -5, to = 5, col = &quot;slategray&quot;, lwd = 3, main = &quot;Density of the standard normal distribution&quot;) text(-5, 0.3, expression(f(x) == frac(1, sigma ~~ sqrt(2*pi)) ~~ e^{-frac((x - mu)^2, 2*sigma^2)}), adj = 0) ?plotmath 5.2 More fancy plots R has many dedicated packages for advanced plotting. You will work with two of them in this Section. 5.2.1 Creating graphics with ggplot2 ggplot2 is a package created and maintained by prof. Hadley Wickham, its aim is to Create Elegant Data Visualisations Using the Grammar of Graphics. Here is the basic explanation of how ggplot2 works from (Grolemund and Wickham 2016). With ggplot2, you begin a plot with the function ggplot(). ggplot() creates a coordinate system that you can add layers to. The first argument of ggplot() is the dataset to use in the graph. So ggplot(data = mpg) or ggplot(mpg) creates an empty graph. You complete your graph by adding one or more layers to ggplot(). The function geom_point() adds a layer of points to your plot, which creates a scatterplot. ggplot2 comes with many geom functions that each add a different type of layer to a plot. Each geom function in ggplot2 takes a mapping argument. This defines how variables in your dataset are mapped to visual properties. The mapping argument is always paired with aes(), and the \\(x\\) and \\(y\\) arguments of aes() specify which variables to map to the \\(x\\) and \\(y\\) axes. ggplot2 looks for the mapped variable in the data argument, in this case, mpg. library(ggplot2) # use default theme ggplot(data = mtcars, mapping = aes(x = hp, y = mpg)) + geom_point(shape=1, alpha = 1/2)+ geom_smooth() `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; # shorter ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point(shape=1, alpha = 1/2)+ geom_smooth() `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; # use black and white lay-out ggplot(mtcars, aes(x = hp, y = mpg)) + theme_bw() + geom_point(shape=1, alpha = 1/2)+ geom_smooth() `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; You can add a third variable to a two dimensional scatterplot by mapping it to an aesthetic. An aesthetic is a visual property of the objects in your plot. Aesthetics include things like the size, the shape, or the color of your points. You can display a point in different ways by changing the values of its aesthetic properties. ggplot(mtcars, aes(x = hp, y = mpg))+ geom_point(mapping = aes(color = gear)) Or you could have mapped this variable to the alpha aesthetic, which controls the transparency of the points, or the shape of the points. ggplot(mtcars, aes(x = hp, y = mpg))+ geom_point(mapping = aes(alpha = gear)) ggplot(mtcars, aes(x = hp, y = mpg))+ geom_point(mapping = aes(size = gear)) Youll now construct a boxplot of mpg per cyl using ggplot(). ggplot(mtcars, aes(factor(cyl), mpg))+ geom_boxplot() + geom_jitter() + theme_bw() Another way to code the same example p &lt;- ggplot(mtcars, aes(factor(cyl), mpg)) p + geom_boxplot() + geom_jitter() + theme_bw() 5.2.2 Fancy correlation plots You use the package corrplot to visualize correlations between variables. For more examples, see corrplot. library(corrplot) corrplot 0.84 loaded # get correlation matrix M &lt;- cor(mtcars) str(M) num [1:11, 1:11] 1 -0.852 -0.848 -0.776 0.681 ... - attr(*, &quot;dimnames&quot;)=List of 2 ..$ : chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... ..$ : chr [1:11] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; ... M mpg cyl disp hp drat wt qsec vs mpg 1.00000 -0.85216 -0.84755 -0.77617 0.681172 -0.86766 0.418684 0.66404 cyl -0.85216 1.00000 0.90203 0.83245 -0.699938 0.78250 -0.591242 -0.81081 disp -0.84755 0.90203 1.00000 0.79095 -0.710214 0.88798 -0.433698 -0.71042 hp -0.77617 0.83245 0.79095 1.00000 -0.448759 0.65875 -0.708223 -0.72310 drat 0.68117 -0.69994 -0.71021 -0.44876 1.000000 -0.71244 0.091205 0.44028 wt -0.86766 0.78250 0.88798 0.65875 -0.712441 1.00000 -0.174716 -0.55492 qsec 0.41868 -0.59124 -0.43370 -0.70822 0.091205 -0.17472 1.000000 0.74454 vs 0.66404 -0.81081 -0.71042 -0.72310 0.440278 -0.55492 0.744535 1.00000 am 0.59983 -0.52261 -0.59123 -0.24320 0.712711 -0.69250 -0.229861 0.16835 gear 0.48028 -0.49269 -0.55557 -0.12570 0.699610 -0.58329 -0.212682 0.20602 carb -0.55093 0.52699 0.39498 0.74981 -0.090790 0.42761 -0.656249 -0.56961 am gear carb mpg 0.599832 0.48028 -0.550925 cyl -0.522607 -0.49269 0.526988 disp -0.591227 -0.55557 0.394977 hp -0.243204 -0.12570 0.749812 drat 0.712711 0.69961 -0.090790 wt -0.692495 -0.58329 0.427606 qsec -0.229861 -0.21268 -0.656249 vs 0.168345 0.20602 -0.569607 am 1.000000 0.79406 0.057534 gear 0.794059 1.00000 0.274073 carb 0.057534 0.27407 1.000000 # visualize the correlation structure corrplot(M, method = &quot;circle&quot;) corrplot(M, method = &quot;square&quot;) corrplot(M, method = &quot;color&quot;) corrplot(M, type = &quot;upper&quot;) corrplot(M, type = &quot;upper&quot;, method = &quot;square&quot;) 5.3 Exercises Learning check Use the Danish fire insurance losses. Plot the arrival of losses over time. Use type= \"l\" for a line plot, label the \\(x\\) and \\(y\\)-axis, and give the plot a title using main. Do the same with instructions from ggplot2. Get inspiration from R for Data Science and use geom_line() to create the line plot. Use the data set car_price.csv available in the documentation. Import the data in R. Explore the data. Make a scatterplot of price versus income, use basic plotting instructions and use ggplot2. Add a smooth line to each of the plots (using lines to add a line to an existing plot and lowess to do scatterplot smoothing and using geom_smooth in the ggplot2 grammar). Use the mpg data set. Work through the following steps. The data contains observations collected by the US Environment Protection Agency on 38 models of car. Explore the data. Plot displ, a cars engine size, in litres on the \\(x\\)-axis and hwy, on the \\(y\\)-axis, that is the cars fuel efficiency on the highway, in miles per gallon (mpg). Now do the same but use different colors for the points, based on the class variable in mpg. Add a smooth line. "],["data-wrangling.html", "6 Data wrangling in R 6.1 Ideas from the tidyverse 6.2 Data science the data.table way 6.3 Exercises", " 6 Data wrangling in R For advanced, and fast, data handling with large R objects and lots of flexibility, two lines of work are available: the RStudio line (with Hadley Wickham) offering the packages from the tidyverse: see tidyverse the data.table line developed by Matt Dowle, see e.g. DataCamps course on data.table. Both have a very specific syntax, with a demanding learning curve. 6.1 Ideas from the tidyverse 6.1.1 A tibble instead of a data.frame Within the tidyverse tibbles are a modern take on data frames. They keep the features that have stood the test of time, and drop the features that used to be convenient but are now frustrating (i.e. converting character vectors to factors). (Quote from tibble vignette) You can use tibble to create a new tibble and as_tibble transforms an object (e.g. a data frame) into a tibble. library(ggplot2) diamonds # A tibble: 53,940 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # ... with 53,930 more rows 6.1.2 Pipes in R Read the story behind the pipe operator in R in this tutorial from DataCamp pipes in R. In R, the pipe operator is %&gt;%. You can think of this operator as being similar to the + in a ggplot2 statement, as introduced in Chapter 5. It takes the output of one statement and makes it the input of the next statement. When describing it, you can think of it as a THEN. 6.1.3 Filter observations using filter Here is a first example of using the pipe in R. library(ggplot2) library(dplyr) Attaching package: &#39;dplyr&#39; The following object is masked from &#39;package:car&#39;: recode The following objects are masked from &#39;package:stats&#39;: filter, lag The following objects are masked from &#39;package:base&#39;: intersect, setdiff, setequal, union diamonds %&gt;% filter(cut == &quot;Ideal&quot;) # A tibble: 21,551 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.23 Ideal J VS1 62.8 56 340 3.93 3.9 2.46 3 0.31 Ideal J SI2 62.2 54 344 4.35 4.37 2.71 4 0.3 Ideal I SI2 62 54 348 4.31 4.34 2.68 5 0.33 Ideal I SI2 61.8 55 403 4.49 4.51 2.78 6 0.33 Ideal I SI2 61.2 56 403 4.49 4.5 2.75 7 0.33 Ideal J SI1 61.1 56 403 4.49 4.55 2.76 8 0.23 Ideal G VS1 61.9 54 404 3.93 3.95 2.44 9 0.32 Ideal I SI1 60.9 55 404 4.45 4.48 2.72 10 0.3 Ideal I SI2 61 59 405 4.3 4.33 2.63 # ... with 21,541 more rows The code chunk above will translate to something like you take the diamonds data, then you subset the data. This is one of the most powerful things about the tidyverse. In fact, having a standardized chain of processing actions is called a pipeline. Here is another example where you now filter diamonds based on two characteristics. diamonds %&gt;% filter(cut == &quot;Ideal&quot; &amp; color == &quot;E&quot;) # A tibble: 3,903 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.26 Ideal E VVS2 62.9 58 554 4.02 4.06 2.54 3 0.7 Ideal E SI1 62.5 57 2757 5.7 5.72 3.57 4 0.59 Ideal E VVS2 62 55 2761 5.38 5.43 3.35 5 0.74 Ideal E SI2 62.2 56 2761 5.8 5.84 3.62 6 0.7 Ideal E VS2 60.7 58 2762 5.73 5.76 3.49 7 0.74 Ideal E SI1 62.3 54 2762 5.8 5.83 3.62 8 0.7 Ideal E SI1 60.9 57 2768 5.73 5.76 3.5 9 0.6 Ideal E VS1 61.7 55 2774 5.41 5.44 3.35 10 0.7 Ideal E SI1 62.7 55 2774 5.68 5.74 3.58 # ... with 3,893 more rows diamonds %&gt;% filter(cut == &quot;Ideal&quot; &amp; color %in% c(&quot;E&quot;, &quot;D&quot;)) # A tibble: 6,737 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.3 Ideal D SI1 62.5 57 552 4.29 4.32 2.69 3 0.3 Ideal D SI1 62.1 56 552 4.3 4.33 2.68 4 0.26 Ideal E VVS2 62.9 58 554 4.02 4.06 2.54 5 0.7 Ideal E SI1 62.5 57 2757 5.7 5.72 3.57 6 0.59 Ideal E VVS2 62 55 2761 5.38 5.43 3.35 7 0.74 Ideal E SI2 62.2 56 2761 5.8 5.84 3.62 8 0.7 Ideal E VS2 60.7 58 2762 5.73 5.76 3.49 9 0.71 Ideal D SI2 62.3 56 2762 5.73 5.69 3.56 10 0.74 Ideal E SI1 62.3 54 2762 5.8 5.83 3.62 # ... with 6,727 more rows 6.1.4 Summarize variables using summarize The code chunk below will translate to something like you take the diamonds data, then you subset the data and then you calculate mean and standard deviation of these data. diamonds %&gt;% filter(cut == &quot;Ideal&quot;) %&gt;% summarize(mean = mean(price), std_dev = sd(price)) # A tibble: 1 x 2 mean std_dev &lt;dbl&gt; &lt;dbl&gt; 1 NA NA 6.1.5 Summarize based on groupings of another variable So, here is what youd like to do. # base R way with aggregate aggregate(price ~ cut, diamonds, mean) cut price 1 Fair 4358.8 2 Good 3928.9 3 Very Good 3982.0 4 Premium 4584.5 5 Ideal 3457.6 How can you do this with the pipe? diamonds %&gt;% group_by(cut) %&gt;% summarize(mean = mean(price)) `summarise()` ungrouping output (override with `.groups` argument) # A tibble: 5 x 2 cut mean &lt;ord&gt; &lt;dbl&gt; 1 Fair 4359. 2 Good 3929. 3 Very Good NA 4 Premium NA 5 Ideal NA Now you want to group by multiple variables. diamonds %&gt;% group_by(cut, color) %&gt;% summarize(price = mean(price)) `summarise()` regrouping output by &#39;cut&#39; (override with `.groups` argument) # A tibble: 35 x 3 # Groups: cut [5] cut color price &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; 1 Fair D 4291. 2 Fair E 3682. 3 Fair F 3827. 4 Fair G 4239. 5 Fair H 5136. 6 Fair I 4685. 7 Fair J 4976. 8 Good D 3405. 9 Good E 3424. 10 Good F 3496. # ... with 25 more rows Now you want to calculate multiple metrics. diamonds %&gt;% group_by(cut) %&gt;% summarize(price = mean(price), carat = mean(carat)) `summarise()` ungrouping output (override with `.groups` argument) # A tibble: 5 x 3 cut price carat &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Fair 4359. 1.05 2 Good 3929. 0.849 3 Very Good NA 0.806 4 Premium NA 0.892 5 Ideal NA 0.703 And finally, multiple metrics and multiple grouping variables. diamonds %&gt;% group_by(cut, color) %&gt;% summarize(price = mean(price), carat = mean(carat)) `summarise()` regrouping output by &#39;cut&#39; (override with `.groups` argument) # A tibble: 35 x 4 # Groups: cut [5] cut color price carat &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Fair D 4291. 0.920 2 Fair E 3682. 0.857 3 Fair F 3827. 0.905 4 Fair G 4239. 1.02 5 Fair H 5136. 1.22 6 Fair I 4685. 1.20 7 Fair J 4976. 1.34 8 Good D 3405. 0.745 9 Good E 3424. 0.745 10 Good F 3496. 0.776 # ... with 25 more rows 6.1.6 Joining tibbles Now you want to add the mean price and mean carat per cut to the original tibble. You use the variable cut as the key to identify observations. d &lt;- diamonds %&gt;% group_by(cut) %&gt;% summarize(price = mean(price), carat = mean(carat)) `summarise()` ungrouping output (override with `.groups` argument) new_diamonds &lt;- diamonds %&gt;% inner_join(d, by = &quot;cut&quot;) View(diamonds) View(new_diamonds) 6.2 Data science the data.table way 6.2.1 Speed junkies love data.table data.table is a package designed for speed junkies. The R data.table package is rapidly making its name as the number one choice for handling large datasets in R. It extends and exchanges the functionality of the basic data.frame in R. The syntax is different and youll have to get used to it. A data.table cheat sheet is available here. 6.2.2 What is a data.table? Here you see some basic illustrations with the diamonds data. library(data.table) Attaching package: &#39;data.table&#39; The following objects are masked from &#39;package:dplyr&#39;: between, first, last library(ggplot2) str(diamonds) tibble [53,940 x 10] (S3: tbl_df/tbl/data.frame) $ carat : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... $ cut : Ord.factor w/ 5 levels &quot;Fair&quot;&lt;&quot;Good&quot;&lt;..: 5 4 2 4 2 3 3 3 1 3 ... $ color : Ord.factor w/ 7 levels &quot;D&quot;&lt;&quot;E&quot;&lt;&quot;F&quot;&lt;&quot;G&quot;&lt;..: 2 2 2 6 7 7 6 5 2 5 ... $ clarity: Ord.factor w/ 8 levels &quot;I1&quot;&lt;&quot;SI2&quot;&lt;&quot;SI1&quot;&lt;..: 2 3 5 4 2 6 7 3 4 5 ... $ depth : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... $ table : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ... $ price : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ... $ x : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... $ y : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... $ z : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... diamonds_DT &lt;- data.table(diamonds) diamonds_DT # notice intelligent printing of this DT carat cut color clarity depth table price x y z 1: 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2: 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3: 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4: 0.29 Premium I VS2 62.4 58 334 4.20 4.23 2.63 5: 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 --- 53936: 0.72 Ideal D SI1 60.8 57 2757 5.75 5.76 3.50 53937: 0.72 Good D SI1 63.1 55 2757 5.69 5.75 3.61 53938: 0.70 Very Good D SI1 62.8 60 2757 5.66 5.68 3.56 53939: 0.86 Premium H SI2 61.0 58 2757 6.15 6.12 3.74 53940: 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 summary(diamonds_DT$cut) Fair Good Very Good Premium Ideal 1610 4906 12082 13791 21551 6.2.3 Identify keys Instead of using subset from the base R, you will use the setkey to extract the observations you want to have. # key is used to index the data.table and will provide the extra speed setkey(diamonds_DT, cut) tables() NAME NROW NCOL MB COLS KEY 1: diamonds_DT 53,940 10 3 carat,cut,color,clarity,depth,table,... cut Total: 3MB diamonds_DT[J(&quot;Ideal&quot;), ] carat cut color clarity depth table price x y z 1: 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2: 0.23 Ideal J VS1 62.8 56 340 3.93 3.90 2.46 3: 0.31 Ideal J SI2 62.2 54 344 4.35 4.37 2.71 4: 0.30 Ideal I SI2 62.0 54 348 4.31 4.34 2.68 5: 0.33 Ideal I SI2 61.8 55 403 4.49 4.51 2.78 --- 21547: 0.79 Ideal I SI1 61.6 56 2756 5.95 5.97 3.67 21548: 0.71 Ideal E SI1 61.9 56 2756 5.71 5.73 3.54 21549: 0.71 Ideal G VS1 61.4 56 2756 5.76 5.73 3.53 21550: 0.72 Ideal D SI1 60.8 57 2757 5.75 5.76 3.50 21551: 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 # more than one column can be set as key setkey(diamonds_DT, cut, color) tables() NAME NROW NCOL MB COLS KEY 1: diamonds_DT 53,940 10 3 carat,cut,color,clarity,depth,table,... cut,color Total: 3MB # access rows according to both keys, use function &#39;J&#39; diamonds_DT[J(&quot;Ideal&quot;, &quot;E&quot;), ] carat cut color clarity depth table price x y z 1: 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2: 0.26 Ideal E VVS2 62.9 58 554 4.02 4.06 2.54 3: 0.70 Ideal E SI1 62.5 57 2757 5.70 5.72 3.57 4: 0.59 Ideal E VVS2 62.0 55 2761 5.38 5.43 3.35 5: 0.74 Ideal E SI2 62.2 56 2761 5.80 5.84 3.62 --- 3899: 0.70 Ideal E SI1 61.7 55 2745 5.71 5.74 3.53 3900: 0.51 Ideal E VVS1 61.9 54 2745 5.17 5.11 3.18 3901: 0.56 Ideal E VVS1 62.1 56 2750 5.28 5.29 3.28 3902: 0.77 Ideal E SI2 62.1 56 2753 5.84 5.86 3.63 3903: 0.71 Ideal E SI1 61.9 56 2756 5.71 5.73 3.54 diamonds_DT[J(&quot;Ideal&quot;, c(&quot;E&quot;, &quot;D&quot;)), ] carat cut color clarity depth table price x y z 1: 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2: 0.26 Ideal E VVS2 62.9 58 554 4.02 4.06 2.54 3: 0.70 Ideal E SI1 62.5 57 2757 5.70 5.72 3.57 4: 0.59 Ideal E VVS2 62.0 55 2761 5.38 5.43 3.35 5: 0.74 Ideal E SI2 62.2 56 2761 5.80 5.84 3.62 --- 6733: 0.51 Ideal D VVS2 61.7 56 2742 5.16 5.14 3.18 6734: 0.51 Ideal D VVS2 61.3 57 2742 5.17 5.14 3.16 6735: 0.81 Ideal D SI1 61.5 57 2748 6.00 6.03 3.70 6736: 0.72 Ideal D SI1 60.8 57 2757 5.75 5.76 3.50 6737: 0.75 Ideal D SI2 62.2 55 2757 5.83 5.87 3.64 # what would be the alternative with base R? subset(diamonds, diamonds$cut == &quot;Ideal&quot; &amp;&amp; diamonds$color == c(&quot;E&quot;, &quot;D&quot;)) # A tibble: 53,940 x 10 carat cut color clarity depth table price x y z &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 # ... with 53,930 more rows 6.2.4 Alternative and faster ways to aggregate Instead of using aggregate from the base R, you will identify the by variable(s). # base R way with aggregate aggregate(price ~ cut, diamonds, mean) cut price 1 Fair 4358.8 2 Good 3928.9 3 Very Good 3982.0 4 Premium 4584.5 5 Ideal 3457.6 system.time(aggregate(price ~ cut, diamonds, mean)) user system elapsed 0.01 0.00 0.02 # aggregation with data.table # will go faster thanks to indexing diamonds_DT[ , mean(price), by=cut] cut V1 1: Fair 4358.8 2: Good 3928.9 3: Very Good NA 4: Premium NA 5: Ideal NA system.time(diamonds_DT[ , mean(price), by=cut]) user system elapsed 0 0 0 # give variable names in the create date.table diamonds_DT[ , list(price = mean(price)), by=cut] cut price 1: Fair 4358.8 2: Good 3928.9 3: Very Good NA 4: Premium NA 5: Ideal NA # aggregate on multiple columns diamonds_DT[ , mean(price), by=list(cut,color)] cut color V1 1: Fair D 4291.1 2: Fair E 3682.3 3: Fair F 3827.0 4: Fair G 4239.3 5: Fair H 5135.7 6: Fair I 4685.4 7: Fair J 4975.7 8: Good D 3405.4 9: Good E 3423.6 10: Good F 3495.8 11: Good G 4123.5 12: Good H 4276.3 13: Good I 5078.5 14: Good J 4574.2 15: Very Good D 3470.5 16: Very Good E 3214.7 17: Very Good F 3778.8 18: Very Good G 3872.8 19: Very Good H 4535.4 20: Very Good I NA 21: Very Good J 5103.5 22: Premium D 3631.3 23: Premium E NA 24: Premium F 4324.9 25: Premium G 4500.7 26: Premium H 5216.7 27: Premium I 5946.2 28: Premium J 6294.6 29: Ideal D 2629.1 30: Ideal E NA 31: Ideal F 3374.9 32: Ideal G 3720.7 33: Ideal H 3889.3 34: Ideal I 4452.0 35: Ideal J 4918.2 cut color V1 # aggregate multiple arguments diamonds_DT[ , list(price = mean(price), carat = mean(carat)), by = cut] cut price carat 1: Fair 4358.8 1.04614 2: Good 3928.9 0.84918 3: Very Good NA 0.80638 4: Premium NA 0.89195 5: Ideal NA 0.70284 diamonds_DT[ , list(price = mean(price), carat = mean(carat), caratSum = sum(carat)), by=cut] cut price carat caratSum 1: Fair 4358.8 1.04614 1684.3 2: Good 3928.9 0.84918 4166.1 3: Very Good NA 0.80638 9742.7 4: Premium NA 0.89195 12301.0 5: Ideal NA 0.70284 15146.8 # multiple metrics and multiple grouping variables diamonds_DT[ , list(price = mean(price), carat = mean(carat)), by = list(cut, color)] cut color price carat 1: Fair D 4291.1 0.92012 2: Fair E 3682.3 0.85661 3: Fair F 3827.0 0.90471 4: Fair G 4239.3 1.02382 5: Fair H 5135.7 1.21917 6: Fair I 4685.4 1.19806 7: Fair J 4975.7 1.34118 8: Good D 3405.4 0.74452 9: Good E 3423.6 0.74513 10: Good F 3495.8 0.77593 11: Good G 4123.5 0.85090 12: Good H 4276.3 0.91473 13: Good I 5078.5 1.05722 14: Good J 4574.2 1.09954 15: Very Good D 3470.5 0.69642 16: Very Good E 3214.7 0.67632 17: Very Good F 3778.8 0.74096 18: Very Good G 3872.8 0.76680 19: Very Good H 4535.4 0.91595 20: Very Good I NA 1.04695 21: Very Good J 5103.5 1.13322 22: Premium D 3631.3 0.72155 23: Premium E NA 0.71774 24: Premium F 4324.9 0.82704 25: Premium G 4500.7 0.84149 26: Premium H 5216.7 1.01645 27: Premium I 5946.2 1.14494 28: Premium J 6294.6 1.29309 29: Ideal D 2629.1 0.56577 30: Ideal E NA 0.57840 31: Ideal F 3374.9 0.65583 32: Ideal G 3720.7 0.70071 33: Ideal H 3889.3 0.79952 34: Ideal I 4452.0 0.91303 35: Ideal J 4918.2 1.06359 cut color price carat 6.2.5 Joining data.tables How to join data.tables? # join two data.tables d &lt;- diamonds_DT[ , list(price = mean(price), carat = mean(carat)), by = cut] d cut price carat 1: Fair 4358.8 1.04614 2: Good 3928.9 0.84918 3: Very Good NA 0.80638 4: Premium NA 0.89195 5: Ideal NA 0.70284 setkey(diamonds_DT, cut) dmerge &lt;- diamonds_DT[d] dmerge carat cut color clarity depth table price x y z i.price 1: 0.75 Fair D SI2 64.6 57 2848 5.74 5.72 3.70 4358.8 2: 0.71 Fair D VS2 56.9 65 2858 5.89 5.84 3.34 4358.8 3: 0.90 Fair D SI2 66.9 57 2885 6.02 5.90 3.99 4358.8 4: 1.00 Fair D SI2 69.3 58 2974 5.96 5.87 4.10 4358.8 5: 1.01 Fair D SI2 64.6 56 3003 6.31 6.24 4.05 4358.8 --- 53936: 0.71 Ideal J SI1 60.6 57 2700 5.78 5.83 3.52 NA 53937: 0.81 Ideal J VS2 62.1 56 2708 5.92 5.97 3.69 NA 53938: 0.84 Ideal J VS2 61.1 57 2709 6.09 6.12 3.73 NA 53939: 0.82 Ideal J VS2 61.6 56 2741 6.00 6.04 3.71 NA 53940: 0.83 Ideal J VS2 62.3 55 2742 6.01 6.03 3.75 NA i.carat 1: 1.04614 2: 1.04614 3: 1.04614 4: 1.04614 5: 1.04614 --- 53936: 0.70284 53937: 0.70284 53938: 0.70284 53939: 0.70284 53940: 0.70284 6.3 Exercises Learning check (An exercise taken from (Kleiber and Zeileis 2008)) PARADE is the Sunday newspaper magazine supplementing the Sunday or weekend edition of some 500 daily newspapers in the United States of America. An important yearly feature is an article providing information on some 120150 randomly selected US citizens, indicating their profession, hometown and state, and their yearly earnings. The Parade2005 (in library AER) data contain the 2005 version, amended by a variable indicating celebrity status (motivated by substantial oversampling of celebrities in these data). For the Parade2005 data and by using %&gt;% answer the following questions. Load the data Parade2005 from the AER package, use data(\"Parade2005\") to make the data accessible. Determine the mean earnings in California. Determine the number of individuals residing in Idaho. Determine the mean and the median earnings of celebrities. "],["probs.html", "7 Working with probability distributions in R 7.1 Discrete distributions 7.2 Continuous distributions 7.3 Exercises", " 7 Working with probability distributions in R In this Section youll learn how to work with probability distributions in R. Before you start, it is important to know that for many standard distributions R has 4 crucial functions: Density: e.g. dexp, dgamma, dlnorm Quantile: e.g. qexp, qgamma, qlnorm Cdf: e.g. pexp, pgamma, plnorm Simulation: e.g. rexp, rgamma, rlnorm The parameters of the distribution are then specified in the arguments of these functions. Below are some examples from Katriens course on Loss Models at KU Leuven. 7.1 Discrete distributions 7.1.1 The binomial distribution nSim &lt;- 100 p &lt;- 0.3 n &lt;- 6 # generate &#39;nSim&#39; obs. from Bin(n,p) distribution data_binom &lt;- rbinom(nSim, n, p) # calculate mean and variance mean(data_binom) # empirical mean [1] 1.65 var(data_binom) # empirical variance [1] 1.1995 n*p # theoretical mean [1] 1.8 n*p*(1-p) # theoretical variance [1] 1.26 # visualize range &lt;- seq(-1,n,1/1000) plot(ecdf(data_binom)) # ecdf lines(range,pbinom(range, n, p), col = &#39;red&#39;) # cdf par(mfrow=c(1,2)) plot(0:n, dbinom(0:n, n, p), type = &#39;h&#39;) # pdf plot(prop.table(table(data_binom))) par(mfrow=c(1,1)) 7.1.2 The Poisson distribution nSim &lt;- 100 lambda &lt;- 1 # generate &#39;nSim&#39; observations from Poisson(\\lambda) distribution data_pois &lt;- rpois(nSim, lambda) # calculate mean and variance mean(data_pois) # empirical mean [1] 1.07 var(data_pois) # empirical variance [1] 0.81323 lambda # theoretical mean [1] 1 lambda # theoretical variance [1] 1 # visualize range &lt;- seq(0,8, 1/1000) plot(ecdf(data_pois)) # ecdf lines(range,ppois(range, lambda), col = &#39;red&#39;) # cdf par(mfrow=c(1,2)) plot(0:8, dpois(0:8, lambda), type = &#39;h&#39;) # pdf plot(prop.table(table(data_pois))) par(mfrow=c(1,1)) 7.2 Continuous distributions 7.2.1 The normal distribution # evaluate cdf of N(0,1) in 0 pnorm(0, mean=0, sd=1) [1] 0.5 # or shorter pnorm(0, 0, 1) [1] 0.5 # 95% quantile of N(0,1) qnorm(0.95, mean=0, sd=1) [1] 1.6449 # a set of quantiles qnorm(c(0.025, 0.05, 0.5, 0.95, 0.975), 0, 1) [1] -1.9600 -1.6449 0.0000 1.6449 1.9600 # generate observations from N(0,1) x &lt;- rnorm(10000, mean=10, sd=1) # visualize hist(x, probability=TRUE, nclass=55, col=&quot;pink&quot;) curve(dnorm(x, mean=10, sd=1), xlim=range(x), col=&quot;black&quot;,add=TRUE) 7.2.2 The gamma distribution # check parametrization of gamma density in R ? dgamma # grid of points to evaluate the gamma density x &lt;- seq(from = 0, to = 20, by = 0.001) # choose a color palette colors &lt;- c(&quot;#000000&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) # shape and rate parameter combinations shown in the plot shape &lt;- c(1, 2, 3) rate &lt;- c(0.5, 0.5, 0.5) plot(x, dgamma(x, shape = shape[1], rate = rate[1]), type=&#39;l&#39;, xlab =&#39;x&#39;, ylab=&#39;Gamma density&#39;, main=&#39;Effect of the shape parameter on the Gamma density&#39;) for(i in 2:length(shape)){ lines(x, dgamma(x, shape = shape[i], rate = rate[i]), col=colors[i]) } # add a legend legend(&quot;topright&quot;, paste(&quot;shape = &quot;, shape, &quot;, rate = &quot;, rate, sep=&quot;&quot;), col = colors, lty=1) 7.3 Exercises Learning check Generating random numbers, tossing coins. Set your seed to 1 and generate 10 random numbers (between 0 and 1) using runif and save these numbers in an object called random_numbers. Using the function ifelse and the object random_numbers simulate coin tosses. Hint: if random_numbers is bigger than 0.5 then the result is head, otherwise it is tail. Another way of generating random coin tosses is by using the rbinom function. Set the seed again to 1 and simulate with this function 10 coin tosses. Simulate samples from a normal distribution. Imagine a population in which the average height is 1.7m with a standard deviation of 0.1. Using rnorm simulate the height of 100 people and save it in an object called heights. To get an idea of the values in heights apply the function summary to it. What is the probability that a person will be smaller or equal to 1.9m? Use prnorm. What is the probability that a person will be taller or equal to 1.6m? Use pnorm. The waiting time (in minutes) at a doctors clinic follows an exponential distribution with a rate parameter of 1/50. Use the function rexp to simulate the waiting time of 30 people at the doctors office. What is the probability that a person will wait less than 10 minutes? Use pexp. What is the waiting time average? "],["functions.html", "8 Writing functions in R 8.1 Conditionals and control flow 8.2 Logical operators 8.3 Conditional statements 8.4 Loops 8.5 Functions in R 8.6 The apply family 8.7 Exercises", " 8 Writing functions in R 8.1 Conditionals and control flow In this chapter youll learn about relational operators to compare R objects and logical operators to combine logical expressions. Next, youll use this knowledge to build conditional statements. [Quote from DataCamps `Intermediate R course] Make sure not to mix up == and =, where the latter is used for assignment and the former checks equality (see Section 3.2). 3 == (2 + 1) [1] TRUE &quot;intermediate&quot; != &quot;r&quot; [1] TRUE TRUE != FALSE [1] TRUE &quot;Rchitect&quot; != &quot;rchitect&quot; [1] TRUE Now youll focus on inequalities. (1 + 2) &gt; 4 [1] FALSE &quot;dog&quot; &lt; &quot;Cats&quot; [1] FALSE TRUE &lt;= FALSE [1] FALSE For string comparison, R determines the greater than relationship based on alphabetical order. Also, keep in mind that TRUE corresponds to 1 in R, and FALSE coerces to 0 behind the scenes. Rs relational operators also work on vectors. katrien &lt;- c(19, 22, 4, 5, 7) katrien &gt; 5 [1] TRUE TRUE FALSE FALSE TRUE jan &lt;- c(34, 55, 76, 25, 4) jan &lt;= 30 [1] FALSE FALSE FALSE TRUE TRUE 8.2 Logical operators We already discussed the logical operators in Chapter 3, so you already know the basics. Can you predict what the outcome will be of the following statements? TRUE &amp; TRUE FALSE | TRUE 5 &lt;= 5 &amp; 2 &lt; 3 3 &lt; 4 | 7 &lt; 6 The logical operators applied to vectors katrien &gt; 5 &amp; jan &lt;= 30 [1] FALSE FALSE FALSE FALSE TRUE 8.3 Conditional statements We encounter conditional statements on a daily basis. Just think of the following behavior that perfectly illustrates this. FunnyJoke &lt;- TRUE if(FunnyJoke) { cat(&quot;Hahahaha =D&quot;) } else { cat(&quot;*Cough* awkward silence...&quot;) } Hahahaha =D FunnyJoke &lt;- FALSE if(FunnyJoke) { cat(&quot;Hahahaha =D&quot;) } else { cat(&quot;*Cough* awkward silence...&quot;) } *Cough* awkward silence... With this, we illustrated the basic concept of a conditional statement and with it, a conditional execution. We basically read this code as if this statement is TRUE then we do this. If it is FALSE, we do another thing. With the above example, it is quite obvious. If the joke is funny, we laugh. If not, an awkward silence follows. We can also just use the if without specifying else (make sure your volume is not at max when running this code and wearing a headphone). SoundCheck &lt;- TRUE if(SoundCheck) { beepr::beep(8) } We can also use the function ifelse, which is a vectorized version of the if/else construct (Douglas et al. 2020). FunnyJoke &lt;- c(TRUE, FALSE) CondTRUE &lt;- &quot;Hahaha =D&quot; CondFALSE &lt;- &quot;*Cough* awkward silence...&quot; ifelse(FunnyJoke, CondTRUE, CondFALSE) [1] &quot;Hahaha =D&quot; &quot;*Cough* awkward silence...&quot; When using the ifelse() function, you have to make sure that you know what it does. ifelse(test, yes, no) returns a vector of the same length as test, with elements yes[i] when test[i] is TRUE and no[i] when test[i] is FALSE. This might sound confusing, so lets just illustrate it with an example. a &lt;- -1:1 b &lt;- rep(&quot;&lt; 0&quot;, length(a)) c &lt;- rep(&quot;&gt;= 0&quot;, length(a)) ifelse(a &lt; 0, b, c) [1] &quot;&lt; 0&quot; &quot;&gt;= 0&quot; &quot;&gt;= 0&quot; We can also use multiple nested if  else statements. ShellExec &lt;- function(x) { # replacement for shell.exe (doesn&#39;t exist on MAC) if (exists(&quot;shell.exec&quot;,where = &quot;package:base&quot;)) return(base::shell.exec(x)) comm &lt;- paste(&quot;open&quot;,x) return(system(comm)) } Samples &lt;- 1:3 SampleVideo &lt;- sample(Samples, 1, FALSE) OpenVid &lt;- if(SampleVideo == 1) { &quot;https://www.youtube.com/watch?v=dQw4w9WgXcQ&quot; } else if(SampleVideo == 2) { &quot;https://www.youtube.com/watch?v=CsGYh8AacgY&amp;t=12s&quot; } else { &quot;https://www.youtube.com/watch?v=1g9sneS2MF4&quot; } ShellExec(OpenVid) SampleVideo &lt;- sample(Samples[!1:3 %in% SampleVideo], 1, FALSE) 8.4 Loops When we have to perform a repetitive task, loops are the way to go. This repetitive task is then executed within each loop for a specified number of times or until a specific condition is met. Within R, there are three main types of loops, the for loop, the while loop and the repeat loop @cite(introRbookdown). 8.4.1 The for loop The for loop is the most well known loop and also one of the most common loops within programming languages. The following example illustrates how for loops work. for(i in 1:5) { print(i) } [1] 1 [1] 2 [1] 3 [1] 4 [1] 5 In this example, we loop over the vector 1:5 and i is used as an index or counter. The inner part of the for loop is then repeated for every iteration. Breaking it down to the basics, we tell R to repeat the inner part (i.e. the code within the curly braces {}) 5 times (length(1:5)), taking subsequent values of the vector 1:5 in every loop and to keep track of the iteration with i. Hence, for the first iteration, i gets the first value of the vector 1:5 which is 1 and runs the code print(i). In the second iteration, it gets the value 2 and so on. The counter is not limited to i, but basically every valid name can be used. Can you guess what the following output will be? primes &lt;- c(2, 3, 5, 7, 11, 13) # loop version 1 for (p in primes) { print(p) } [1] 2 [1] 3 [1] 5 [1] 7 [1] 11 [1] 13 When working with loops, we have to be careful to not get stuck in an infinite loop. Even though this is more common in while loops, this can also happen in for loops. For example, when we adjust the counter i in the inner part, we get stuck in an infinite loop. One way to get out of this loop, is by using the break statement. LoopIter &lt;- 1 for(i in 1:3) { i &lt;- 1 print(i) LoopIter &lt;- LoopIter + 1 if(LoopIter &gt; 5) break } [1] 1 [1] 1 [1] 1 Consequently, when using a for loop, make sure that you dont adjust the counter variable i that keeps track of the iterations! To make sure that everything goes as expected in the inner part of your for loop, you can always run a test by first creating i &lt;- 1 (or whatever value of the vector you are looping over) for example and then run the inner part. To save the results of each iteration, we can basically use any data structure. A list is most often used, as its structure is quite intuitive to save the results of our loop. To save our results in this way, we create an empty list. Lets work with the data base birthwt from the package MASS library(MASS) Attaching package: &#39;MASS&#39; The following object is masked from &#39;package:dplyr&#39;: select bwt &lt;- with(birthwt, { race &lt;- factor(race, labels = c(&quot;white&quot;, &quot;black&quot;, &quot;other&quot;)) ptd &lt;- factor(ptl &gt; 0) ftv &lt;- factor(ftv) levels(ftv)[-(1:2)] &lt;- &quot;2+&quot; data.frame(low = factor(low), age, lwt, race, smoke = (smoke &gt; 0), ptd, ht = (ht &gt; 0), ui = (ui &gt; 0), ftv) }) Cols &lt;- colnames(bwt) DescrVars &lt;- list() for(Var in Cols) { x &lt;- bwt[, Var] DescrVars[[Var]] &lt;- if(is.numeric(x)) { mean(x) } else { table(x) } } DescrVars $low x 0 1 130 59 $age [1] 23.238 $lwt [1] 129.81 $race x white black other 96 26 67 $smoke x FALSE TRUE 115 74 $ptd x FALSE TRUE 159 30 $ht x FALSE TRUE 177 12 $ui x FALSE TRUE 161 28 $ftv x 0 1 2+ 100 47 42 When using loops, it is also important to try to be as efficiently as possible by avoiding repetition within the loop. Try to avoid recomputing the same thing within the same loop. This is why we use x &lt;- bwt[, Var] in the above example and not DescrVars &lt;- list() for(Var in Cols) { DescrVars[[Var]] &lt;- if(is.numeric(bwt[, Var])) { mean(bwt[, Var]) } else { table(bwt[, Var]) } } By avoiding repetition, the code will run faster and this will be more important when working with large inner functions or large data sets. In addition, the chance of a coding error is also smaller in this case. 8.4.1.1 Nested for loops We can also use a for loop within another for loop and thats what we called nested for loops. For example, Alph = matrix(letters[1:4], nrow = 2) for(j in 1:ncol(Alph)) { for(i in 1:nrow(Alph)) { cat(paste0(&quot;\\nAlph[&quot;,i, &quot;, &quot;, j , &quot;] = &quot;, Alph[i, j], &quot;\\n&quot;)) } } Alph[1, 1] = a Alph[2, 1] = b Alph[1, 2] = c Alph[2, 2] = d To save the results of nested loops, lists are the perfect type of object. In order to be able to save the results in an empty list (which does not have the nested structure yet), we have to use names when saving these in the slots of the list. Results &lt;- list() for(j in seq_len(ncol(Alph))) { for(i in seq_len(nrow(Alph))) { Results[[paste0(&quot;Column&quot;, j)]][[paste0(&quot;Row&quot;, i)]] &lt;- paste0(&quot;Alph[&quot;,i, &quot;, &quot;, j , &quot;] = &quot;, Alph[i, j]) } } Results $Column1 $Column1$Row1 [1] &quot;Alph[1, 1] = a&quot; $Column1$Row2 [1] &quot;Alph[2, 1] = b&quot; $Column2 $Column2$Row1 [1] &quot;Alph[1, 2] = c&quot; $Column2$Row2 [1] &quot;Alph[2, 2] = d&quot; Alternatively, we can already specify the dimensions of the nested list using the following code. Results &lt;- list(vector(&quot;list&quot;, 2), vector(&quot;list&quot;, 2)) # Create an empty nested list for(j in seq_len(ncol(Alph))) { for(i in seq_len(nrow(Alph))) { Results[[j]][[i]] &lt;- paste0(&quot;Alph[&quot;,i, &quot;, &quot;, j , &quot;] = &quot;, Alph[i, j]) } } Results [[1]] [[1]][[1]] [1] &quot;Alph[1, 1] = a&quot; [[1]][[2]] [1] &quot;Alph[2, 1] = b&quot; [[2]] [[2]][[1]] [1] &quot;Alph[1, 2] = c&quot; [[2]][[2]] [1] &quot;Alph[2, 2] = d&quot; 8.4.2 The while loop With a while loop, you repeat the inner part until some logical condition is met. while(LogicalCondition) { DoSomething(&quot;ButIDon&#39;tKnowWhat&quot;) } Take, for example, that we have 64 tasks on our to-do list and we want to get at least 5 tasks done for today. todo &lt;- 10 while (todo &gt; 5) { cat(todo, &quot;tasks left\\n&quot;) todo &lt;- todo - 1 } 10 tasks left 9 tasks left 8 tasks left 7 tasks left 6 tasks left todo [1] 5 Another example is the following. Tired &lt;- TRUE Sleep &lt;- 0 while(Tired) { Sleep &lt;- Sleep + 1 if(Sleep &gt;= 8) Tired &lt;- FALSE } Tired [1] FALSE As with for loops, we can also use the break statement to stop the loop. Tired &lt;- TRUE Sleep &lt;- 0 while(Tired) { Sleep &lt;- Sleep + 1 if(Sleep &gt;= 8) break } Sleep [1] 8 The example below puts many things together (taken from DataCamps `Intermediate R course). i &lt;- 1 while (i &lt;= 10) { print(3 * i) if ( (3 * i) %% 8 == 0) { break } i &lt;- i + 1 } [1] 3 [1] 6 [1] 9 [1] 12 [1] 15 [1] 18 [1] 21 [1] 24 The repeat loop is similar to the while loop, but has no conditional check (Douglas et al. 2020). So in this case, you have to make sure that you dont get stuck in an infinite loop by building in a break statement. i &lt;- 0 repeat { print(&quot;Eat&quot;) print(&quot;Sleep&quot;) print(&quot;Rave&quot;) print(&quot;Repeat&quot;) i &lt;- i + 1 if(i &gt;= 3) break } [1] &quot;Eat&quot; [1] &quot;Sleep&quot; [1] &quot;Rave&quot; [1] &quot;Repeat&quot; [1] &quot;Eat&quot; [1] &quot;Sleep&quot; [1] &quot;Rave&quot; [1] &quot;Repeat&quot; [1] &quot;Eat&quot; [1] &quot;Sleep&quot; [1] &quot;Rave&quot; [1] &quot;Repeat&quot; 8.5 Functions in R The book An Introduction to R (Douglas et al. 2020) gives a splendid (and funny) description of functions: Functions are your loyal servants, waiting patiently to do your bidding to the best of their ability. Theyre made with the utmost care and attention  though sometimes may end up being something of a Frankensteins monster - with an extra limb or two and a head put on backwards. But no matter how ugly they may be, theyre completely faithful to you. Theyre also very stupid. 8.5.1 Using a function Throughout this book, we have already been using functions, but what exactly is a function? Lets start from a classic one, the mean() function. ? mean help(mean) args(mean) function (x, ...) NULL Basically, we give input to this function and it will return the arithmetic mean. In mean(x, trim = 0, na.rm = FALSE, ...) x is required; if you do not specify it, R will throw an error. This is a good thing, because you of course need to tell R of what object you want to compute the mean. Here, x is an argument of the function mean() and with mean(x = a), you tell R to compute the mean of the object a. trim and na.rm are optional arguments: they have a default value which is used if the arguments are not explicitly specified. [Quote from DataCamps Intermediate R course.] You will now use the mean function as follows katrien &lt;- c(2, 9, 6, 8, NA) mean(katrien) [1] NA mean(katrien, na.rm = TRUE) [1] 6.25 Functions return objects that can be used elsewhere. As such, you can use a function within function. katrien &lt;- c(2, 9, 6, 8, NA) jan &lt;- c(0, 3, 2, NA, 5) katrien - jan [1] 2 6 4 NA NA mean(abs(katrien - jan), na.rm = TRUE) [1] 4 8.5.2 Write your own function The ability to use self-written functions makes R incredibly powerful, efficient, convenient and elegant. Once you learn how to write your own functions, programming with R will be even more comfortable and productive (Venables, Smith, and R Core Team 2020). Creating a function in R is basically the assignment of a function object to a variable. Thats why you will use the assignment operator &lt;-. The basic form of a function is given by (slightly altered example and text from (Douglas et al. 2020)): NameFunction &lt;- function(Argument1, Argument2) { Expression } With this code, we are telling R that we want to create an object of type function and that we want to give it the name NameFunction. As input, it takes Argument1 and Argument2 and it then performs the inner part of the function which is denoted by Expression. As always, its easier to comprehend this with an example. my_sqrt &lt;- function(x) { sqrt(x) } # Use the function my_sqrt(12) [1] 3.4641 my_sqrt(16) [1] 4 sum_abs &lt;- function(x, y) { abs(x) + abs(y) } # Use the function sum_abs(-2, 3) [1] 5 You can define default argument values in your own R functions as well. Here you see an example. my_sqrt &lt;- function(x, print_info = TRUE) { y &lt;- sqrt(x) if (print_info) { print(paste(&quot;sqrt&quot;, x, &quot;equals&quot;, y)) } return(y) } # some calls of the function my_sqrt(16) [1] &quot;sqrt 16 equals 4&quot; [1] 4 my_sqrt(16, FALSE) [1] 4 my_sqrt(16, TRUE) [1] &quot;sqrt 16 equals 4&quot; [1] 4 R works in a vectorized way and most of Rs functions are vectorized. This means that the function will operate on all elements of a vector. Check this by calling the function my_sqrt on an input vector. v &lt;- c(16, 25, 36) my_sqrt(v) [1] &quot;sqrt 16 equals 4&quot; &quot;sqrt 25 equals 5&quot; &quot;sqrt 36 equals 6&quot; [1] 4 5 6 With this simple self-written function, we can also illustrated why functions can be called stupid and why we should always include checks for the arguments of your self-written function. With regard to this issue, (Douglas et al. 2020) gives an accurate description of what it means to be a programmer: Remember two things: the intelligence of code comes from the coder, not the computer and functions need exact instructions to work. my_sqrt(-1) Warning in sqrt(x): NaNs produced [1] &quot;sqrt -1 equals NaN&quot; [1] NaN We can fix this issue by adding some checks to our self-written function. Its up to you to decide on the checks that you add to your function and also what action will be taken. With the function stop() the code will be stopped and with the function warning() the function will proceed, but a warning will be given when using the function. my_sqrt &lt;- function(x, print_info = TRUE) { if(!is.complex(x)) { if(x &lt; 0) { stop(&quot;Computer says no.&quot;) } } y &lt;- sqrt(x) if (print_info) { print(paste(&quot;sqrt&quot;, x, &quot;equals&quot;, y)) } return(y) } my_sqrt(-1) Error in my_sqrt(-1): Computer says no. x &lt;- 3i^2 # my_sqrt(x) [1] &quot;sqrt -9+0i equals 0+3i&quot; [1] 0+3i The example also illustrates that you can specify the error/warning message yourself. Even if its not the case for this example, make sure that these messages are informative. Both for the user and yourself. This way, the user will know what went wrong and why the function doesnt work. This will save you a lot of time in the future, when you will use code that you have written a long time ago. 8.5.3 The  argument You probably already encountered the ellipsis argument ... in the help files of some functions (see ?plot for example). This argument allows you to pass the arguments of your self-written function to another function (within your function). f &lt;- function(x, y, ...) { plot(x, y, ...) lmFit &lt;- lm(y ~ x) muHat &lt;- fitted(lmFit) muHat &lt;- muHat[order(x)] x &lt;- x[order(x)] lines(x, muHat, lwd = 2) } x &lt;- rnorm(1e2) y &lt;- 2 * x + rnorm(1e2, 0, 0.5) f(x, y, pch = 16, col = &quot;red&quot;, xlab = &quot;x-values&quot;, ylab = &quot;y-values&quot;, main = &quot;OLS plot&quot;) 8.5.4 Function environments (advanced) This part is a bit more advanced, so dont worry if you dont fully understand everything in this section. Know that it suffices to have a general understanding of this chapter and that you can read it again once you are more familiar with writing functions. For an in-depth discussion on (function) environments, we refer the reader to the Advanced R book (Wickham 2019). Remember the sandbox analogy? Our global environment is our big sandbox and the objects that we create in this sandbox, stay in it. When we define a function and give it a name, we create a new object in our sandbox. Just like any other object, we can call it by using its name. Keeping the sandbox analogy, we can look at a function as a mini-sandbox within our larger sandbox, just like other objects. Functions, however, are fundamentally different from other objects. Not only because of their class, but also because of how they operate and their interaction with the different environments in R. The inner part of the function (i.e. the part in the curly braces {}) has its own environment, called the execution environment. Further, functions have what they call a fresh start principle (Wickham 2019). This means that, every time we run a function, a new execution environment is created in which the inner part is executed. Translating this to the sandbox analogy, we start with a clean mini-sandbox every time we run the function and after the function has been completed, we throw away everything that was computed and created in this mini-sandbox. f &lt;- function() { a &lt;- 1 a &lt;- a + 1 cat(&quot;\\nenvironment:&quot;) print(environment()) a } f() environment:&lt;environment: 0x0000000060f528e8&gt; [1] 2 f() environment:&lt;environment: 0x0000000060fb53a0&gt; [1] 2 In this example we see that a new (execution) environment is created every time the function is called. Due to this fresh start principle, R makes sure that results are not contaminated due to previous function calls and that we get the same result when we run the function with the same arguments. rm(x, a) f &lt;- function(x) { x * a } f(2) Error in f(2): object &#39;a&#39; not found a &lt;- 3 f(2) [1] 6 8.6 The apply family Whenever youre using a for loop, you might want to revise your code and see whether you can use a member of the apply family instead. [Quote from DataCamps `Intermediate R course] These functions may be a bit confusing at first, but once you get the hang of them they will be your best friends. With this family of functions, we are able to avoid loops, to run our code faster (certainly when using the parallel versions) and the risk of having an error is considerably lower. As (Douglas et al. 2020) accurately and honestly put it: If you can, use the apply version. Theres nothing worse than realizing there was a small, tiny, seemingly meaningless mistake in a loop which weeks, months or years down the line has propagated into a huge mess. We strongly recommend trying to use the apply functions whenever possible. Just look at the following example which is a simplification of what might happen in reality. This all has to do with our (execution) environment. In the for loop this is our global environment and thats why the original x gets overwritten. Conversely, when using sapply() we have the fresh start principle. Every loop within sapply starts with a clean environment. x &lt;- 3 for(i in 1:3) { x = x + i print(x) } [1] 4 [1] 6 [1] 9 x &lt;- 3 sapply(1:3, function(i) { x &lt;- x + i x }) [1] 4 5 6 The first member of this family is the function apply. It must be applied on an array and is also applicable to a matrix and a data.frame. It takes the following arguments: first argument: matrix you are working with second argument: margin to apply the function over (1 for rows and 2 for columns) third argument: function you want to apply. Here you see how it works. my_matrix &lt;- matrix(1:9, nrow = 3) # sum the rows apply(my_matrix, 1, sum) [1] 12 15 18 # sum the columns apply(my_matrix, 2, sum) [1] 6 15 24 # impute a missing observation in my_matrix my_matrix[2,1] &lt;- NA apply(my_matrix, 1, sum) [1] 12 NA 18 apply(my_matrix, 1, sum, na.rm = TRUE) [1] 12 13 18 You already encountered a first illustration of tapply in Chapter 4. The tapply function is useful when we need to break up a vector into groups defined by some classifying factor, compute a function on the subsets, and return the results in a convenient form. wages &lt;- c(5500, 3500, 6500, 7500) gender &lt;- c(&quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;) region &lt;- c(&quot;North&quot;, &quot;South&quot;, &quot;North&quot;, &quot;South&quot;) salary &lt;- data.frame(wages, gender, region) tapply(salary$wages, salary$gender, mean) F M 4500 7000 tapply(salary$wages, list(salary$gender, salary$region), mean) North South F 5500 3500 M 6500 7500 lapply works by applying a function to each element of a list and returning the results as a list. To return the result of lapply as a vector instead of a list, use sapply. my_list &lt;- list(A = matrix(1:9, 3), B = 1:5, C = matrix(1:4, 2), D = 2) my_list $A [,1] [,2] [,3] [1,] 1 4 7 [2,] 2 5 8 [3,] 3 6 9 $B [1] 1 2 3 4 5 $C [,1] [,2] [1,] 1 3 [2,] 2 4 $D [1] 2 lapply(my_list, sum) $A [1] 45 $B [1] 15 $C [1] 10 $D [1] 2 sapply(my_list, sum) A B C D 45 15 10 2 my_names &lt;- c(&quot;Katrien&quot;, &quot;Jan&quot;, &quot;Leen&quot;) lapply(my_names, nchar) [[1]] [1] 7 [[2]] [1] 3 [[3]] [1] 4 sapply(my_names, nchar) Katrien Jan Leen 7 3 4 mapply applies a function to each element of multiple lists. 8.7 Exercises Learning check Create a function that will return the sum of 2 integers. The function var in R calculates the unbiased variance estimator, given a random sample of data. Write a function variance which returns the biased or unbiased estimate of the variance, depending on the value of the argument bias which can be TRUE or FALSE. By default the function variance should produce the same result as var. Formulas: unbiased = \\(\\frac{1}{n-1}\\sum_i (x_i-\\bar{x})^2\\) and biased = \\(\\frac{1}{n}\\sum_i(x_i-\\bar{x})^2\\) where \\(\\bar{x}=\\frac{1}{n} \\sum_i x_i\\). Create a function that given a vector and an integer will return how many times the integer appears inside the vector. Create a function that given a vector will print by screen the mean and the standard deviation, it will optionally also print the median. "],["optimization.html", "9 Optimization in R 9.1 Find the root of a function 9.2 Find the maximum of a function 9.3 Do Maximum Likelihood Estimation (MLE)", " 9 Optimization in R Actuaries often write functions (e.g. a likelihood) that have to be optimized. Here youll get to know some R functionalities to do optimization. 9.1 Find the root of a function Consider the function \\(f: x \\mapsto x^2-3^{-x}\\). What is the root of this function over the interval \\([0,1]\\)? # in one line of code uniroot(function(x) x^2-3^(-x), lower=0, upper=1) $root [1] 0.68602 $f.root [1] -8.0827e-06 $iter [1] 4 $init.it [1] NA $estim.prec [1] 6.1035e-05 ? uniroot # in more lines of code f &lt;- function(x){ x^2-3^(-x) } # calculate root opt &lt;- uniroot(f, lower=0, upper=1) # check arguments names(opt) [1] &quot;root&quot; &quot;f.root&quot; &quot;iter&quot; &quot;init.it&quot; &quot;estim.prec&quot; # evaluate &#39;f(.)&#39; in the root f(opt$root) [1] -8.0827e-06 # visualize the function range &lt;- seq(-2, 2, by=0.2) plot(range, f(range), type=&quot;l&quot;) points(opt$root, f(opt$root), pch=20) segments(opt$root, -7, opt$root, 0, lty=2) segments(-3, 0, opt$root, 0, lty=2) 9.2 Find the maximum of a function You look for the maximum of the beta density with a given set of parameters. # visualize the density shape1 &lt;- 3 shape2 &lt;- 2 x &lt;- seq(from=0, to=1, by=0.01) curve(dbeta(x,shape1,shape2), xlim=range(x)) opt_beta &lt;- optimize(dbeta, interval=c(0,1), maximum=TRUE, shape1, shape2) points(opt_beta$maximum, opt_beta$objective, pch=20, cex=1.5) segments(opt_beta$maximum, 0, opt_beta$maximum, opt_beta$objective, lty=2) 9.3 Do Maximum Likelihood Estimation (MLE) nsim &lt;- 10000 x &lt;- rgamma(nsim, shape=3, rate=1.5) # calculate log-likelihood f &lt;- function(p,x){ -sum(dgamma(x, shape=p[1], rate=p[2], log=TRUE)) } nlm(f, c(1, 1), x=x) Warning in dgamma(x, shape = p[1], rate = p[2], log = TRUE): NaNs produced Warning in nlm(f, c(1, 1), x = x): NA/Inf replaced by maximum positive value Warning in dgamma(x, shape = p[1], rate = p[2], log = TRUE): NaNs produced Warning in nlm(f, c(1, 1), x = x): NA/Inf replaced by maximum positive value Warning in dgamma(x, shape = p[1], rate = p[2], log = TRUE): NaNs produced Warning in nlm(f, c(1, 1), x = x): NA/Inf replaced by maximum positive value $minimum [1] 14400 $estimate [1] 2.9613 1.4909 $gradient [1] -0.0023231 0.0024499 $code [1] 1 $iterations [1] 14 # same example, now use &#39;optim&#39; optim(c(1, 1), f, x=x) $par [1] 2.9622 1.4914 $value [1] 14400 $counts function gradient 65 NA $convergence [1] 0 $message NULL "],["lms.html", "10 Linear Regression Models in R 10.1 A simple linear regression model 10.2 A multiple linear regression model 10.3 Exercises", " 10 Linear Regression Models in R Your journey as a model builder in R will start from studying linear models and the use of the lm function. 10.1 A simple linear regression model You analyze Ford dealership data as registered in Milwaukee, September/October 1990. Data on 62 credit card applicants are available, including the car purchase price \\(Y\\) and the applicants annual income \\(X\\). Data are available in the .csv file car_price. You first load the data. path &lt;- file.path(&#39;data&#39;) path.car &lt;- file.path(path, &quot;car_price.csv&quot;) car_price &lt;- read.csv(path.car) Then you explore the data. attach(car_price) summary(price) Min. 1st Qu. Median Mean 3rd Qu. Max. 7200 11850 14500 15416 18500 28000 summary(income) Min. 1st Qu. Median Mean 3rd Qu. Max. 15000 30500 40950 45190 55500 102000 # average mean(price) [1] 15416 mean(income) [1] 45190 # standard deviation sd(price) [1] 5040.4 sd(income) [1] 20873 # the 5-th and 95-th percentiles quantile(price, c(0.05, 0.95)) 5% 95% 7830 25900 quantile(income, c(0.05, 0.95)) 5% 95% 18150 85700 # histograms of price and income # density histogram for &#39;price&#39; hist(price, br = 20, xlim = c(5000, 30000), col=&quot;grey&quot;, freq=FALSE) lines(density(price), col=4) # frequency histogram for &#39;income/1000&#39; hist(income/1000, br=10, xlab=&quot;income (in $000&#39;s)&quot;, xlim=c(0, 120), col=&quot;grey&quot;) # scatter plot &#39;income/1000&#39; versus &#39;price&#39; plot(income/1000, price, pch=21, cex=1.2, xlab=&quot;income (in $000&#39;s)&quot;) detach(car_price) Explore the data with ggplot. library(&quot;ggplot2&quot;) ggplot(car_price, aes(x = income/1000, y = price)) + theme_bw() + geom_point(shape=1, alpha = 1/2) + geom_smooth() `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; You will now fit a simple regression model with income as predictor to purchase price. That is: \\[\\begin{eqnarray*} Y_i &amp;=&amp; \\beta_0+\\beta_1 \\cdot x_i +\\epsilon_i, \\end{eqnarray*}\\] where \\(Y_i\\) is the car price for observation \\(i\\), \\(x_i\\) the corresponding income and \\(\\epsilon_i\\) an error term. \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) the slope. Recall that fitting a (simple) linear regression model implies minimizing the residual sum of squares. That is: \\[\\begin{eqnarray*} (\\hat{\\beta}_0,\\ \\hat{\\beta}_1) = \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n \\left(Y_i - (\\beta_0+\\beta_1 \\cdot x_{i})\\right)^2, \\end{eqnarray*}\\] and the fitted values are then specified as \\(\\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1\\cdot x_{i}\\). The corresponding residuals are then defined as \\(\\hat{\\epsilon}_i = y_i - \\hat{y}_i\\). You assign the output of the lm function to the object lm1. lm1 &lt;- lm(price ~ income, data = car_price) summary(lm1) Call: lm(formula = price ~ income, data = car_price) Residuals: Min 1Q Median 3Q Max -5365 -1185 -251 1334 6284 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.87e+03 7.50e+02 7.82 9.8e-11 *** income 2.11e-01 1.51e-02 14.01 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2460 on 60 degrees of freedom Multiple R-squared: 0.766, Adjusted R-squared: 0.762 F-statistic: 196 on 1 and 60 DF, p-value: &lt;2e-16 # check attributes of object &#39;lm1&#39; names(lm1) [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; # some useful stuff: &#39;coefficients&#39;, &#39;residuals&#39;, &#39;fitted.values&#39;, &#39;model&#39; lm1$coef (Intercept) income 5866.33390 0.21132 lm1$residuals 1 2 3 4 5 6 7 8 -719.290 1146.822 4329.836 3258.062 -3649.431 -483.570 4144.823 -1206.051 9 10 11 12 13 14 15 16 -664.585 480.710 6284.374 -1040.023 -4383.403 -1262.670 -4421.372 4273.216 17 18 19 20 21 22 23 24 -1373.761 33.583 -1051.346 -1102.387 1392.034 -1836.192 54.316 167.554 25 26 27 28 29 30 31 32 -257.092 -877.824 3124.091 1427.921 367.554 -2355.177 77.679 -2462.670 33 34 35 36 37 38 39 40 1159.977 -274.078 732.384 -2707.167 -2566.817 -1120.805 5129.669 2895.864 41 42 43 44 45 46 47 48 1111.484 2197.613 1971.301 -883.403 3129.669 1744.823 1026.006 142.908 49 50 51 52 53 54 55 56 -3653.428 190.119 -1616.575 -1085.318 -1866.501 -4379.739 1980.943 -40.023 57 58 59 60 61 62 3735.415 -1000.472 -672.079 -245.684 -1087.233 -5364.585 lm1$fitted.values 1 2 3 4 5 6 7 8 9 10 14319.3 9353.2 9670.2 14741.9 11149.4 22983.6 16855.2 12206.1 15164.6 14319.3 11 12 13 14 15 16 17 18 19 20 21715.6 12840.0 11783.4 13262.7 27421.4 10726.8 23173.8 11466.4 13051.3 19602.4 21 22 23 24 25 26 27 28 29 30 14108.0 9036.2 12945.7 10832.4 18757.1 17277.8 15375.9 11572.1 10832.4 16855.2 31 32 33 34 35 36 37 38 39 40 15122.3 13262.7 12840.0 19074.1 15967.6 11107.2 12966.8 14720.8 20870.3 10304.1 41 42 43 44 45 46 47 48 49 50 25688.5 19602.4 12628.7 11783.4 20870.3 16855.2 13474.0 18757.1 26153.4 16009.9 51 52 53 54 55 56 57 58 59 60 9416.6 13685.3 17066.5 19179.7 24019.1 12840.0 15164.6 17700.5 11572.1 12945.7 61 62 15587.2 15164.6 To visualize this linear model fit you can use the built-in plot function, applied to object lm1. # use built-in plot function # you may have noticed that we have used the function plot with all kinds of arguments: # one or two variables, a data frame, and now a linear model fit; # in R jargon plot is a generic function; it checks for the kind of object that you # are plotting and then calls the appropriate (more specialized) function to do the work. plot(lm1) Or you can construct your own plots, e.g. by adding the least squares line to the scatter plot. # add the regression line to the scatter plot plot(car_price$income, car_price$price, pch=21, cex=1.2, xlab = &quot;income&quot;, main = &quot;Simple linear regression&quot;) # add LS line like this abline(lm1, col=&quot;blue&quot;, lwd=2) # or like this abline(lm1$coefficients[1], lm1$coefficients[2]) Similarly, you can illustrate the fit with ggplot. ggplot(car_price, aes(x = income, y = price)) + theme_bw() + geom_point(shape=1, alpha = 1/2) + geom_smooth()+geom_abline(intercept = lm1$coef[1], slope = lm1$coef[2], colour=&quot;red&quot;, size=1.25) `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The least squares fit minimizes the sum of the squares of the vertical distances between the observed response values and the least squares line (or plane). You now graphically illustrate what vertical distance means. plot(car_price$income, car_price$price, pch=21, cex=1.2, xlab = &quot;income&quot;, main = &quot;Simple linear regression&quot;) abline(lm1, col = &quot;blue&quot;, lwd=2) segments(car_price$income, car_price$price, car_price$income, lm1$fitted.values, lty=1) You now return to the summary(lm1) and try to understand the (rest of the) output that is printed. summary(lm1) Call: lm(formula = price ~ income, data = car_price) Residuals: Min 1Q Median 3Q Max -5365 -1185 -251 1334 6284 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.87e+03 7.50e+02 7.82 9.8e-11 *** income 2.11e-01 1.51e-02 14.01 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2460 on 60 degrees of freedom Multiple R-squared: 0.766, Adjusted R-squared: 0.762 F-statistic: 196 on 1 and 60 DF, p-value: &lt;2e-16 Recall that in a general linear model \\(Y = X\\beta + \\epsilon\\) where \\(E[\\epsilon]=0\\) and \\(\\text{Var}(\\epsilon)=\\sigma^2 I\\) with \\(I\\) the identity matrix, the following estimator is used for the variance of the error terms \\[\\begin{eqnarray*} s^2 &amp;=&amp; \\frac{1}{n-(p+1)}(Y-X\\hat{\\beta})^{&#39;}(Y-X\\hat{\\beta}), \\end{eqnarray*}\\] where \\(\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)^{&#39;}\\). You can recognize this in the output of lm1 as follows error.SS &lt;- sum(lm1$resid^2) error.SS [1] 362851718 sqrt(error.SS/(nrow(car_price)-2)) [1] 2459.2 The proportion of the variability in the data that is explained by the regression model is \\[\\begin{eqnarray*} R^2 &amp;=&amp; \\frac{\\text{Regression SS}}{\\text{Total SS}} \\\\ &amp;=&amp; \\frac{\\sum_{i=1}^n (\\hat{Y}_i-\\bar{Y})^2}{\\sum_{i=1}^n (Y_i-\\bar{Y})^2} \\\\ &amp;=&amp; \\frac{\\sum_{i=1}^n (Y_i-\\bar{Y})^2 - \\sum_{i=1}^n (Y_i-\\hat{Y})^2}{\\sum_{i=1}^n (Y_i-\\bar{Y})^2}. \\end{eqnarray*}\\] attach(car_price) total.SS &lt;- sum((price-mean(price))^2) total.SS [1] 1549743871 error.SS &lt;- sum(lm1$resid^2) error.SS [1] 362851718 # R^2? (total.SS-error.SS)/total.SS [1] 0.76586 detach(car_price) Finally, the output of lm1 displays the result of a so-called \\(F\\) test, constructed as follows: attach(car_price) # anova table in R? anova(lm1) Analysis of Variance Table Response: price Df Sum Sq Mean Sq F value Pr(&gt;F) income 1 1.19e+09 1.19e+09 196 &lt;2e-16 *** Residuals 60 3.63e+08 6.05e+06 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # F-statistic in anova and in output lm1? lm0 &lt;- lm(price ~ 1) error0.SS &lt;- sum(lm0$resid^2) # calculate F-statistic F &lt;- ((anova(lm0)$&quot;Sum Sq&quot;)-(anova(lm1)$&quot;Sum Sq&quot;[2]))/(anova(lm1)$&quot;Mean Sq&quot;[2]) F [1] 196.26 # critical values qf(0.95, 1, 60) [1] 4.0012 1-pf(F, 1, 60) [1] 0 detach(car_price) 10.2 A multiple linear regression model Youll now move on from simple to multiple linear regression. You model the data by McDonald and Schwing (1973) published in Technometrics. The sampled data consists of variables obtained for year 1960 for 60 Standard Metropolitan Statistical Areas (SMSA) in the US. The goal is to relate mortality in these SMSA to explanatory variables. For each sample area, you have information concerning the age-adjusted mortality rate for all causes, expressed as deaths per 100,000 population. This will be your response variable. The list of explanatory variables is: weather-related variables: prec: average annual precipitation in inches jant: average January temperature in degrees F jult: average July temperature in degrees F humid: annual average % relative humidity at 1 pm scocio-economic characteristics: ovr65: % of 1960 SMSA population aged 65 and older popn: average household size educ : median school years completed by those over 22 hous : % of housing units which are sound and with all facilities educ : median school years completed by those over 22 dens : population per sq mile in urbanized areas, 1960 nonw : % of non-white population in urbanized areas, 1960 wwdrk : % of employed in white collar occupations poor : % of families with income less than $3,000 pollutants: hc : relative pollution potential of hydrocarbons nox : relative pollution potential of oxides of nitrogen so2: relative pollution potential of sulfur dioxides First, youll load the data. path &lt;- file.path(&#39;data&#39;) path.mort &lt;- file.path(path, &quot;pollution.csv&quot;) mort_poll &lt;- read.csv(path.mort) Then, youll explore the data. attach(mort_poll) summary(mort_poll) prec jant jult ovr65 popn Min. :10.0 Min. :12.0 Min. :63.0 Min. : 5.60 Min. :2.92 1st Qu.:32.8 1st Qu.:27.0 1st Qu.:72.0 1st Qu.: 7.67 1st Qu.:3.21 Median :38.0 Median :31.5 Median :74.0 Median : 9.00 Median :3.27 Mean :37.4 Mean :34.0 Mean :74.6 Mean : 8.80 Mean :3.26 3rd Qu.:43.2 3rd Qu.:40.0 3rd Qu.:77.2 3rd Qu.: 9.70 3rd Qu.:3.36 Max. :60.0 Max. :67.0 Max. :85.0 Max. :11.80 Max. :3.53 educ hous dens nonw wwdrk Min. : 9.0 Min. :66.8 Min. :1441 Min. : 0.80 Min. :33.8 1st Qu.:10.4 1st Qu.:78.4 1st Qu.:3104 1st Qu.: 4.95 1st Qu.:43.2 Median :11.1 Median :81.2 Median :3567 Median :10.40 Median :45.5 Mean :11.0 Mean :80.9 Mean :3876 Mean :11.87 Mean :46.1 3rd Qu.:11.5 3rd Qu.:83.6 3rd Qu.:4520 3rd Qu.:15.65 3rd Qu.:49.5 Max. :12.3 Max. :90.7 Max. :9699 Max. :38.50 Max. :59.7 poor hc nox so2 humid Min. : 9.4 Min. : 1.0 Min. : 1.0 Min. : 1.0 Min. :38.0 1st Qu.:12.0 1st Qu.: 7.0 1st Qu.: 4.0 1st Qu.: 11.0 1st Qu.:55.0 Median :13.2 Median : 14.5 Median : 9.0 Median : 30.0 Median :57.0 Mean :14.4 Mean : 37.9 Mean : 22.6 Mean : 53.8 Mean :57.7 3rd Qu.:15.2 3rd Qu.: 30.2 3rd Qu.: 23.8 3rd Qu.: 69.0 3rd Qu.:60.0 Max. :26.4 Max. :648.0 Max. :319.0 Max. :278.0 Max. :73.0 mort Min. : 791 1st Qu.: 898 Median : 944 Mean : 940 3rd Qu.: 983 Max. :1113 # get correlation matrix round(cor(mort_poll), 4) prec jant jult ovr65 popn educ hous dens nonw prec 1.0000 0.0922 0.5033 0.1011 0.2634 -0.4904 -0.4908 -0.0035 0.4132 jant 0.0922 1.0000 0.3463 -0.3981 -0.2092 0.1163 0.0149 -0.1001 0.4538 jult 0.5033 0.3463 1.0000 -0.4340 0.2623 -0.2385 -0.4150 -0.0610 0.5753 ovr65 0.1011 -0.3981 -0.4340 1.0000 -0.5091 -0.1389 0.0650 0.1620 -0.6378 popn 0.2634 -0.2092 0.2623 -0.5091 1.0000 -0.3951 -0.4106 -0.1843 0.4194 educ -0.4904 0.1163 -0.2385 -0.1389 -0.3951 1.0000 0.5522 -0.2439 -0.2088 hous -0.4908 0.0149 -0.4150 0.0650 -0.4106 0.5522 1.0000 0.1819 -0.4103 dens -0.0035 -0.1001 -0.0610 0.1620 -0.1843 -0.2439 0.1819 1.0000 -0.0057 nonw 0.4132 0.4538 0.5753 -0.6378 0.4194 -0.2088 -0.4103 -0.0057 1.0000 wwdrk -0.2973 0.2380 -0.0214 -0.1177 -0.4257 0.7032 0.3387 -0.0318 -0.0044 poor 0.5066 0.5653 0.6193 -0.3098 0.2599 -0.4033 -0.6807 -0.1629 0.7049 hc -0.5318 0.3508 -0.3565 -0.0205 -0.3882 0.2868 0.3868 0.1203 -0.0259 nox -0.4873 0.3210 -0.3377 -0.0021 -0.3584 0.2244 0.3483 0.1653 0.0184 so2 -0.1069 -0.1078 -0.0993 0.0172 -0.0041 -0.2343 0.1180 0.4321 0.1593 humid -0.0773 0.0679 -0.4528 0.1124 -0.1357 0.1765 0.1219 -0.1250 -0.1180 mort 0.5095 -0.0300 0.2770 -0.1746 0.3573 -0.5110 -0.4268 0.2655 0.6437 wwdrk poor hc nox so2 humid mort prec -0.2973 0.5066 -0.5318 -0.4873 -0.1069 -0.0773 0.5095 jant 0.2380 0.5653 0.3508 0.3210 -0.1078 0.0679 -0.0300 jult -0.0214 0.6193 -0.3565 -0.3377 -0.0993 -0.4528 0.2770 ovr65 -0.1177 -0.3098 -0.0205 -0.0021 0.0172 0.1124 -0.1746 popn -0.4257 0.2599 -0.3882 -0.3584 -0.0041 -0.1357 0.3573 educ 0.7032 -0.4033 0.2868 0.2244 -0.2343 0.1765 -0.5110 hous 0.3387 -0.6807 0.3868 0.3483 0.1180 0.1219 -0.4268 dens -0.0318 -0.1629 0.1203 0.1653 0.4321 -0.1250 0.2655 nonw -0.0044 0.7049 -0.0259 0.0184 0.1593 -0.1180 0.6437 wwdrk 1.0000 -0.1852 0.2037 0.1600 -0.0685 0.0607 -0.2848 poor -0.1852 1.0000 -0.1298 -0.1025 -0.0965 -0.1522 0.4105 hc 0.2037 -0.1298 1.0000 0.9838 0.2823 -0.0202 -0.1772 nox 0.1600 -0.1025 0.9838 1.0000 0.4094 -0.0459 -0.0774 so2 -0.0685 -0.0965 0.2823 0.4094 1.0000 -0.1026 0.4259 humid 0.0607 -0.1522 -0.0202 -0.0459 -0.1026 1.0000 -0.0885 mort -0.2848 0.4105 -0.1772 -0.0774 0.4259 -0.0885 1.0000 # create dataframes # weather related vars mort_poll_1 &lt;- data.frame(mort, prec, jant, jult, humid) # socio-economic vars mort_poll_2 &lt;- data.frame(mort, ovr65, popn, educ, hous, dens, nonw, wwdrk, poor) # pollution effects mort_poll_3 &lt;- data.frame(mort, hc, nox, so2) # matrix scatterplots pairs(mort_poll_1, cex=1, pch=19) pairs(mort_poll_2, cex=0.5, pch=19) pairs(mort_poll_3, cex=1, pch=19) detach(mort_poll) First, you fit a rather simple linear model to explain mort. That is \\[\\begin{eqnarray*} Y &amp;=&amp; \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i, \\end{eqnarray*}\\] where \\(\\beta_0\\) is the intercept, \\(x_1\\) the educ and \\(x_2\\) the so2. attach(mort_poll) lm1 &lt;- lm(mort ~ educ + so2) summary(lm1) Call: lm(formula = mort ~ educ + so2) Residuals: Min 1Q Median 3Q Max -136.56 -30.37 -7.73 34.48 148.80 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1274.602 89.761 14.20 &lt; 2e-16 *** educ -32.017 8.020 -3.99 0.00019 *** so2 0.318 0.107 2.97 0.00432 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 50.6 on 57 degrees of freedom Multiple R-squared: 0.36, Adjusted R-squared: 0.338 F-statistic: 16.1 on 2 and 57 DF, p-value: 2.96e-06 detach(mort_poll) You inspect the analysis-of-variance table for this linear model lm1. That is anova(lm1) Analysis of Variance Table Response: mort Df Sum Sq Mean Sq F value Pr(&gt;F) educ 1 59612 59612 23.26 1.1e-05 *** so2 1 22642 22642 8.84 0.0043 ** Residuals 57 146054 2562 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 attach(mort_poll) lm0 &lt;- lm(mort ~ 1) lm_educ &lt;- lm(mort ~ educ) anova(lm_educ) Analysis of Variance Table Response: mort Df Sum Sq Mean Sq F value Pr(&gt;F) educ 1 59612 59612 20.5 3e-05 *** Residuals 58 168696 2909 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 F_educ &lt;- ((anova(lm0)$&quot;Sum Sq&quot;)-(anova(lm_educ)$&quot;Sum Sq&quot;[2]))/(anova(lm1)$&quot;Mean Sq&quot;[3]) F_educ [1] 23.265 F_so2 &lt;- ((anova(lm_educ)$&quot;Sum Sq&quot;[2])-(anova(lm1)$&quot;Sum Sq&quot;[3]))/(anova(lm1)$&quot;Mean Sq&quot;[3]) F_so2 [1] 8.8363 detach(mort_poll) You will now use the object lm1 to construct confidence and prediction intervals for a single observation. Given a set of predictor values in \\(x_0\\) the predicted response is \\(\\hat{y}_0 = x_0^{&#39;}\\hat{\\beta}\\). The uncertainty in predicting the mean response is \\(\\text{Var}(x_0^{&#39;}\\hat{\\beta})\\) whereas the uncertainty in predicting the value of an observation is \\(\\text{Var}(x_0^{&#39;}\\hat{\\beta}+\\epsilon_0)\\). attach(mort_poll) x0 &lt;- data.frame(educ = 10, so2 = exp(2)) predict(lm1, x0, interval = &quot;confidence&quot;) fit lwr upr 1 956.78 932.55 981.01 predict(lm1, x0, interval = &quot;prediction&quot;) fit lwr upr 1 956.78 852.56 1061 detach(mort_poll) For a grid of educ values, when so2 is fixed, this goes as follows: attach(mort_poll) grid &lt;- seq(8, 15, 0.1) x.new &lt;- data.frame(educ = grid, so2 = exp(2)) p &lt;- predict(lm1, x.new, se=TRUE, interval=&quot;prediction&quot;) p1 &lt;- predict(lm1, x.new, se=TRUE, interval=&quot;confidence&quot;) # use `matplot` to plot the columns of one matrix against the columnsof another matplot(grid, p$fit, lty=c(1,2,2), col=c(&quot;black&quot;, &quot;red&quot;, &quot;red&quot;), type = &quot;l&quot;, xlab = &quot;educ&quot;, ylab = &quot;mort&quot;, main = &quot;Predicted mort over a range of educ, log(so2)=2&quot;) matlines(grid, p1$fit, lty = c(1, 2, 2), col = c(&quot;black&quot;, &quot;blue&quot;, &quot;blue&quot;)) rug(educ) # for an explanation wrt different shapes, see # http://stats.stackexchange.com/questions/85560/shape-of-confidence-interval-for-p# redicted-values-in-linear-regression detach(mort_poll) Then you fit a linear model with all 15 variables in the dataset. attach(mort_poll) lm2 &lt;- lm(mort ~ prec + jant + jult + humid + hc + nox + so2 + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor) lm2$coef (Intercept) prec jant jult humid hc 1.7640e+03 1.9054e+00 -1.9376e+00 -3.1004e+00 1.0680e-01 -6.7214e-01 nox so2 ovr65 popn educ hous 1.3401e+00 8.6252e-02 -9.0654e+00 -1.0683e+02 -1.7157e+01 -6.5112e-01 dens nonw wwdrk poor 3.6005e-03 4.4596e+00 -1.8706e-01 -1.6764e-01 detach(mort_poll) Now perform model selection stepwise, based on AIC. # model selection based on AIC library(MASS) attach(mort_poll) lm1 &lt;- lm(mort ~ 1) # get AIC, mind the difference AIC(lm1) [1] 668.92 extractAIC(lm1) [1] 1.00 496.65 # for linear models with unknown scale (i.e., for lm and aov), # -2 log L is computed from the deviance and uses a different additive constant to # logLik and hence AIC # forward search stepAIC(lm1, list(upper = ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid, lower = ~ 1), direction = &quot;forward&quot;) Start: AIC=496.65 mort ~ 1 Df Sum of Sq RSS AIC + nonw 1 94613 133695 467 + educ 1 59612 168696 480 + prec 1 59266 169041 481 + hous 1 41592 186716 487 + poor 1 38470 189838 488 + log(so2) 1 37087 191221 488 + popn 1 29149 199159 490 + log(nox) 1 19465 208843 493 + wwdrk 1 18518 209789 494 + jult 1 17520 210788 494 + dens 1 16093 212214 494 &lt;none&gt; 228308 497 + hc 1 7172 221136 497 + ovr65 1 6960 221347 497 + humid 1 1788 226520 498 + jant 1 206 228102 499 Step: AIC=466.54 mort ~ nonw Df Sum of Sq RSS AIC + educ 1 33853 99841 451 + log(so2) 1 31223 102471 453 + jant 1 29835 103859 453 + ovr65 1 21435 112259 458 + wwdrk 1 18153 115541 460 + dens 1 16540 117155 461 + prec 1 16324 117371 461 + hous 1 7264 126430 465 + log(nox) 1 6836 126859 465 + hc 1 5892 127803 466 &lt;none&gt; 133695 467 + jult 1 2973 130721 467 + popn 1 2112 131582 468 + poor 1 851 132844 468 + humid 1 37 133658 469 Step: AIC=451.02 mort ~ nonw + educ Df Sum of Sq RSS AIC + log(so2) 1 18177 81664 441 + jant 1 17453 82389 441 + poor 1 10921 88920 446 + log(nox) 1 8814 91027 447 + ovr65 1 7352 92489 448 + dens 1 7262 92579 448 + jult 1 6836 93005 449 &lt;none&gt; 99841 451 + prec 1 2468 97373 452 + hc 1 617 99224 453 + humid 1 530 99312 453 + popn 1 359 99482 453 + hous 1 167 99674 453 + wwdrk 1 14 99827 453 Step: AIC=440.96 mort ~ nonw + educ + log(so2) Df Sum of Sq RSS AIC + prec 1 9399 72266 436 + jant 1 7885 73780 437 + hc 1 6355 75309 438 + ovr65 1 2770 78895 441 &lt;none&gt; 81664 441 + poor 1 2236 79428 441 + dens 1 794 80871 442 + humid 1 692 80973 442 + hous 1 538 81126 443 + log(nox) 1 312 81352 443 + wwdrk 1 281 81383 443 + jult 1 132 81532 443 + popn 1 3 81661 443 Step: AIC=435.63 mort ~ nonw + educ + log(so2) + prec Df Sum of Sq RSS AIC + jant 1 5764 66502 433 + poor 1 2878 69388 435 &lt;none&gt; 72266 436 + hc 1 1594 70672 436 + log(nox) 1 1089 71177 437 + jult 1 981 71285 437 + dens 1 752 71513 437 + humid 1 508 71758 437 + wwdrk 1 405 71861 437 + hous 1 125 72141 438 + popn 1 70 72196 438 + ovr65 1 2 72263 438 Step: AIC=432.64 mort ~ nonw + educ + log(so2) + prec + jant Df Sum of Sq RSS AIC + log(nox) 1 8313 58188 427 &lt;none&gt; 66502 433 + popn 1 1724 64778 433 + dens 1 1455 65047 433 + jult 1 1097 65405 434 + humid 1 958 65544 434 + poor 1 474 66028 434 + hous 1 74 66428 435 + ovr65 1 56 66446 435 + wwdrk 1 27 66475 435 + hc 1 25 66477 435 Step: AIC=426.63 mort ~ nonw + educ + log(so2) + prec + jant + log(nox) Df Sum of Sq RSS AIC + popn 1 2793 55396 426 &lt;none&gt; 58188 427 + dens 1 1379 56810 427 + hc 1 792 57396 428 + jult 1 43 58145 429 + poor 1 25 58164 429 + humid 1 16 58172 429 + hous 1 8 58181 429 + wwdrk 1 6 58182 429 + ovr65 1 0 58188 429 Step: AIC=425.67 mort ~ nonw + educ + log(so2) + prec + jant + log(nox) + popn Df Sum of Sq RSS AIC &lt;none&gt; 55396 426 + ovr65 1 1802 53594 426 + hc 1 1587 53809 426 + dens 1 570 54826 427 + jult 1 142 55254 428 + wwdrk 1 129 55267 428 + poor 1 54 55341 428 + humid 1 10 55386 428 + hous 1 4 55392 428 Call: lm(formula = mort ~ nonw + educ + log(so2) + prec + jant + log(nox) + popn) Coefficients: (Intercept) nonw educ log(so2) prec jant 1315.91 4.50 -19.04 -6.15 2.19 -2.86 log(nox) popn 24.01 -73.76 # backward search lm1 &lt;- lm(mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid) stepAIC(lm1, list(upper = ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid, lower = ~ 1), direction = &quot;backward&quot;) Start: AIC=435.55 mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - poor 1 1 50019 434 - hous 1 224 50243 434 - wwdrk 1 447 50465 434 - dens 1 448 50467 434 - humid 1 704 50723 434 - ovr65 1 1490 51508 435 - jult 1 1543 51562 435 &lt;none&gt; 50019 436 - log(so2) 1 1758 51776 436 - hc 1 1879 51897 436 - educ 1 2667 52686 437 - popn 1 5218 55237 440 - jant 1 6882 56901 441 - prec 1 9111 59130 444 - nonw 1 10236 60255 445 - log(nox) 1 10651 60669 445 Step: AIC=433.55 mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - hous 1 386 50406 432 - dens 1 450 50469 432 - wwdrk 1 473 50492 432 - humid 1 728 50747 432 - jult 1 1561 51580 433 &lt;none&gt; 50019 434 - ovr65 1 1703 51723 434 - log(so2) 1 1757 51776 434 - hc 1 1950 51969 434 - educ 1 2668 52687 435 - popn 1 5284 55304 438 - prec 1 9583 59602 442 - log(nox) 1 10672 60691 443 - jant 1 11569 61589 444 - nonw 1 13554 63573 446 Step: AIC=432.01 mort ~ prec + jant + jult + ovr65 + popn + educ + dens + nonw + wwdrk + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - dens 1 256 50662 430 - wwdrk 1 401 50806 430 - humid 1 691 51097 431 - ovr65 1 1434 51840 432 - jult 1 1434 51840 432 &lt;none&gt; 50406 432 - log(so2) 1 1774 52179 432 - hc 1 2076 52482 432 - educ 1 4067 54473 435 - popn 1 5101 55507 436 - prec 1 9231 59637 440 - log(nox) 1 10385 60791 441 - jant 1 11860 62266 443 - nonw 1 17454 67860 448 Step: AIC=430.31 mort ~ prec + jant + jult + ovr65 + popn + educ + nonw + wwdrk + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - wwdrk 1 351 51013 429 - humid 1 752 51414 429 - jult 1 1398 52059 430 - log(so2) 1 1630 52292 430 &lt;none&gt; 50662 430 - ovr65 1 1735 52397 430 - hc 1 2141 52803 431 - educ 1 4949 55610 434 - popn 1 6396 57058 435 - prec 1 9657 60318 439 - log(nox) 1 10995 61657 440 - jant 1 12455 63117 442 - nonw 1 17200 67862 446 Step: AIC=428.73 mort ~ prec + jant + jult + ovr65 + popn + educ + nonw + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - humid 1 672 51684 428 - jult 1 1512 52524 428 - log(so2) 1 1699 52712 429 &lt;none&gt; 51013 429 - ovr65 1 1816 52829 429 - hc 1 1862 52874 429 - popn 1 6052 57064 433 - prec 1 10101 61113 438 - educ 1 10538 61551 438 - log(nox) 1 10737 61749 438 - jant 1 13041 64054 440 - nonw 1 16852 67864 444 Step: AIC=427.51 mort ~ prec + jant + jult + ovr65 + popn + educ + nonw + hc + log(nox) + log(so2) Df Sum of Sq RSS AIC - jult 1 847 52531 426 - log(so2) 1 1161 52846 427 - hc 1 1248 52932 427 &lt;none&gt; 51684 428 - ovr65 1 1801 53485 428 - popn 1 5567 57251 432 - prec 1 9735 61419 436 - log(nox) 1 10131 61815 436 - educ 1 10422 62106 437 - jant 1 13560 65244 439 - nonw 1 16185 67869 442 Step: AIC=426.49 mort ~ prec + jant + ovr65 + popn + educ + nonw + hc + log(nox) + log(so2) Df Sum of Sq RSS AIC - log(so2) 1 1024 53555 426 - hc 1 1063 53594 426 - ovr65 1 1277 53809 426 &lt;none&gt; 52531 426 - popn 1 4802 57333 430 - prec 1 9117 61648 434 - educ 1 9576 62107 435 - log(nox) 1 11716 64248 437 - jant 1 13524 66055 438 - nonw 1 15419 67951 440 Step: AIC=425.65 mort ~ prec + jant + ovr65 + popn + educ + nonw + hc + log(nox) Df Sum of Sq RSS AIC - hc 1 959 54515 425 - ovr65 1 1306 54861 425 &lt;none&gt; 53555 426 - popn 1 4006 57562 428 - prec 1 8495 62050 432 - educ 1 8553 62108 433 - nonw 1 14573 68129 438 - jant 1 14730 68285 438 - log(nox) 1 17866 71421 441 Step: AIC=424.71 mort ~ prec + jant + ovr65 + popn + educ + nonw + log(nox) Df Sum of Sq RSS AIC - ovr65 1 1809 56323 425 &lt;none&gt; 54515 425 - popn 1 3867 58382 427 - educ 1 9051 63566 432 - prec 1 12481 66996 435 - nonw 1 14126 68641 437 - log(nox) 1 18324 72839 440 - jant 1 21424 75939 443 Step: AIC=424.67 mort ~ prec + jant + popn + educ + nonw + log(nox) Df Sum of Sq RSS AIC &lt;none&gt; 56323 425 - popn 1 2067 58391 425 - educ 1 7387 63710 430 - prec 1 11450 67773 434 - log(nox) 1 16531 72854 438 - jant 1 20400 76724 441 - nonw 1 33353 89676 451 Call: lm(formula = mort ~ prec + jant + popn + educ + nonw + log(nox)) Coefficients: (Intercept) prec jant popn educ nonw 1232.06 2.08 -2.38 -60.09 -16.88 4.36 log(nox) 17.74 # both directions search lm1 &lt;- lm(mort ~ 1) lm1 &lt;- lm(mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid) stepAIC(lm1, list(upper = ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid, lower = ~ 1), direction = &quot;both&quot;) Start: AIC=435.55 mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + poor + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - poor 1 1 50019 434 - hous 1 224 50243 434 - wwdrk 1 447 50465 434 - dens 1 448 50467 434 - humid 1 704 50723 434 - ovr65 1 1490 51508 435 - jult 1 1543 51562 435 &lt;none&gt; 50019 436 - log(so2) 1 1758 51776 436 - hc 1 1879 51897 436 - educ 1 2667 52686 437 - popn 1 5218 55237 440 - jant 1 6882 56901 441 - prec 1 9111 59130 444 - nonw 1 10236 60255 445 - log(nox) 1 10651 60669 445 Step: AIC=433.55 mort ~ prec + jant + jult + ovr65 + popn + educ + hous + dens + nonw + wwdrk + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - hous 1 386 50406 432 - dens 1 450 50469 432 - wwdrk 1 473 50492 432 - humid 1 728 50747 432 - jult 1 1561 51580 433 &lt;none&gt; 50019 434 - ovr65 1 1703 51723 434 - log(so2) 1 1757 51776 434 - hc 1 1950 51969 434 - educ 1 2668 52687 435 + poor 1 1 50019 436 - popn 1 5284 55304 438 - prec 1 9583 59602 442 - log(nox) 1 10672 60691 443 - jant 1 11569 61589 444 - nonw 1 13554 63573 446 Step: AIC=432.01 mort ~ prec + jant + jult + ovr65 + popn + educ + dens + nonw + wwdrk + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - dens 1 256 50662 430 - wwdrk 1 401 50806 430 - humid 1 691 51097 431 - ovr65 1 1434 51840 432 - jult 1 1434 51840 432 &lt;none&gt; 50406 432 - log(so2) 1 1774 52179 432 - hc 1 2076 52482 432 + hous 1 386 50019 434 + poor 1 163 50243 434 - educ 1 4067 54473 435 - popn 1 5101 55507 436 - prec 1 9231 59637 440 - log(nox) 1 10385 60791 441 - jant 1 11860 62266 443 - nonw 1 17454 67860 448 Step: AIC=430.31 mort ~ prec + jant + jult + ovr65 + popn + educ + nonw + wwdrk + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - wwdrk 1 351 51013 429 - humid 1 752 51414 429 - jult 1 1398 52059 430 - log(so2) 1 1630 52292 430 &lt;none&gt; 50662 430 - ovr65 1 1735 52397 430 - hc 1 2141 52803 431 + dens 1 256 50406 432 + hous 1 193 50469 432 + poor 1 61 50600 432 - educ 1 4949 55610 434 - popn 1 6396 57058 435 - prec 1 9657 60318 439 - log(nox) 1 10995 61657 440 - jant 1 12455 63117 442 - nonw 1 17200 67862 446 Step: AIC=428.73 mort ~ prec + jant + jult + ovr65 + popn + educ + nonw + hc + log(nox) + log(so2) + humid Df Sum of Sq RSS AIC - humid 1 672 51684 428 - jult 1 1512 52524 428 - log(so2) 1 1699 52712 429 &lt;none&gt; 51013 429 - ovr65 1 1816 52829 429 - hc 1 1862 52874 429 + wwdrk 1 351 50662 430 + dens 1 206 50806 430 + hous 1 161 50852 431 + poor 1 103 50909 431 - popn 1 6052 57064 433 - prec 1 10101 61113 438 - educ 1 10538 61551 438 - log(nox) 1 10737 61749 438 - jant 1 13041 64054 440 - nonw 1 16852 67864 444 Step: AIC=427.51 mort ~ prec + jant + jult + ovr65 + popn + educ + nonw + hc + log(nox) + log(so2) Df Sum of Sq RSS AIC - jult 1 847 52531 426 - log(so2) 1 1161 52846 427 - hc 1 1248 52932 427 &lt;none&gt; 51684 428 - ovr65 1 1801 53485 428 + humid 1 672 51013 429 + wwdrk 1 270 51414 429 + dens 1 264 51420 429 + poor 1 142 51542 429 + hous 1 129 51555 429 - popn 1 5567 57251 432 - prec 1 9735 61419 436 - log(nox) 1 10131 61815 436 - educ 1 10422 62106 437 - jant 1 13560 65244 439 - nonw 1 16185 67869 442 Step: AIC=426.49 mort ~ prec + jant + ovr65 + popn + educ + nonw + hc + log(nox) + log(so2) Df Sum of Sq RSS AIC - log(so2) 1 1024 53555 426 - hc 1 1063 53594 426 - ovr65 1 1277 53809 426 &lt;none&gt; 52531 426 + jult 1 847 51684 428 + wwdrk 1 434 52098 428 + dens 1 172 52360 428 + hous 1 89 52442 428 + poor 1 30 52502 428 + humid 1 7 52524 428 - popn 1 4802 57333 430 - prec 1 9117 61648 434 - educ 1 9576 62107 435 - log(nox) 1 11716 64248 437 - jant 1 13524 66055 438 - nonw 1 15419 67951 440 Step: AIC=425.65 mort ~ prec + jant + ovr65 + popn + educ + nonw + hc + log(nox) Df Sum of Sq RSS AIC - hc 1 959 54515 425 - ovr65 1 1306 54861 425 &lt;none&gt; 53555 426 + log(so2) 1 1024 52531 426 + jult 1 710 52846 427 + wwdrk 1 526 53029 427 + hous 1 139 53416 427 + dens 1 59 53496 428 + poor 1 40 53515 428 + humid 1 32 53523 428 - popn 1 4006 57562 428 - prec 1 8495 62050 432 - educ 1 8553 62108 433 - nonw 1 14573 68129 438 - jant 1 14730 68285 438 - log(nox) 1 17866 71421 441 Step: AIC=424.71 mort ~ prec + jant + ovr65 + popn + educ + nonw + log(nox) Df Sum of Sq RSS AIC - ovr65 1 1809 56323 425 &lt;none&gt; 54515 425 + hc 1 959 53555 426 + log(so2) 1 921 53594 426 + jult 1 556 53959 426 + wwdrk 1 233 54281 426 + hous 1 221 54293 426 + humid 1 211 54304 426 + poor 1 109 54406 427 + dens 1 86 54429 427 - popn 1 3867 58382 427 - educ 1 9051 63566 432 - prec 1 12481 66996 435 - nonw 1 14126 68641 437 - log(nox) 1 18324 72839 440 - jant 1 21424 75939 443 Step: AIC=424.67 mort ~ prec + jant + popn + educ + nonw + log(nox) Df Sum of Sq RSS AIC &lt;none&gt; 56323 425 + ovr65 1 1809 54515 425 - popn 1 2067 58391 425 + hc 1 1462 54861 425 + log(so2) 1 928 55396 426 + dens 1 348 55975 426 + wwdrk 1 187 56136 426 + humid 1 102 56222 427 + jult 1 97 56226 427 + poor 1 47 56277 427 + hous 1 15 56308 427 - educ 1 7387 63710 430 - prec 1 11450 67773 434 - log(nox) 1 16531 72854 438 - jant 1 20400 76724 441 - nonw 1 33353 89676 451 Call: lm(formula = mort ~ prec + jant + popn + educ + nonw + log(nox)) Coefficients: (Intercept) prec jant popn educ nonw 1232.06 2.08 -2.38 -60.09 -16.88 4.36 log(nox) 17.74 detach(mort_poll) 10.3 Exercises Learning check Load the Boston Housing dataset from the mlbench package. Use the following instructions library(mlbench) data(&quot;BostonHousing&quot;) Inspect the different types of variables present. Explore and visualize the distribution of our target variable medv. Explore and visualize any potential correlations between medv and the variables crim, rm, age, rad, tax and lstat. Set a seed of 123 and split your data into a train and test set using a 75/25 split. You may find the caret library helpful here. We have seen that crim, rm, tax, and lstat could be good predictors of medv. To get the ball rolling, let us fit a linear model for these terms. Obtain an R-squared value for your model and examine the diagnostic plots found by plotting your linear model. We can see a few problems with our model immediately with variables such as 381 exhibiting a high leverage, a poor QQ plot in the tails a relatively poor r-squared value. Let us try another model, this time transforming medv due to the positive skewness it exhibited. Examine the diagnostics for the model. What do you conclude? Is this an improvement on the first model? One assumption of a linear model is that the mean of the residuals is zero. You could try and test this. Create a data frame of your predicted values and the original values. Plot this to visualize the performance of your model. "],["glms.html", "11 Generalized Linear Models in R 11.1 Modelling count data with Poisson regression models 11.2 Overdispersed Poisson regression 11.3 Negative Binomial regression", " 11 Generalized Linear Models in R Youll now study the use of Generalized Linear Models in R for insurance ratemaking. You focus first on the example from Rob Kaas et al. (2008) Modern Actuarial Risk Theory book (see Section 9.5 in this book), with simulated claim frequency data. 11.1 Modelling count data with Poisson regression models 11.1.1 A first data set This example uses artifical, simulated data. You consider data on claim frequencies, registered on 54 risk cells over a period of 7 years. n gives the number of claims, and expo the corresponding number of policies in a risk cell; each policy is followed over a period of 7 years and n is the number of claims reported over this total period. n &lt;- scan(n = 54) 1 8 10 8 5 11 14 12 11 10 5 12 13 12 15 13 12 24 12 11 6 8 16 19 28 11 14 4 12 8 18 3 17 6 11 18 12 3 10 18 10 13 12 31 16 16 13 14 8 19 20 9 23 27 expo &lt;- scan(n = 54) * 7 10 22 30 11 15 20 25 25 23 28 19 22 19 21 19 16 18 29 25 18 20 13 26 21 27 14 16 11 23 26 29 13 26 13 17 27 20 18 20 29 27 24 23 26 18 25 17 29 11 24 16 11 22 29 n expo [1] 1 8 10 8 5 11 14 12 11 10 5 12 13 12 15 13 12 24 12 11 6 8 16 19 28 [26] 11 14 4 12 8 18 3 17 6 11 18 12 3 10 18 10 13 12 31 16 16 13 14 8 19 [51] 20 9 23 27 [1] 70 154 210 77 105 140 175 175 161 196 133 154 133 147 133 112 126 203 175 [20] 126 140 91 182 147 189 98 112 77 161 182 203 91 182 91 119 189 140 126 [39] 140 203 189 168 161 182 126 175 119 203 77 168 112 77 154 203 The goal is to illustrate ratemaking by explaining the expected number of claims as a function of a set of observable risk factors. Since artificial data are used in this example, you use simulated or self constructed risk factors. 4 factor variables are created, the sex of the policyholder (1=female and 2=male), the region where she lives (1=countryside, 2=elsewhere and 3=big city), the type of car (1=small, 2=middle and 3=big) and job class of the insured (1=civil servant/actuary/, 2=in-between and 3=dynamic drivers). You use the R instruction rep() to construct these risk factors. In total 54 risk cells are created in this way. Note that you use the R instruction as.factor() to specify the risk factors as factor (or: categorical) covariates. sex &lt;- as.factor(rep(1:2, each=27, len=54)) region &lt;- as.factor(rep(1:3, each=9, len=54)) type &lt;- as.factor(rep(1:3, each=3, len=54)) job &lt;- as.factor(rep(1:3, each=1, len=54)) sex [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 [39] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 Levels: 1 2 region [1] 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 2 2 [39] 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 Levels: 1 2 3 type [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 [39] 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 Levels: 1 2 3 job [1] 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 [39] 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 Levels: 1 2 3 11.1.2 Fit a Poisson GLM The response variable \\(N_i\\) is the number of claims reported on risk cell i, hence it is reasonable to assume a Poisson distribution for this random variable. You fit the following Poisson GLM to the data \\[\\begin{eqnarray*} N_i &amp;\\sim&amp; \\text{POI}(d_i \\cdot \\lambda_i) \\end{eqnarray*}\\] where \\(\\lambda_i = \\exp{(\\boldsymbol{x}^{&#39;}_i\\boldsymbol{\\beta})}\\) and \\(d_i\\) is the exposure for risk cell \\(i\\). In R you use the instruction glm to fit a GLM. Covariates are listed with +, and the log of expo is used as an offset. Indeed, \\[\\begin{eqnarray*} N_i &amp;\\sim&amp; \\text{POI}(d_i \\cdot \\lambda_i) \\\\ &amp;= &amp; \\text{POI}(\\exp{(\\boldsymbol{x}^{&#39;}_i \\boldsymbol{\\beta}+\\log{(d_i)})}) \\end{eqnarray*}\\] The R instruction to fit this GLM (with sex, region, type and job the factor variables that construct the linear predictor) then goes as follows g1 &lt;- glm(n ~ sex + region + type + job + offset(log(expo)), fam = poisson(link = log)) where the argument fam= indicates the distribution from the exponential family that is assumed. In this case you work with the Poisson distribution with logarithmic link (which is the default link in R). All available distributions and their default link functions are listed here http://stat.ethz.ch/R-manual/R-patched/library/stats/html/family.html. You store the results of the glm fit in the object g1. You consult this object with the summary instruction summary(g1) Call: glm(formula = n ~ sex + region + type + job + offset(log(expo)), family = poisson(link = log)) Deviance Residuals: Min 1Q Median 3Q Max -1.9278 -0.6303 -0.0215 0.5380 2.3000 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.0996 0.1229 -25.21 &lt; 2e-16 *** sex2 0.1030 0.0763 1.35 0.1771 region2 0.2347 0.0992 2.36 0.0180 * region3 0.4643 0.0965 4.81 1.5e-06 *** type2 0.3946 0.1017 3.88 0.0001 *** type3 0.5844 0.0971 6.02 1.8e-09 *** job2 -0.0362 0.0970 -0.37 0.7091 job3 0.0607 0.0926 0.66 0.5121 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 104.73 on 53 degrees of freedom Residual deviance: 41.93 on 46 degrees of freedom AIC: 288.2 Number of Fisher Scoring iterations: 4 This summary of a glm fit lists (among others) the following items: the covariates used in the model, the corresponding estimates for the regression parameters (\\(\\boldsymbol{\\hat{\\beta}}\\)), their standard errors, \\(z\\) statistics and corresponding \\(P\\) values; the dispersion parameter used; for the standard Poisson regression model this dispersion parameter is equal to 1, as indicated in the R output; the null deviance - the deviance of the model that uses only an intercept - and the residual deviance - the deviance of the current model; the null deviance corresponds to \\(53\\) degrees of freedom, that is \\(54-1\\) where \\(54\\) is the number of observations used and \\(1\\) the number of parameters (here: just the intercept); the residual deviance corresponds to \\(54-8=46\\) degrees of freedom, since it uses \\(8\\) parameters; the AIC calculated for the considered regression model; the number of Fishers iterations needed to get convergence of the iterative numerical method to calculate the MLEs of the regression parameters in \\(\\boldsymbol{\\beta}\\). The instruction names shows the names of the variables stored within a glm object. One of these variables is called coef and contains the vector of regression parameter estimates (\\(\\hat{\\boldsymbol{\\beta}}\\)). It can be extracted with the instruction g1$coef. names(g1) [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;fitted.values&quot; [4] &quot;effects&quot; &quot;R&quot; &quot;rank&quot; [7] &quot;qr&quot; &quot;family&quot; &quot;linear.predictors&quot; [10] &quot;deviance&quot; &quot;aic&quot; &quot;null.deviance&quot; [13] &quot;iter&quot; &quot;weights&quot; &quot;prior.weights&quot; [16] &quot;df.residual&quot; &quot;df.null&quot; &quot;y&quot; [19] &quot;converged&quot; &quot;boundary&quot; &quot;model&quot; [22] &quot;call&quot; &quot;formula&quot; &quot;terms&quot; [25] &quot;data&quot; &quot;offset&quot; &quot;control&quot; [28] &quot;method&quot; &quot;contrasts&quot; &quot;xlevels&quot; g1$coef (Intercept) sex2 region2 region3 type2 type3 -3.099592 0.103034 0.234677 0.464340 0.394632 0.584428 job2 job3 -0.036174 0.060717 Other variables can be consulted in a similar way. For example, fitted values at the original level are \\(\\hat{\\mu}_i=\\exp{(\\hat{\\eta}_i)}\\) where the fitted values at the level of the linear predictor are stored in \\(\\hat{\\eta}_i=\\log{(d_i)}+\\boldsymbol{x}^{&#39;}_i\\hat{\\boldsymbol{\\beta}}\\). You then plot the fitted values versus the observed number of claims n. You add two reference lines: the diagonal and the least squares line. g1$fitted.values 1 2 3 4 5 6 7 8 9 10 3.1547 6.6938 10.0566 5.1492 6.7722 9.9483 14.1487 13.6460 13.8316 11.1697 11 12 13 14 15 16 17 18 19 20 7.3101 9.3255 11.2466 11.9888 11.9506 11.4503 12.4239 22.0527 12.5477 8.7134 21 22 23 24 25 26 27 28 29 30 10.6665 9.6817 18.6755 16.6187 24.3109 12.1578 15.3083 3.8468 7.7576 9.6617 31 32 33 34 35 36 37 38 39 40 15.0485 6.5062 14.3364 8.1558 10.2864 17.9993 8.8442 7.6770 9.3978 19.0289 41 42 43 44 45 46 47 48 49 50 17.0871 16.7338 18.2461 19.8933 15.1734 13.9095 9.1224 17.1450 9.0813 19.1099 51 52 53 54 14.0361 10.9794 21.1786 30.7575 g1$linear.predictors 1 2 3 4 5 6 7 8 9 10 11 1.1489 1.9012 2.3082 1.6388 1.9128 2.2974 2.6496 2.6134 2.6270 2.4132 1.9893 12 13 14 15 16 17 18 19 20 21 22 2.2328 2.4201 2.4840 2.4808 2.4380 2.5196 3.0934 2.5295 2.1649 2.3671 2.2702 23 24 25 26 27 28 29 30 31 32 33 2.9272 2.8105 3.1909 2.4980 2.7284 1.3472 2.0487 2.2682 2.7113 1.8728 2.6628 34 35 36 37 38 39 40 41 42 43 44 2.0987 2.3308 2.8903 2.1798 2.0382 2.2405 2.9460 2.8383 2.8174 2.9040 2.9904 45 46 47 48 49 50 51 52 53 54 2.7195 2.6326 2.2107 2.8417 2.2062 2.9502 2.6416 2.3960 3.0530 3.4261 plot(g1$fitted.values, n, xlab = &quot;Fitted values&quot;, ylab = &quot;Observed claims&quot;) abline(lm(g1$fitted ~ n), col=&quot;light blue&quot;, lwd=2) abline(0, 1, col = &quot;dark blue&quot;, lwd=2) To extract the AIC you use AIC(g1) [1] 288.24 11.1.3 The use of exposure The use of expo, the exposure measure, in a Poisson GLM often leads to confusion. For example, the following glm instruction uses a transformed response variable \\(n/expo\\) g2 &lt;- glm(n/expo ~ sex+region+type+job,fam=poisson(link=log)) summary(g2) Call: glm(formula = n/expo ~ sex + region + type + job, family = poisson(link = log)) Deviance Residuals: Min 1Q Median 3Q Max -0.16983 -0.05628 -0.00118 0.04680 0.17676 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.1392 1.4846 -2.11 0.034 * sex2 0.1007 0.9224 0.11 0.913 region2 0.2624 1.2143 0.22 0.829 region3 0.4874 1.1598 0.42 0.674 type2 0.4095 1.2298 0.33 0.739 type3 0.5757 1.1917 0.48 0.629 job2 -0.0308 1.1502 -0.03 0.979 job3 0.0957 1.1150 0.09 0.932 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 0.77537 on 53 degrees of freedom Residual deviance: 0.31739 on 46 degrees of freedom AIC: Inf Number of Fisher Scoring iterations: 5 and the object g3 stores the result of a Poisson fit on the same response variable, while taking expo into account as weights in the likelihood. g3 &lt;- glm(n/expo ~ sex+region+type+job,weights=expo,fam=poisson(link=log)) summary(g3) Call: glm(formula = n/expo ~ sex + region + type + job, family = poisson(link = log), weights = expo) Deviance Residuals: Min 1Q Median 3Q Max -1.9278 -0.6303 -0.0215 0.5380 2.3000 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.0996 0.1229 -25.21 &lt; 2e-16 *** sex2 0.1030 0.0763 1.35 0.1771 region2 0.2347 0.0992 2.36 0.0180 * region3 0.4643 0.0965 4.81 1.5e-06 *** type2 0.3946 0.1017 3.88 0.0001 *** type3 0.5844 0.0971 6.02 1.8e-09 *** job2 -0.0362 0.0970 -0.37 0.7091 job3 0.0607 0.0926 0.66 0.5121 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 104.73 on 53 degrees of freedom Residual deviance: 41.93 on 46 degrees of freedom AIC: Inf Number of Fisher Scoring iterations: 5 Based on this output you conclude that g1 (with the log of exposure as offset in the linear predictor) and g3 are the same, but g2 is not. The mathematical explanation for this observation is given in the note WeightsInGLMs.pdf available from Katriens lecture notes (available upon request). 11.1.4 Analysis of deviance for GLMs 11.1.4.1 The basics You now focus on the selection of variables within a GLM based on a drop in deviance analysis. Your starting point is the GLM object g1 and the anova instruction. g1 &lt;- glm(n ~ 1 + region + type + job, poisson, offset = log(expo)) anova(g1, test=&quot;Chisq&quot;) Analysis of Deviance Table Model: poisson, link: log Response: n Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) NULL 53 104.7 region 2 21.6 51 83.1 2.0e-05 *** type 2 38.2 49 44.9 5.1e-09 *** job 2 1.2 47 43.8 0.55 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The analysis of deviance table first summarizes the Poisson GLM object (response n, link is log, family is poisson). The table starts with the deviance of the NULL model (just using an intercept), and then adds risk factors sequentially. Recall that in this example only factor covariates are present. Adding region (which has three levels, and requires two dummy variables) to the NULL model causes a drop in deviance of 21.597, corresponding to 54-1-2 degrees of freedom and a resulting (residual) deviance of 83.135. The drop in deviance test allows to test whether the model term region is significant. That is: \\[ H_0: \\beta_{\\text{region}_2}=0\\ \\text{and}\\ \\beta_{\\text{region}_3}=0. \\] The distribution of the corresponding test statistic is a Chi-squared distribution with 2 (i.e 53-51) degrees of freedom. The corresponding \\(P\\)-value is 2.043e-05. Hence, the model using region and the intercept is preferred above the NULL model. We can verify the \\(P\\)-value by calculating the following probability \\[ Pr(X &gt; 21.597)\\ \\text{with}\\ X \\sim \\chi^2_{2}.\\] Indeed, this is the probability - under \\(H_0\\) - to obtain a value of the test statistic that is the same or more extreme than the actual observed value of the test statistic. Calculations in R are as follows: # p-value for region 1 - pchisq(21.597, 2) [1] 2.043e-05 # or pchisq(21.597, 2, lower.tail = FALSE) [1] 2.043e-05 Continuing the discussion of the above printed anova table, the next step is to add type to the model using an intercept and region. This causes a drop in deviance of 38.195. You conclude that also type is a significant model term. The last step adds job to the existing model (with intercept, region and type). You conclude that job does not have a significant impact when explaining the expected number of claims. Based on this analysis of deviance table region and type seem to be relevant risk factors, but job is not, when explaining the expected number of claims. The Chi-squared distribution is used here, since the regular Poisson regression model does not require the estimation of a dispersion parameter. anova(g1,test=&quot;Chisq&quot;) The setting changes when the dispersion parameter is unknown and should be estimated. If you run the analysis of deviance for glm object g1 with the F distribution as distribution for the test statistic, you obtain: # what if we use &#39;F&#39; instead of &#39;Chisq&#39;? anova(g1,test=&quot;F&quot;) Warning in anova.glm(g1, test = &quot;F&quot;): using F test with a &#39;poisson&#39; family is inappropriate Analysis of Deviance Table Model: poisson, link: log Response: n Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev F Pr(&gt;F) NULL 53 104.7 region 2 21.6 51 83.1 10.80 2.0e-05 *** type 2 38.2 49 44.9 19.10 5.1e-09 *** job 2 1.2 47 43.8 0.59 0.55 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # not appropriate for regular Poisson regression, see Warning message in the console! and a Warning message is printed in the console that says # Warning message: # In anova.glm(g1, test = &quot;F&quot;) : # using F test with a &#39;poisson&#39; family is inappropriate It is insightful to understand how the output shown for the \\(F\\) statistic and corresponding \\(P\\)-value is calculated. For example, the drop in deviance test comparing the NULL model viz a model using an intercept and region corresponds to an observed test statistic of 10.7985. The calculation of the \\(F\\) statistic requires \\[ \\frac{\\text{Drop-in-deviance}/q}{\\hat{\\phi}}, \\] where \\(q\\) is the difference in degrees of freedom between the compared models and \\(\\hat{\\phi}\\) is the estimate for the dispersion parameter. In this example \\(F\\) corresponding to region is calculated as (21.597/2)/1 [1] 10.799 However, as explained, since the model investigated has a known dispersion, the Chi-squared test is most appropriate here. More details are here: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/anova.glm.html. 11.1.5 An example You are now ready to study a complete analysis-of-deviance table. This table investigates 10 possible model specifications g1-g10. # construct an analysis-of-deviance table g1 &lt;- glm(n ~ 1, poisson , offset=log(expo)) g2 &lt;- glm(n ~ sex, poisson , offset=log(expo)) g3 &lt;- glm(n ~ sex+region, poisson, offset=log(expo)) g4 &lt;- glm(n ~ sex+region+sex:region, poisson, offset=log(expo)) g5 &lt;- glm(n ~ type, poisson, offset=log(expo)) g6 &lt;- glm(n ~ region, poisson, offset=log(expo)) g7 &lt;- glm(n ~ region+type, poisson, offset=log(expo)) g8 &lt;- glm(n ~ region+type+region:type, poisson, offset=log(expo)) g9 &lt;- glm(n ~ region+type+job, poisson, offset=log(expo)) g10 &lt;- glm(n ~ region+type+sex, poisson, offset=log(expo)) For example, the residual deviance obtained with model g8 (using intercept, region, type and the interaction of region and type) is 42.4, see summary(g8) Call: glm(formula = n ~ region + type + region:type, family = poisson, offset = log(expo)) Deviance Residuals: Min 1Q Median 3Q Max -1.8296 -0.4893 -0.0622 0.5377 1.8974 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.9887 0.1525 -19.60 &lt;2e-16 *** region2 0.1499 0.2061 0.73 0.467 region3 0.4216 0.1927 2.19 0.029 * type2 0.4338 0.1985 2.19 0.029 * type3 0.4520 0.1927 2.35 0.019 * region2:type2 -0.0808 0.2664 -0.30 0.762 region3:type2 -0.0223 0.2537 -0.09 0.930 region2:type3 0.2556 0.2562 1.00 0.318 region3:type3 0.1086 0.2449 0.44 0.657 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 104.732 on 53 degrees of freedom Residual deviance: 42.412 on 45 degrees of freedom AIC: 290.7 Number of Fisher Scoring iterations: 4 g8$deviance [1] 42.412 Using the technique of drop in deviance analysis you compare the models that are nested (!!) and decide which model specification is the preferred one. To do this, one can run multiple anova instructions such as anova(g1, g2, test = &quot;Chisq&quot;) Analysis of Deviance Table Model 1: n ~ 1 Model 2: n ~ sex Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 53 105 2 52 103 1 1.93 0.17 which compares nested models g1 and g2, or g7 and g8 anova(g7, g8, test = &quot;Chisq&quot;) Analysis of Deviance Table Model 1: n ~ region + type Model 2: n ~ region + type + region:type Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 49 44.9 2 45 42.4 4 2.53 0.64 11.2 Overdispersed Poisson regression The overdispersed Poisson model builds a regression model for the mean of the response variable \\[ EN_i = \\exp{(\\log d_i + \\boldsymbol{x}_i^{&#39;}\\boldsymbol{\\beta})} \\] and expressses the variance as \\[ \\text{Var}(N_i) = \\phi \\cdot EN_i, \\] with \\(N_i\\) the number of claims reported by policyholder \\(i\\) and \\(\\phi\\) an unknown dispersion parameter that should be estimated. This is called a quasi-Poisson model (see http://stat.ethz.ch/R-manual/R-patched/library/stats/html/family.html) and Section 1 in http://data.princeton.edu/wws509/notes/c4a.pdf for a more detailed explanation. To illustrate the differences between a regular Poisson and an overdispersed Poisson model, we fit the models g.poi and g.quasi: g.poi &lt;- glm(n ~ 1 + region + type, poisson, offset = log(expo)) summary(g.poi) Call: glm(formula = n ~ 1 + region + type, family = poisson, offset = log(expo)) Deviance Residuals: Min 1Q Median 3Q Max -1.9233 -0.6564 -0.0573 0.4790 2.3144 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.0313 0.1015 -29.86 &lt; 2e-16 *** region2 0.2314 0.0990 2.34 0.0195 * region3 0.4605 0.0965 4.77 1.8e-06 *** type2 0.3942 0.1015 3.88 0.0001 *** type3 0.5833 0.0971 6.01 1.9e-09 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 104.73 on 53 degrees of freedom Residual deviance: 44.94 on 49 degrees of freedom AIC: 285.2 Number of Fisher Scoring iterations: 4 g.quasi &lt;- glm(n ~ 1 + region + type, quasipoisson, offset = log(expo)) summary(g.quasi) Call: glm(formula = n ~ 1 + region + type, family = quasipoisson, offset = log(expo)) Deviance Residuals: Min 1Q Median 3Q Max -1.9233 -0.6564 -0.0573 0.4790 2.3144 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -3.0313 0.0961 -31.54 &lt; 2e-16 *** region2 0.2314 0.0938 2.47 0.01715 * region3 0.4605 0.0913 5.04 6.7e-06 *** type2 0.3942 0.0961 4.10 0.00015 *** type3 0.5833 0.0919 6.35 6.8e-08 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for quasipoisson family taken to be 0.89654) Null deviance: 104.73 on 53 degrees of freedom Residual deviance: 44.94 on 49 degrees of freedom AIC: NA Number of Fisher Scoring iterations: 4 Parameter estimates in both models are the same, but standard errors (and hence \\(P\\)-values) are not! You also see that g.poi reports z-value whereas g.quasi reports t-value, because the latter model estimates an extra parameter, i.e. the dispersion parameter. Various methods are available to estimate the dispersion parameter, e.g. \\[ \\hat{\\phi} = \\frac{\\text{Deviance}}{n-(p+1)}\\] and \\[ \\hat{\\phi} = \\frac{\\text{Pearson}\\ \\chi^2}{n-(p+1)}\\] where \\(p+1\\) is the total number of parameters (including the intercept) used in the considered model. The (residual) deviance is the deviance of the considered model and can also be obtained as the sum of squared deviance residuals. The Pearson \\(\\chi^2\\) statistic is the sum of the squared Pearson residuals. The latter is the default in R. Hence, you can verify the dispersion parameter of 0.896 as printed in the summary of g.quasi: # dispersion parameter in g is estimated as follows phi &lt;- sum(residuals(g.poi, &quot;pearson&quot;)^2)/g.poi$df.residual phi [1] 0.89654 Since \\(\\hat{\\phi}\\) is less than 1, the result seems to indicate underdispersion. However, as discussed in Section 2.4 Overdispersion in the book of Denuit et al. (2007), real data on reported claim counts very often reveal overdispersion. The counterintuitive result that is obtained here is probably due to the fact that artificial, self-constructed data are used. When going from g.poi (regular Poisson) to g.quasi the standard errors are changed as follows: \\[ \\text{SE}_{\\text{Q-POI}} = \\sqrt{\\hat{\\phi}} \\cdot \\text{SE}_{\\text{POI}},\\] where \\(\\text{Q-POI}\\) is for quasi-Poisson. As a last step, you run the analysis of deviance for the quasi-Poisson model: anova(g.quasi, test = &quot;F&quot;) Analysis of Deviance Table Model: quasipoisson, link: log Response: n Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev F Pr(&gt;F) NULL 53 104.7 region 2 21.6 51 83.1 12.0 5.6e-05 *** type 2 38.2 49 44.9 21.3 2.2e-07 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For example, the \\(F\\)-statistic for region is calculated as F &lt;- (21.597/2)/phi F [1] 12.045 and the corresponding \\(P\\)-value is pf(F, 2, 49, lower.tail = FALSE) [1] 5.5642e-05 11.3 Negative Binomial regression You now focus on the use of yet another useful count regression model, that is the Negative Binomial regression model. The routine to fit a NB regression model is available in the package MASS and is called glm.nb, see https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/glm.nb.html # install.packages(&quot;MASS&quot;) library(MASS) g.nb &lt;- glm.nb(n ~ 1+region+sex+offset(log(expo))) summary(g.nb) Call: glm.nb(formula = n ~ 1 + region + sex + offset(log(expo)), init.theta = 26.38897379, link = log) Deviance Residuals: Min 1Q Median 3Q Max -2.6222 -0.6531 -0.0586 0.6587 2.3542 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.7342 0.1019 -26.84 &lt; 2e-16 *** region2 0.2359 0.1195 1.97 0.04837 * region3 0.4533 0.1176 3.85 0.00012 *** sex2 0.1049 0.0939 1.12 0.26392 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Negative Binomial(26.389) family taken to be 1) Null deviance: 71.237 on 53 degrees of freedom Residual deviance: 54.917 on 50 degrees of freedom AIC: 316.1 Number of Fisher Scoring iterations: 1 Theta: 26.4 Std. Err.: 15.3 2 x log-likelihood: -306.06 "],["biblio.html", "12 References", " 12 References "]]
